{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "import pickle, gzip, math, torch, matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 200\n",
    "en1_units=100\n",
    "en2_units=100\n",
    "num_topic=50\n",
    "num_input=1995\n",
    "variance=0.995\n",
    "init_mult=1.0\n",
    "learning_rate=0.002\n",
    "batch_size=200\n",
    "momentum=0.99\n",
    "num_epoch=100\n",
    "nogpu=True\n",
    "drop_rate=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(o):\n",
    "    if o is None: return []\n",
    "    if isinstance(o, list): return o\n",
    "    if isinstance(o, str): return [o]\n",
    "    if isinstance(o, Iterable): return list(o)\n",
    "    return [o]\n",
    "def setify(o): return o if isinstance(o,set) else set(listify(o))\n",
    "def compose(x, funcs, *args, order_key='_order', **kwargs):\n",
    "    key = lambda o: getattr(o, order_key, 0)\n",
    "    for f in sorted(listify(funcs), key=key): x = f(x, **kwargs)\n",
    "    return x\n",
    "\n",
    "def collate(b):\n",
    "    return torch.stack(b)\n",
    "\n",
    "class Onehotify():\n",
    "    _order=1\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "    def __call__(self, item):\n",
    "        return np.array(np.bincount(item.astype('int'), minlength=self.vocab_size))\n",
    "\n",
    "class Tensorify():\n",
    "    _order=2\n",
    "    def __call__(self, item):\n",
    "        return torch.from_numpy(item)\n",
    "\n",
    "class Floatify():\n",
    "    _order=3\n",
    "    def __call__(self, item):\n",
    "        return item.float()\n",
    "    \n",
    "class News20Dataset(Dataset):\n",
    "    def __init__(self, x, tfms): \n",
    "        self.x = x\n",
    "        self.x_tfms = tfms\n",
    "    def __len__(self): \n",
    "        return len(self.x)\n",
    "    def __getitem__(self, i): \n",
    "        return compose(self.x[i], self.x_tfms)\n",
    "    \n",
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_perp(model):\n",
    "    cost=[]\n",
    "    model.eval()                        # switch to testing mode\n",
    "    input_ = tensor_te\n",
    "    recon, loss = model(input_, compute_loss=True, avg_loss=False)\n",
    "    loss = loss.data\n",
    "    counts = tensor_te.sum(1)\n",
    "    avg = (loss / counts).mean()\n",
    "    print('The approximated perplexity is: ', math.exp(avg))\n",
    "\n",
    "def visualize():\n",
    "    global recon\n",
    "    input_ = tensor_te[:10]\n",
    "    register_vis_hooks(model)\n",
    "    recon = model(input_, compute_loss=False)\n",
    "    remove_vis_hooks()\n",
    "    save_visualization('pytorch_model', 'png')\n",
    "    \n",
    "def print_perp(model):\n",
    "    cost=[]\n",
    "    model.eval()                        # switch to testing mode\n",
    "    input_ = next(iter(test_dl))\n",
    "    recon, loss = model(input_, compute_loss=True, avg_loss=False)\n",
    "    loss = loss.data\n",
    "    counts = input_.sum(1)\n",
    "    avg = (loss / counts).mean()\n",
    "    print('The approximated perplexity is: ', math.exp(avg))\n",
    "\n",
    "def visualize():\n",
    "    global recon\n",
    "    input_ = tensor_te[:10]\n",
    "    register_vis_hooks(model)\n",
    "    recon = model(input_, compute_loss=False)\n",
    "    remove_vis_hooks()\n",
    "    save_visualization('pytorch_model', 'png')\n",
    "\n",
    "def print_top_words(beta, feature_names, n_top_words=10):\n",
    "    print ('---------------Printing the Topics------------------')\n",
    "    for i in range(len(beta)):\n",
    "        line = \" \".join([feature_names[j] \n",
    "                         for j in beta[i].argsort()[:-n_top_words - 1:-1]])\n",
    "        print('{}'.format(line))\n",
    "    print ('---------------End of Topics------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('data/20news_clean/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/20news_clean/test.txt.npy'),\n",
       " PosixPath('data/20news_clean/train.txt.npy'),\n",
       " PosixPath('data/20news_clean/valid.txt.npy'),\n",
       " PosixPath('data/20news_clean/vocab.pkl')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = path/'train.txt.npy'\n",
    "path_test = path/'test.txt.npy'\n",
    "path_vocab = path/'vocab.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr = np.load(path_train, encoding=\"latin1\")\n",
    "data_te = np.load(path_test, encoding=\"latin1\")\n",
    "vocab_file = open(path_vocab,'rb')\n",
    "vocab = pickle.load(vocab_file)\n",
    "vocab_size=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr = np.array([doc for doc in data_tr if np.sum(doc)!=0])\n",
    "data_te = np.array([doc for doc in data_te if np.sum(doc)!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [Onehotify(vocab_size=vocab_size), Tensorify(), Floatify()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = News20Dataset(data_tr, tfms=tfms)\n",
    "test_ds = News20Dataset(data_te, tfms=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = Sampler(train_ds, bs, shuffle=True)\n",
    "test_samp = Sampler(test_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n",
    "test_dl = DataLoader(test_ds, sampler=test_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProdLDA(nn.Module):\n",
    "    def __init__(self, num_input, en1_units, en2_units, num_topic, drop_rate, init_mult):\n",
    "        super(ProdLDA, self).__init__()\n",
    "        self.num_input, self.en1_units, self.en2_units, \\\n",
    "        self.num_topic, self.drop_rate, self.init_mult = num_input, en1_units, en2_units, \\\n",
    "                                                            num_topic, drop_rate, init_mult\n",
    "        # encoder\n",
    "        self.en = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(num_input, en1_units)),\n",
    "            ('act1', nn.Softplus()),\n",
    "            ('linear2', nn.Linear(en1_units, en2_units)),\n",
    "            ('act2', nn.Softplus()),\n",
    "            ('dropout', nn.Dropout(drop_rate))\n",
    "        ]))\n",
    "        self.mean = nn.Sequential(OrderedDict([\n",
    "            ('linear', nn.Linear(en2_units, num_topic)),\n",
    "            ('batchnorm', nn.BatchNorm1d(num_topic))\n",
    "        ]))\n",
    "        self.logvar = nn.Sequential(OrderedDict([\n",
    "            ('linear', nn.Linear(en2_units, num_topic)),\n",
    "            ('batchnorm', nn.BatchNorm1d(num_topic))\n",
    "        ]))\n",
    "        # decoder\n",
    "        self.de = nn.Sequential(OrderedDict([\n",
    "            ('act1', nn.Softmax(dim=-1)),\n",
    "            ('dropout', nn.Dropout(drop_rate)),\n",
    "            ('linear', nn.Linear(num_topic, num_input)),\n",
    "            ('batchnorm', nn.BatchNorm1d(num_input)),\n",
    "            ('act2', nn.Softmax(dim=-1))\n",
    "        ]))\n",
    "        # prior mean and variance as constant buffers\n",
    "        self.prior_mean   = torch.Tensor(1, num_topic).fill_(0)\n",
    "        self.prior_var    = torch.Tensor(1, num_topic).fill_(variance)\n",
    "        self.prior_mean = nn.Parameter(self.prior_mean, requires_grad=False)\n",
    "        self.prior_var = nn.Parameter(self.prior_var, requires_grad=False)\n",
    "        self.prior_logvar = nn.Parameter(self.prior_var.log(), requires_grad=False)\n",
    "        # initialize decoder weight\n",
    "        if init_mult != 0:\n",
    "            #std = 1. / math.sqrt( init_mult * (num_topic + num_input))\n",
    "            self.de.linear.weight.data.uniform_(0, init_mult)\n",
    "        # remove BN's scale parameters\n",
    "        for component in [self.mean, self.logvar, self.de]:\n",
    "            component.batchnorm.weight.requires_grad = False\n",
    "            component.batchnorm.weight.fill_(1.0)\n",
    "\n",
    "    def encode(self, input_):\n",
    "        encoded = self.en(input_)\n",
    "        posterior_mean = self.mean(encoded)\n",
    "        posterior_logvar = self.logvar(encoded)\n",
    "        return encoded, posterior_mean, posterior_logvar\n",
    "    \n",
    "    def decode(self, input_, posterior_mean, posterior_var):\n",
    "        # take sample\n",
    "        eps = input_.data.new().resize_as_(posterior_mean.data).normal_() # noise \n",
    "        z = posterior_mean + posterior_var.sqrt() * eps                   # reparameterization\n",
    "        # do reconstruction\n",
    "        recon = self.de(z)          # reconstructed distribution over vocabulary\n",
    "        return recon\n",
    "    \n",
    "    def forward(self, input_, compute_loss=False, avg_loss=True):\n",
    "        # compute posterior\n",
    "        en2, posterior_mean, posterior_logvar = self.encode(input_) \n",
    "        posterior_var    = posterior_logvar.exp()\n",
    "        \n",
    "        recon = self.decode(input_, posterior_mean, posterior_var)\n",
    "        if compute_loss:\n",
    "            return recon, self.loss(input_, recon, posterior_mean, posterior_logvar, posterior_var, avg_loss)\n",
    "        else:\n",
    "            return recon\n",
    "\n",
    "    def loss(self, input_, recon, posterior_mean, posterior_logvar, posterior_var, avg=True):\n",
    "        # NL\n",
    "        NL  = -(input_ * (recon + 1e-10).log()).sum(1)\n",
    "        # KLD, see Section 3.3 of Akash Srivastava and Charles Sutton, 2017, \n",
    "        # https://arxiv.org/pdf/1703.01488.pdf\n",
    "        prior_mean   = self.prior_mean.expand_as(posterior_mean)\n",
    "        prior_var    = self.prior_var.expand_as(posterior_mean)\n",
    "        prior_logvar = self.prior_logvar.expand_as(posterior_mean)\n",
    "        var_division    = posterior_var  / prior_var\n",
    "        diff            = posterior_mean - prior_mean\n",
    "        diff_term       = diff * diff / prior_var\n",
    "        logvar_division = prior_logvar - posterior_logvar\n",
    "        # put KLD together\n",
    "        KLD = 0.5 * ( (var_division + diff_term + logvar_division).sum(1) - self.num_topic)\n",
    "        # loss\n",
    "        loss = (NL + KLD)\n",
    "        # in traiming mode, return averaged loss. In testing mode, return individual loss\n",
    "        if avg:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProdLDA(num_input, en1_units, en2_units, num_topic, drop_rate, init_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate, betas=(momentum, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss=746.7075184789197\n",
      "Epoch 5, loss=665.4300326643319\n",
      "Epoch 10, loss=656.3798186203529\n",
      "Epoch 15, loss=640.3864467226226\n",
      "Epoch 20, loss=635.5135119207974\n",
      "Epoch 25, loss=627.7520083723397\n",
      "Epoch 30, loss=619.6737539357152\n",
      "Epoch 35, loss=615.8232500799771\n",
      "Epoch 40, loss=613.9448368467133\n",
      "Epoch 45, loss=610.6923949142982\n",
      "Epoch 50, loss=608.6022507240033\n",
      "Epoch 55, loss=606.7663190118197\n",
      "Epoch 60, loss=604.5161595837824\n",
      "Epoch 65, loss=604.0063718598465\n",
      "Epoch 70, loss=602.491992292733\n",
      "Epoch 75, loss=605.2028750715584\n",
      "Epoch 80, loss=599.282444394868\n",
      "Epoch 85, loss=599.0273027091191\n",
      "Epoch 90, loss=598.9137646905307\n",
      "Epoch 95, loss=598.7541203992121\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    loss_epoch = 0.0\n",
    "    model.train()                    # switch to training mode\n",
    "    for input_ in train_dl:\n",
    "        recon, loss = model(input_, compute_loss=True)\n",
    "        # optimize\n",
    "        optimizer.zero_grad()        # clear previous gradients\n",
    "        loss.backward()              # backprop\n",
    "        optimizer.step()             # update parameters\n",
    "        # report\n",
    "        loss_epoch += loss.item()    # add loss to loss_epoch\n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch {}, loss={}'.format(epoch, loss_epoch / len(input_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Printing the Topics------------------\n",
      "offense hitter pitcher team player defensive season roger career braves\n",
      "launch rocket spacecraft satellite nuclear fund moon lunar orbit administration\n",
      "thanks advance hus honda appreciate surrender _eos_ boot jeff wonder\n",
      "microsoft printer modem hello thanks advance appreciate postscript fax greatly\n",
      "bio mb jumper connector controller rom hd drive floppy interface\n",
      "greece jews turkish turks jew armenian constitution palestinian professor handgun\n",
      "anonymous abuse privacy threat security electronic social rights responsibility militia\n",
      "armenian neighbor apartment beat floor woman armenians doctor stephanopoulos azerbaijan\n",
      "annual bmw rider club ride shipping cd green motorcycle organize\n",
      "st pp calgary philadelphia rangers detroit louis winnipeg pittsburgh jose\n",
      "surrender gordon bank keith ignorance kent thanks uucp associate georgia\n",
      "existence doctrine faith absolute truth belief revelation biblical interpretation conclusion\n",
      "flyers puck neighbor apartment floor penalty beat hide woman girl\n",
      "calgary det puck nj detroit montreal los leafs pit angeles\n",
      "det nj min que pit tor van water buf cal\n",
      "armenia armenian genocide massacre turks armenians village azerbaijan muslim population\n",
      "lebanese israel lebanon civilian arab israeli village palestinian terrorist adam\n",
      "atlanta april germany vote schedule sexual jeff daniel houston gary\n",
      "medical health disease handgun volume md patient laboratory director medicine\n",
      "hitter career helmet pitcher braves baseball rear hit bike pitch\n",
      "panel wiring circuit nec wire neutral voltage outlet fuel ab\n",
      "processing workstation postscript pixel visual export mit dec toolkit image\n",
      "nhl team season offense wings coach european player acquire injury\n",
      "scsi scsus ide isa mb transfer quadra mhz bus meg\n",
      "gun batf violent gang cop criminal handgun crime police weapon\n",
      "colormap font window microsoft xterm screen expose printer windows nt\n",
      "key escrow encrypt serial clipper rsa security enforcement chip nsa\n",
      "spacecraft flight satellite rocket lunar solar shuttle annual km mission\n",
      "encryption americans enforcement administration agency wiretap clipper security government escrow\n",
      "wiretap nsa escrow clipper handgun libertarian violate police gun warrant\n",
      "graphics shipping database amiga workstation graphic sale processing microsoft turbo\n",
      "armenia troops village armenian soldier massacre civilian greece turkish turks\n",
      "msg food sex sexual moral homosexual morality marriage reaction marry\n",
      "mit rsa toolkit dec implementation xt cryptography xterm binary export\n",
      "medical engineering signature research copyright gordon australia newsgroup medicine surrender\n",
      "remark entry output contest oname winner printf postscript int copyright\n",
      "heaven sin christ lord eternal jesus hell pray mary satan\n",
      "shipping speaker item remote sale condition external motorola band brand\n",
      "rg mw sl bhj pl mg ax mi wm pd\n",
      "rear gear helmet ford honda transmission tire mile engine auto\n",
      "dos motherboard mhz quadra adapter processor connector apple clock switch\n",
      "swap microsoft windows modem memory nt meg postscript ram setup\n",
      "mhz motherboard os slot quadra cpu apple vga video clock\n",
      "db mov ax cs mw rg byte voltage push wm\n",
      "militia constitution sex homosexual islamic meaning liberal amendment passage organize\n",
      "moral morality objective scientific absolute science definition existence truth observation\n",
      "jpeg pixel gif visual default image xt setting colormap compression\n",
      "israeli muslims israel islamic arab arabs muslim jew marriage islam\n",
      "doctrine biblical mary jesus spirit christ holy lord scripture interpretation\n",
      "president stephanopoulos administration senate package economic congress meeting aid myers\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1024.778147045771\n"
     ]
    }
   ],
   "source": [
    "emb = model.de.linear.weight.data.cpu().numpy().T\n",
    "print_top_words(emb, list(zip(*sorted(vocab.items(), key=lambda x:x[1])))[0])\n",
    "print_perp(model)\n",
    "# visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt-avitm-1.0",
   "language": "python",
   "name": "pt-avitm-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
