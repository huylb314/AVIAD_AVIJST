{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters, imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "import pickle, gzip, math, torch, matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URSA Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_ds_path = Path('./data/User Review Structure Analysis (URSA)/')\n",
    "xml_path = (folder_ds_path/'Classified_Corpus.xml')\n",
    "ds_path = (folder_ds_path/'5k')\n",
    "sentence_npy_path = (folder_ds_path/'sentence.npy')\n",
    "vocab_pkl_path = (folder_ds_path/'vocab.pkl')\n",
    "\n",
    "# log words not pass\n",
    "aspect_tags = ['Food', 'Staff', 'Ambience']\n",
    "polatiry_tags = ['Positive', 'Negative', 'Neutral']\n",
    "xml_review_tag = './/Review'\n",
    "log_np = [[], [], []]\n",
    "\n",
    "# length allowed sentences\n",
    "length_allowed = [-1, -1, -1]\n",
    "min_freq_allowed = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URSA Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_nested_xml(axml):\n",
    "    return ' '.join([the_aiter for the_aiter in axml.itertext()])\n",
    "\n",
    "def get_firstchild(axml):\n",
    "    try:\n",
    "        if len(axml.getchildren()) > 0:\n",
    "            return axml.getchildren()[0].tag\n",
    "        else:\n",
    "            raise (Exception('ListIndex', 'aXmlElement input has no children.'))\n",
    "    except Exception as e:\n",
    "        print (str(e))\n",
    "\n",
    "def xml_unique_valid(axml, alist_tag_allowed):\n",
    "    return (len(axml.getchildren()) == 0) or (get_firstchild(axml) in alist_tag_allowed)\n",
    "\n",
    "def xml_name_valid(axml, atag_name):\n",
    "    return axml.tag == atag_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listsentence_unique(alist_xml, alist_tag_allowed):\n",
    "    the_listsentence = []\n",
    "    for the_axml in alist_xml:\n",
    "        if xml_unique_valid(the_axml, alist_tag_allowed):\n",
    "            the_listsentence.append(string_nested_xml(the_axml))\n",
    "\n",
    "    return the_listsentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listxml_child(list_xml, tag):\n",
    "    return_ = []\n",
    "    for xml_ in list_xml:\n",
    "        for xml_child in xml_:\n",
    "            if xml_name_valid(xml_child, tag):\n",
    "                return_.append(xml_child)\n",
    "\n",
    "    return return_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listxml_child_list(document_list, tag_list):\n",
    "    return_ = []\n",
    "    for tag in tag_list:\n",
    "        xml_children = get_listxml_child(document_list, tag)\n",
    "        return_.append(xml_children)\n",
    "    \n",
    "    return return_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xml_unique_list(xml_children_list, polatiry_tags):\n",
    "    return_ = []\n",
    "    for xml_children in xml_children_list:\n",
    "        xml_unique = get_listsentence_unique(xml_children, polatiry_tags)\n",
    "        return_.append(xml_unique)\n",
    "    \n",
    "    return return_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tree = ET.parse(xml_path)\n",
    "corpus_root = corpus_tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_list = corpus_root.findall(xml_review_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96235\n",
      "32892\n",
      "16803\n"
     ]
    }
   ],
   "source": [
    "xml_children_list = get_listxml_child_list(document_list, aspect_tags)\n",
    "for idx in range(0, len(xml_children_list)): print (len(xml_children_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62348\n",
      "23730\n",
      "13385\n"
     ]
    }
   ],
   "source": [
    "xml_unique_list = get_xml_unique_list(xml_children_list, polatiry_tags)\n",
    "for idx in range(0, len(xml_unique_list)): print (len(xml_unique_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The food is a melding of Moroccan comfort food and Spanish tapas fare : tagines , stews and salads , with surprises like baby eggplants and olives where you might not expect them . '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_unique_list[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/huylb314/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/huylb314/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabet(atext):\n",
    "    return re.sub(\"[^a-zA-Z]\", \" \", atext)\n",
    "\n",
    "def liststopword():\n",
    "    en_stopwords = list(nltk.corpus.stopwords.words(\"english\"))\n",
    "    additional_list = [\"'s\",\"...\",\"'ve\",\"``\",\"''\",\"'m\",'--',\"'ll\",\"'d\"]\n",
    "    stopwords_ = set(en_stopwords + additional_list)\n",
    "    return stopwords_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(sentence, sw):\n",
    "    alphabet_ = alphabet(sentence)\n",
    "    tokenized_ = nltk.word_tokenize(alphabet_.lower())\n",
    "    stemmed_ = [st.stem(word) for word in tokenized_ if word not in sw]\n",
    "\n",
    "    return (stemmed_, len(stemmed_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence_list(sentence_list, allowed_length, sw):\n",
    "    np_log_ = []\n",
    "    return_ = []\n",
    "    for idx_, sentence_ in enumerate(sentence_list):\n",
    "        processed_, length_ = process_sentence(sentence_, sw)\n",
    "        if length_ > allowed_length:\n",
    "            return_.append(processed_)\n",
    "        else:\n",
    "            np_log_.append(processed_)\n",
    "\n",
    "    return return_, np_log_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_process_sentence_list(xml_list, length_allowed):\n",
    "    return_ = []\n",
    "    np_ = []\n",
    "    sw = liststopword()\n",
    "    for xml_, la_ in zip(xml_list, length_allowed):\n",
    "        processed_, np_log_ = process_sentence_list(xml_, la_, sw)\n",
    "        return_.append(processed_)\n",
    "        np_.append(np_log_)\n",
    "    \n",
    "    return return_, np_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sentence_list, log_np = get_process_sentence_list(xml_unique_list, length_allowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62348\n",
      "23730\n",
      "13385\n"
     ]
    }
   ],
   "source": [
    "for idx in range(0, len(p_sentence_list)): print (len(p_sentence_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "for idx in range(0, len(p_sentence_list)): label_list.append([idx] * len(p_sentence_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62348\n",
      "23730\n",
      "13385\n"
     ]
    }
   ],
   "source": [
    "for idx in range(0, len(label_list)): print (len(label_list[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_valid(aword):\n",
    "    return aword not in [\"\",\" \"]\n",
    "\n",
    "def create_vocab_listsentence(alist_sentence, amin_freq_allowed):\n",
    "    the_words = []\n",
    "    for sentence_list_ in alist_sentence:\n",
    "        for the_asentence in sentence_list_:\n",
    "            for the_aword in the_asentence:\n",
    "                the_words.append(the_aword)\n",
    "        the_words_freq = nltk.FreqDist(the_words)\n",
    "        the_vocab = []\n",
    "        for the_aword, the_afreq in the_words_freq.items():\n",
    "            if the_afreq > amin_freq_allowed:\n",
    "                if word_valid(the_aword):\n",
    "                    the_vocab.append(the_aword)\n",
    "\n",
    "    the_vocab_sorted = sorted(the_vocab)\n",
    "    #Assign a number corresponding to each word. Makes counting easier.\n",
    "    the_vocab_sorted_dict = dict(zip(the_vocab_sorted, range(len(the_vocab_sorted))))\n",
    "    return the_vocab_sorted, the_vocab_sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, vocab2id = create_vocab_listsentence(p_sentence_list, min_freq_allowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23162"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_, y_ = [], []\n",
    "for p_sentence, label_ in zip(p_sentence_list, label_list): \n",
    "    x_.extend(p_sentence)\n",
    "    y_.extend(label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_) == len(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y =  train_test_split(\n",
    "    x_, y_, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_vocab = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 200\n",
    "en1_units=100\n",
    "en2_units=100\n",
    "num_topic=3\n",
    "num_input=limited_vocab\n",
    "variance=0.995\n",
    "init_mult=1.0\n",
    "learning_rate=0.002\n",
    "batch_size=200\n",
    "momentum=0.99\n",
    "num_epoch=100\n",
    "nogpu=True\n",
    "drop_rate=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(o):\n",
    "    if o is None: return []\n",
    "    if isinstance(o, list): return o\n",
    "    if isinstance(o, str): return [o]\n",
    "    if isinstance(o, Iterable): return list(o)\n",
    "    return [o]\n",
    "def setify(o): return o if isinstance(o,set) else set(listify(o))\n",
    "def compose(x, funcs, *args, order_key='_order', **kwargs):\n",
    "    key = lambda o: getattr(o, order_key, 0)\n",
    "    for f in sorted(listify(funcs), key=key): x = f(x, **kwargs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_perp(model):\n",
    "    cost=[]\n",
    "    model.eval()                        # switch to testing mode\n",
    "    input_ = tensor_te\n",
    "    recon, loss = model(input_, compute_loss=True, avg_loss=False)\n",
    "    loss = loss.data\n",
    "    counts = tensor_te.sum(1)\n",
    "    avg = (loss / counts).mean()\n",
    "    print('The approximated perplexity is: ', math.exp(avg))\n",
    "    \n",
    "def print_perp(model):\n",
    "    cost=[]\n",
    "    model.eval()                        # switch to testing mode\n",
    "    input_, _ = next(iter(test_dl))\n",
    "    recon, loss = model(input_, compute_loss=True, avg_loss=False)\n",
    "    loss = loss.data\n",
    "    counts = input_.sum(1)\n",
    "    avg = (loss / counts).mean()\n",
    "    print('The approximated perplexity is: ', math.exp(avg))\n",
    "\n",
    "def print_top_words(beta, feature_names, n_top_words=10):\n",
    "    print ('---------------Printing the Topics------------------')\n",
    "    for i in range(len(beta)):\n",
    "        line = \" \".join([feature_names[j] \n",
    "                         for j in beta[i].argsort()[:-n_top_words - 1:-1]])\n",
    "        print('{}'.format(line))\n",
    "    print ('---------------End of Topics------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    x, y = zip(*b)\n",
    "    return torch.stack(x), torch.stack(y)\n",
    "\n",
    "class IdifyAndLimitedVocab():\n",
    "    _order=-1\n",
    "    def __init__(self, vocab2id, limited_vocab):\n",
    "        self.vocab2id = vocab2id\n",
    "        self.limited_vocab = limited_vocab\n",
    "    def __call__(self, item):\n",
    "        idlist = [self.vocab2id[w] for w in item if self.vocab2id[w] < limited_vocab]\n",
    "        return np.array(idlist)\n",
    "\n",
    "class Numpyify():\n",
    "    _order=0\n",
    "    def __call__(self, item):\n",
    "        return np.array(item)\n",
    "\n",
    "class Onehotify():\n",
    "    _order=1\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "    def __call__(self, item):\n",
    "        return np.array(np.bincount(item.astype('int'), minlength=self.vocab_size))\n",
    "    \n",
    "class YToOnehot():\n",
    "    _order=1\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "    def __call__(self, item):\n",
    "        categorical = np.zeros((1, self.num_classes))\n",
    "        categorical[0, item] = 1\n",
    "        return categorical\n",
    "\n",
    "class Tensorify():\n",
    "    _order=2\n",
    "    def __call__(self, item):\n",
    "        return torch.from_numpy(item)\n",
    "\n",
    "class Floatify():\n",
    "    _order=3\n",
    "    def __call__(self, item):\n",
    "        return item.float()\n",
    "    \n",
    "class CheckAndCudify():\n",
    "    _order=100\n",
    "    def __init__(self):\n",
    "        self.ic = torch.cuda.is_available()\n",
    "    def __call__(self, item):\n",
    "        return item.cuda() if self.ic else item\n",
    "    \n",
    "class URSADataset(Dataset):\n",
    "    def __init__(self, x, y, tfms_x, tfms_y): \n",
    "        self.x, self.y = x, y\n",
    "        self.x_tfms = tfms_x\n",
    "        self.y_tfms = tfms_y\n",
    "    def __len__(self): \n",
    "        return len(self.x)\n",
    "    def __getitem__(self, i): \n",
    "        return compose(self.x[i], self.x_tfms), compose(self.y[i], self.y_tfms)\n",
    "    \n",
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = np.max(train_y) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms_x = [IdifyAndLimitedVocab(vocab2id, limited_vocab), Numpyify(), Onehotify(vocab_size=limited_vocab), Tensorify(), Floatify(), CheckAndCudify()]\n",
    "tfms_y = [YToOnehot(num_classes=num_classes), Tensorify(), Floatify(), CheckAndCudify()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = URSADataset(train_x, train_y, tfms_x=tfms_x, tfms_y=tfms_y)\n",
    "test_ds = URSADataset(test_x, test_y, tfms_x=tfms_x, tfms_y=tfms_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = Sampler(train_ds, bs, shuffle=True)\n",
    "test_samp = Sampler(test_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n",
    "test_dl = DataLoader(test_ds, sampler=test_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "idify = IdifyAndLimitedVocab(vocab2id, limited_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(in_feature, hidden_feature1, hidden_feature2, drop_rate):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "                ('linear1', nn.Linear(in_feature, hidden_feature1)),\n",
    "                ('act1', nn.Softplus()),\n",
    "                ('linear2', nn.Linear(hidden_feature1, hidden_feature2)),\n",
    "                ('act2', nn.Softplus()),\n",
    "                ('dropout', nn.Dropout(drop_rate))\n",
    "            ]))\n",
    "\n",
    "def decoder(in_feature, out_feature, drop_rate):\n",
    "     return nn.Sequential(OrderedDict([\n",
    "                ('act1', nn.Softmax(dim=-1)),\n",
    "                ('dropout', nn.Dropout(drop_rate)),\n",
    "                ('linear', nn.Linear(in_feature, out_feature)),\n",
    "                ('batchnorm', nn.BatchNorm1d(out_feature)),\n",
    "                ('act2', nn.Softmax(dim=-1))\n",
    "            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden(in_feature, out_feature):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "                ('linear', nn.Linear(in_feature, out_feature)),\n",
    "                ('batchnorm', nn.BatchNorm1d(out_feature))\n",
    "            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProdLDA(nn.Module):\n",
    "    def __init__(self, num_input, en1_units, en2_units, num_topic, drop_rate, init_mult):\n",
    "        super(ProdLDA, self).__init__()\n",
    "        self.num_input, self.en1_units, self.en2_units, \\\n",
    "        self.num_topic, self.drop_rate, self.init_mult = num_input, en1_units, en2_units, \\\n",
    "                                                            num_topic, drop_rate, init_mult\n",
    "        # encoder\n",
    "        self.en = encoder(num_input, en1_units, en2_units, drop_rate)\n",
    "        self.mean = hidden(en2_units, num_topic)\n",
    "        self.logvar = hidden(en2_units, num_topic)\n",
    "        # decoder\n",
    "        self.de = decoder(num_topic, num_input, drop_rate)\n",
    "        # prior mean and variance as constant buffers\n",
    "        self.prior_mean   = torch.Tensor(1, num_topic).fill_(0)\n",
    "        self.prior_var    = torch.Tensor(1, num_topic).fill_(variance)\n",
    "        self.prior_mean   = nn.Parameter(self.prior_mean, requires_grad=False)\n",
    "        self.prior_var    = nn.Parameter(self.prior_var, requires_grad=False)\n",
    "        self.prior_logvar = nn.Parameter(self.prior_var.log(), requires_grad=False)\n",
    "        # initialize decoder weight\n",
    "        if init_mult != 0:\n",
    "            #std = 1. / math.sqrt( init_mult * (num_topic + num_input))\n",
    "            self.de.linear.weight.data.uniform_(0, init_mult)\n",
    "        # remove BN's scale parameters\n",
    "        for component in [self.mean, self.logvar, self.de]:\n",
    "            component.batchnorm.weight.requires_grad = False\n",
    "            component.batchnorm.weight.fill_(1.0)\n",
    "\n",
    "    def encode(self, input_):\n",
    "        encoded = self.en(input_)\n",
    "        posterior_mean = self.mean(encoded)\n",
    "        posterior_logvar = self.logvar(encoded)\n",
    "        return encoded, posterior_mean, posterior_logvar\n",
    "    \n",
    "    def decode(self, input_, posterior_mean, posterior_var):\n",
    "        # take sample\n",
    "        eps = input_.data.new().resize_as_(posterior_mean.data).normal_() # noise \n",
    "        z = posterior_mean + posterior_var.sqrt() * eps                   # reparameterization\n",
    "        # do reconstruction\n",
    "        recon = self.de(z)          # reconstructed distribution over vocabulary\n",
    "        return recon\n",
    "    \n",
    "    def forward(self, input_, compute_loss=False, avg_loss=True):\n",
    "        # compute posterior\n",
    "        en2, posterior_mean, posterior_logvar = self.encode(input_) \n",
    "        posterior_var    = posterior_logvar.exp()\n",
    "        \n",
    "        recon = self.decode(input_, posterior_mean, posterior_var)\n",
    "        if compute_loss:\n",
    "            return recon, self.loss(input_, recon, posterior_mean, posterior_logvar, posterior_var, avg_loss)\n",
    "        else:\n",
    "            return recon\n",
    "\n",
    "    def loss(self, input_, recon, posterior_mean, posterior_logvar, posterior_var, avg=True):\n",
    "        # NL\n",
    "        NL  = -(input_ * (recon + 1e-10).log()).sum(1)\n",
    "        # KLD, see Section 3.3 of Akash Srivastava and Charles Sutton, 2017, \n",
    "        # https://arxiv.org/pdf/1703.01488.pdf\n",
    "        prior_mean   = self.prior_mean.expand_as(posterior_mean)\n",
    "        prior_var    = self.prior_var.expand_as(posterior_mean)\n",
    "        prior_logvar = self.prior_logvar.expand_as(posterior_mean)\n",
    "        var_division    = posterior_var  / prior_var\n",
    "        diff            = posterior_mean - prior_mean\n",
    "        diff_term       = diff * diff / prior_var\n",
    "        logvar_division = prior_logvar - posterior_logvar\n",
    "        # put KLD together\n",
    "        KLD = 0.5 * ( (var_division + diff_term + logvar_division).sum(1) - self.num_topic)\n",
    "        # loss\n",
    "        loss = (NL + KLD)\n",
    "        # in traiming mode, return averaged loss. In testing mode, return individual loss\n",
    "        if avg:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProdLDA(num_input, en1_units, en2_units, num_topic, drop_rate, init_mult)\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate, betas=(momentum, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss=32.5855837317074\n",
      "Epoch 5, loss=25.479743570439958\n",
      "Epoch 10, loss=24.401526518429026\n",
      "Epoch 15, loss=24.179173935160918\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    loss_epoch = 0.0\n",
    "    model.train()                    # switch to training mode\n",
    "    for input_, _ in train_dl:\n",
    "        recon, loss = model(input_, compute_loss=True)\n",
    "        # optimize\n",
    "        optimizer.zero_grad()        # clear previous gradients\n",
    "        loss.backward()              # backprop\n",
    "        optimizer.step()             # update parameters\n",
    "        # report\n",
    "        loss_epoch += loss.item()    # add loss to loss_epoch\n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch {}, loss={}'.format(epoch, loss_epoch / len(input_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model.de.linear.weight.data.cpu().numpy().T\n",
    "print_top_words(emb, vocab, 20)\n",
    "print_perp(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avi",
   "language": "python",
   "name": "avi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
