{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import groupby\n",
    "from sklearn import datasets\n",
    "from numpy import random\n",
    "from scipy.stats import dirichlet, norm, poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.datasets import reuters, imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "import pickle, gzip, math, torch, matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URSA Datasets 10K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_ds_path = Path('./data/User Review Structure Analysis (URSA)/')\n",
    "xml_path = (folder_ds_path/'Classified_Corpus.xml')\n",
    "ds_path = (folder_ds_path/'10k')\n",
    "sentence_npy_path = (folder_ds_path/'sentence.npy')\n",
    "vocab_pkl_path = (folder_ds_path/'vocab.pkl')\n",
    "seed_words_path = (ds_path/'seed_words.txt')\n",
    "\n",
    "# log words not pass\n",
    "aspect_tags = ['Food', 'Staff', 'Ambience']\n",
    "polatiry_tags = ['Positive', 'Negative', 'Neutral']\n",
    "xml_review_tag = './/Review'\n",
    "log_np = [[], [], []]\n",
    "\n",
    "# length allowed sentences\n",
    "length_allowed = [11, 7, 4]\n",
    "min_freq_allowed = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URSA Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_nested_xml(axml):\n",
    "    return ' '.join([the_aiter for the_aiter in axml.itertext()])\n",
    "\n",
    "def get_firstchild(axml):\n",
    "    try:\n",
    "        if len(axml.getchildren()) > 0:\n",
    "            return axml.getchildren()[0].tag\n",
    "        else:\n",
    "            raise (Exception('ListIndex', 'aXmlElement input has no children.'))\n",
    "    except Exception as e:\n",
    "        print (str(e))\n",
    "\n",
    "def xml_unique_valid(axml, alist_tag_allowed):\n",
    "    return (len(axml.getchildren()) == 0) or (get_firstchild(axml) in alist_tag_allowed)\n",
    "\n",
    "def xml_name_valid(axml, atag_name):\n",
    "    return axml.tag == atag_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listsentence_unique(alist_xml, alist_tag_allowed):\n",
    "    the_listsentence = []\n",
    "    for the_axml in alist_xml:\n",
    "        if xml_unique_valid(the_axml, alist_tag_allowed):\n",
    "            the_listsentence.append(string_nested_xml(the_axml))\n",
    "\n",
    "    return the_listsentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listxml_child(list_xml, tag):\n",
    "    return_ = []\n",
    "    for xml_ in list_xml:\n",
    "        for xml_child in xml_:\n",
    "            if xml_name_valid(xml_child, tag):\n",
    "                return_.append(xml_child)\n",
    "\n",
    "    return return_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listxml_child_list(document_list, tag_list):\n",
    "    return_ = []\n",
    "    for tag in tag_list:\n",
    "        xml_children = get_listxml_child(document_list, tag)\n",
    "        return_.append(xml_children)\n",
    "    \n",
    "    return return_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xml_unique_list(xml_children_list, polatiry_tags):\n",
    "    return_ = []\n",
    "    for xml_children in xml_children_list:\n",
    "        xml_unique = get_listsentence_unique(xml_children, polatiry_tags)\n",
    "        return_.append(xml_unique)\n",
    "    \n",
    "    return return_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tree = ET.parse(xml_path)\n",
    "corpus_root = corpus_tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_list = corpus_root.findall(xml_review_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96235\n",
      "32892\n",
      "16803\n"
     ]
    }
   ],
   "source": [
    "xml_children_list = get_listxml_child_list(document_list, aspect_tags)\n",
    "for idx in range(0, len(xml_children_list)): print (len(xml_children_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62348\n",
      "23730\n",
      "13385\n"
     ]
    }
   ],
   "source": [
    "xml_unique_list = get_xml_unique_list(xml_children_list, polatiry_tags)\n",
    "for idx in range(0, len(xml_unique_list)): print (len(xml_unique_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The food is a melding of Moroccan comfort food and Spanish tapas fare : tagines , stews and salads , with surprises like baby eggplants and olives where you might not expect them . '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_unique_list[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/huylb314/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/huylb314/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabet(atext):\n",
    "    return re.sub(\"[^a-zA-Z]\", \" \", atext)\n",
    "\n",
    "def liststopword():\n",
    "    en_stopwords = list(nltk.corpus.stopwords.words(\"english\"))\n",
    "    additional_list = [\"'s\",\"...\",\"'ve\",\"``\",\"''\",\"'m\",'--',\"'ll\",\"'d\"]\n",
    "    stopwords_ = set(en_stopwords + additional_list)\n",
    "    return stopwords_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(sentence, sw):\n",
    "    alphabet_ = alphabet(sentence)\n",
    "    tokenized_ = nltk.word_tokenize(alphabet_.lower())\n",
    "    stemmed_ = [st.stem(word) for word in tokenized_ if word not in sw]\n",
    "\n",
    "    return (stemmed_, len(stemmed_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence_list(sentence_list, allowed_length, sw):\n",
    "    np_log_ = []\n",
    "    return_ = []\n",
    "    for idx_, sentence_ in enumerate(sentence_list):\n",
    "        processed_, length_ = process_sentence(sentence_, sw)\n",
    "        if length_ > allowed_length:\n",
    "            return_.append(processed_)\n",
    "        else:\n",
    "            np_log_.append(processed_)\n",
    "\n",
    "    return return_, np_log_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_process_sentence_list(xml_list, length_allowed):\n",
    "    return_ = []\n",
    "    np_ = []\n",
    "    sw = liststopword()\n",
    "    for xml_, la_ in zip(xml_list, length_allowed):\n",
    "        processed_, np_log_ = process_sentence_list(xml_, la_, sw)\n",
    "        return_.append(processed_)\n",
    "        np_.append(np_log_)\n",
    "    \n",
    "    return return_, np_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sentence_list, log_np = get_process_sentence_list(xml_unique_list, length_allowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11547\n",
      "10175\n",
      "10640\n"
     ]
    }
   ],
   "source": [
    "for idx in range(0, len(p_sentence_list)): print (len(p_sentence_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "for idx in range(0, len(p_sentence_list)): label_list.append([idx] * len(p_sentence_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11547\n",
      "10175\n",
      "10640\n"
     ]
    }
   ],
   "source": [
    "for idx in range(0, len(label_list)): print (len(label_list[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_valid(aword):\n",
    "    return aword not in [\"\",\" \"]\n",
    "\n",
    "def create_vocab_listsentence(alist_sentence, amin_freq_allowed):\n",
    "    the_words = []\n",
    "    for sentence_list_ in alist_sentence:\n",
    "        for the_asentence in sentence_list_:\n",
    "            for the_aword in the_asentence:\n",
    "                the_words.append(the_aword)\n",
    "        the_words_freq = nltk.FreqDist(the_words)\n",
    "        the_vocab = []\n",
    "        for the_aword, the_afreq in the_words_freq.items():\n",
    "            if the_afreq > amin_freq_allowed:\n",
    "                if word_valid(the_aword):\n",
    "                    the_vocab.append(the_aword)\n",
    "\n",
    "    the_vocab_sorted = sorted(the_vocab)\n",
    "    #Assign a number corresponding to each word. Makes counting easier.\n",
    "    the_vocab_sorted_dict = dict(zip(the_vocab_sorted, range(len(the_vocab_sorted))))\n",
    "    return the_vocab_sorted, the_vocab_sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, vocab2id = create_vocab_listsentence(p_sentence_list, min_freq_allowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_, y_ = [], []\n",
    "for p_sentence, label_ in zip(p_sentence_list, label_list): \n",
    "    x_.extend(p_sentence)\n",
    "    y_.extend(label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_) == len(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y =  train_test_split(\n",
    "    x_, y_, test_size=0.9, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded\n",
      "Dim Training Data 3236 18073\n",
      "Dim Test Data 29126 18073\n"
     ]
    }
   ],
   "source": [
    "print ('Data Loaded')\n",
    "print ('Dim Training Data',len(train_x), vocab_size)\n",
    "print ('Dim Test Data', len(test_x), vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 200\n",
    "en1_units=100\n",
    "en2_units=100\n",
    "num_topic=3\n",
    "num_input=vocab_size\n",
    "variance=0.995\n",
    "init_mult=1.0\n",
    "learning_rate=0.002\n",
    "batch_size=200\n",
    "momentum=0.99\n",
    "num_epoch=100\n",
    "nogpu=True\n",
    "drop_rate=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_seed_words(fn):\n",
    "    with open(fn, \"r\") as fr:\n",
    "        def p_string_sw(l):\n",
    "            return l.replace('\\n','').split(',')\n",
    "        rl = [p_string_sw(l) for l in fr]\n",
    "    return rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_words = read_file_seed_words(seed_words_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = np.zeros((vocab_size, num_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18073, 3)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_bin = np.zeros((batch_size, vocab_size, num_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 18073, 3)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_bin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seed_words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['staff',\n",
       " 'servic',\n",
       " 'friendli',\n",
       " 'rude',\n",
       " 'hostess',\n",
       " 'waiter',\n",
       " 'bartend',\n",
       " 'waitress',\n",
       " 'help',\n",
       " 'polit',\n",
       " 'bar',\n",
       " 'courteou',\n",
       " 'member',\n",
       " 'waitstaff',\n",
       " 'attitud',\n",
       " 'reserv',\n",
       " 'tip']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(seed_words)): # number of topics\n",
    "    for idx in range(len(seed_words[k])): # number of words\n",
    "        ivocab = vocab2id[seed_words[k][idx]]\n",
    "        gamma[ivocab, k] = 1.0\n",
    "        gamma_bin[:, ivocab, :] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_prior(fn, n_k=3):\n",
    "    gamma = torch.zeros((len(vocab),n_k))\n",
    "    gamma_bin = torch.zeros((batch_size, len(vocab),n_k))\n",
    "\n",
    "    full_vocab = read_file_seed_words(fn)\n",
    "    for k in range(len(full_vocab)):\n",
    "        for idx in range(len(full_vocab[k])):\n",
    "            ivocab = vocab2id[full_vocab[k][idx]]\n",
    "            gamma[ivocab, k] = 1.0\n",
    "            gamma_bin[:, ivocab, :] = 1.0\n",
    "\n",
    "    return (gamma, gamma_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(o):\n",
    "    if o is None: return []\n",
    "    if isinstance(o, list): return o\n",
    "    if isinstance(o, str): return [o]\n",
    "    if isinstance(o, Iterable): return list(o)\n",
    "    return [o]\n",
    "def setify(o): return o if isinstance(o,set) else set(listify(o))\n",
    "def compose(x, funcs, *args, order_key='_order', **kwargs):\n",
    "    key = lambda o: getattr(o, order_key, 0)\n",
    "    for f in sorted(listify(funcs), key=key): x = f(x, **kwargs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_perp(model):\n",
    "    cost=[]\n",
    "    model.eval()                        # switch to testing mode\n",
    "    input_ = tensor_te\n",
    "    recon, loss = model(input_, compute_loss=True, avg_loss=False)\n",
    "    loss = loss.data\n",
    "    counts = tensor_te.sum(1)\n",
    "    avg = (loss / counts).mean()\n",
    "    print('The approximated perplexity is: ', math.exp(avg))\n",
    "    \n",
    "def print_perp(model):\n",
    "    cost=[]\n",
    "    model.eval()                        # switch to testing mode\n",
    "    input_, _ = next(iter(test_dl))\n",
    "    recon, loss = model(input_, compute_loss=True, avg_loss=False)\n",
    "    loss = loss.data\n",
    "    counts = input_.sum(1)\n",
    "    avg = (loss / counts).mean()\n",
    "    print('The approximated perplexity is: ', math.exp(avg))\n",
    "\n",
    "def print_top_words(beta, feature_names, n_top_words=10):\n",
    "    print ('---------------Printing the Topics------------------')\n",
    "    for i in range(len(beta)):\n",
    "        line = \" \".join([feature_names[j] \n",
    "                         for j in beta[i].argsort()[:-n_top_words - 1:-1]])\n",
    "        print('{}'.format(line))\n",
    "    print ('---------------End of Topics------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    x, y = zip(*b)\n",
    "    return torch.stack(x), torch.stack(y)\n",
    "\n",
    "class IdifyAndLimitedVocab():\n",
    "    _order=-1\n",
    "    def __init__(self, vocab2id, limited_vocab):\n",
    "        self.vocab2id = vocab2id\n",
    "        self.limited_vocab = limited_vocab\n",
    "    def __call__(self, item):\n",
    "        idlist = [self.vocab2id[w] for w in item if self.vocab2id[w] < self.limited_vocab]\n",
    "        return np.array(idlist)\n",
    "    \n",
    "\n",
    "class Numpyify():\n",
    "    _order=0\n",
    "    def __call__(self, item):\n",
    "        return np.array(item)\n",
    "\n",
    "class Onehotify():\n",
    "    _order=1\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "    def __call__(self, item):\n",
    "        return np.array(np.bincount(item.astype('int'), minlength=self.vocab_size))\n",
    "    \n",
    "class YToOnehot():\n",
    "    _order=1\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "    def __call__(self, item):\n",
    "        categorical = np.zeros((1, self.num_classes))\n",
    "        categorical[0, item] = 1\n",
    "        return categorical\n",
    "\n",
    "class Tensorify():\n",
    "    _order=2\n",
    "    def __call__(self, item):\n",
    "        return torch.from_numpy(item)\n",
    "\n",
    "class Floatify():\n",
    "    _order=3\n",
    "    def __call__(self, item):\n",
    "        return item.float()\n",
    "    \n",
    "class CheckAndCudify():\n",
    "    _order=100\n",
    "    def __init__(self):\n",
    "        self.ic = torch.cuda.is_available()\n",
    "    def __call__(self, item):\n",
    "        return item.cuda() if self.ic else item\n",
    "    \n",
    "class URSADataset(Dataset):\n",
    "    def __init__(self, x, y, tfms_x, tfms_y): \n",
    "        self.x, self.y = x, y\n",
    "        self.x_tfms = tfms_x\n",
    "        self.y_tfms = tfms_y\n",
    "    def __len__(self): \n",
    "        return len(self.x)\n",
    "    def __getitem__(self, i): \n",
    "        return compose(self.x[i], self.x_tfms), compose(self.y[i], self.y_tfms)\n",
    "    \n",
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = np.max(train_y) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms_x = [IdifyAndLimitedVocab(vocab2id, vocab_size), Numpyify(), Onehotify(vocab_size=vocab_size), Tensorify(), Floatify(), CheckAndCudify()]\n",
    "tfms_y = [YToOnehot(num_classes=num_classes), Tensorify(), Floatify(), CheckAndCudify()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = URSADataset(train_x, train_y, tfms_x=tfms_x, tfms_y=tfms_y)\n",
    "test_ds = URSADataset(test_x, test_y, tfms_x=tfms_x, tfms_y=tfms_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = Sampler(train_ds, bs, shuffle=False)\n",
    "test_samp = Sampler(test_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n",
    "test_dl = DataLoader(test_ds, sampler=test_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "abatchx, _  = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 18073])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abatchx.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma, gamma_bin = setup_prior(seed_words_path, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 18073, 3])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_bin.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (abatchx > 0).unsqueeze(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 18073, 1])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 18073, 3])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((gamma_bin == 1) & t).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(in_feature, hidden_feature1, hidden_feature2, drop_rate):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "                ('linear1', nn.Linear(in_feature, hidden_feature1)),\n",
    "                ('act1', nn.Softplus()),\n",
    "                ('linear2', nn.Linear(hidden_feature1, hidden_feature2)),\n",
    "                ('act2', nn.Softplus()),\n",
    "                ('dropout', nn.Dropout(drop_rate))\n",
    "            ]))\n",
    "\n",
    "def decoder(in_feature, out_feature, drop_rate):\n",
    "     return nn.Sequential(OrderedDict([\n",
    "                ('act1', nn.Softmax(dim=-1)),\n",
    "                ('dropout', nn.Dropout(drop_rate)),\n",
    "                ('linear', nn.Linear(in_feature, out_feature)),\n",
    "                ('batchnorm', nn.BatchNorm1d(out_feature)),\n",
    "                ('act2', nn.Softmax(dim=-1))\n",
    "            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden(in_feature, out_feature):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "                ('linear', nn.Linear(in_feature, out_feature)),\n",
    "                ('batchnorm', nn.BatchNorm1d(out_feature))\n",
    "            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProdLDA(nn.Module):\n",
    "    def __init__(self, num_input, en1_units, en2_units, num_topic, drop_rate, init_mult):\n",
    "        super(ProdLDA, self).__init__()\n",
    "        self.num_input, self.en1_units, self.en2_units, \\\n",
    "        self.num_topic, self.drop_rate, self.init_mult = num_input, en1_units, en2_units, \\\n",
    "                                                            num_topic, drop_rate, init_mult\n",
    "        # encoder\n",
    "        self.en = encoder(num_input, en1_units, en2_units, drop_rate)\n",
    "        self.mean = hidden(en2_units, num_topic)\n",
    "        self.logvar = hidden(en2_units, num_topic)\n",
    "        # decoder\n",
    "        self.de = decoder(num_topic, num_input, drop_rate)\n",
    "        # prior mean and variance as constant buffers\n",
    "        self.prior_mean   = torch.Tensor(1, num_topic).fill_(0)\n",
    "        self.prior_var    = torch.Tensor(1, num_topic).fill_(variance)\n",
    "        self.prior_mean   = nn.Parameter(self.prior_mean, requires_grad=False)\n",
    "        self.prior_var    = nn.Parameter(self.prior_var, requires_grad=False)\n",
    "        self.prior_logvar = nn.Parameter(self.prior_var.log(), requires_grad=False)\n",
    "        # initialize decoder weight\n",
    "        if init_mult != 0:\n",
    "            #std = 1. / math.sqrt( init_mult * (num_topic + num_input))\n",
    "            self.de.linear.weight.data.uniform_(0, init_mult)\n",
    "        # remove BN's scale parameters\n",
    "        for component in [self.mean, self.logvar, self.de]:\n",
    "            component.batchnorm.weight.requires_grad = False\n",
    "            component.batchnorm.weight.fill_(1.0)\n",
    "\n",
    "    def gamma(self):\n",
    "        # this function have to run after self.encode\n",
    "        encoder_w1 = self.en.linear1.weight\n",
    "        encoder_b1 = self.en.linear1.bias\n",
    "        encoder_w2 = self.en.linear2.weight\n",
    "        encoder_b2 = self.en.linear2.bias\n",
    "        mean_w = self.mean.linear.weight\n",
    "        mean_b = self.mean.linear.bias\n",
    "        mean_running_mean = self.mean.batchnorm.running_mean\n",
    "        mean_running_var = self.mean.batchnorm.running_var\n",
    "        logvar_w = self.logvar.linear.weight\n",
    "        logvar_b = self.logvar.linear.bias\n",
    "        logvar_running_mean = self.logvar.batchnorm.running_mean\n",
    "        logvar_running_var = self.logvar.batchnorm.running_var\n",
    "        \n",
    "        w1 = F.softplus(encoder_w1.t() + encoder_b1)\n",
    "        w2 = F.softplus(F.linear(w1, encoder_w2, encoder_b2))\n",
    "        wdr = F.dropout(w2, self.drop_rate)\n",
    "        wo_mean = F.softmax(F.batch_norm(F.linear(wdr, mean_w, mean_b), mean_running_mean, mean_running_var), dim=-1)\n",
    "        wo_logvar = F.softmax(F.batch_norm(F.linear(wdr, logvar_w, logvar_b), logvar_running_mean, logvar_running_var), dim=-1)\n",
    "        \n",
    "        print (\"wo_mean: {}\".format(wo_mean[0]))\n",
    "        print (\"gamma_mean: {}\".format(wo_mean.size()))\n",
    "        print (\"gamma_logvar: {}\".format(wo_logvar.size()))\n",
    "        return wo_mean, wo_logvar\n",
    "            \n",
    "    def encode(self, input_):\n",
    "        encoded = self.en(input_)\n",
    "        posterior_mean = self.mean(encoded)\n",
    "        posterior_logvar = self.logvar(encoded)\n",
    "        return encoded, posterior_mean, posterior_logvar\n",
    "    \n",
    "    def decode(self, input_, posterior_mean, posterior_var):\n",
    "        # take sample\n",
    "        eps = input_.data.new().resize_as_(posterior_mean.data).normal_() # noise \n",
    "        z = posterior_mean + posterior_var.sqrt() * eps                   # reparameterization\n",
    "        # do reconstruction\n",
    "        recon = self.de(z)          # reconstructed distribution over vocabulary\n",
    "        return recon\n",
    "    \n",
    "    def forward(self, input_, compute_loss=False, avg_loss=True):\n",
    "        # compute posterior\n",
    "        en2, posterior_mean, posterior_logvar = self.encode(input_) \n",
    "        posterior_var    = posterior_logvar.exp()\n",
    "        \n",
    "        recon = self.decode(input_, posterior_mean, posterior_var)\n",
    "        if compute_loss:\n",
    "            return recon, self.loss(input_, recon, posterior_mean, posterior_logvar, posterior_var, avg_loss)\n",
    "        else:\n",
    "            return recon\n",
    "\n",
    "    def loss(self, input_, recon, posterior_mean, posterior_logvar, posterior_var, avg=True):\n",
    "        # NL\n",
    "        NL  = -(input_ * (recon + 1e-10).log()).sum(1)\n",
    "        # KLD, see Section 3.3 of Akash Srivastava and Charles Sutton, 2017, \n",
    "        # https://arxiv.org/pdf/1703.01488.pdf\n",
    "        prior_mean   = self.prior_mean.expand_as(posterior_mean)\n",
    "        prior_var    = self.prior_var.expand_as(posterior_mean)\n",
    "        prior_logvar = self.prior_logvar.expand_as(posterior_mean)\n",
    "        var_division    = posterior_var  / prior_var\n",
    "        diff            = posterior_mean - prior_mean\n",
    "        diff_term       = diff * diff / prior_var\n",
    "        logvar_division = prior_logvar - posterior_logvar\n",
    "        # put KLD together\n",
    "        KLD = 0.5 * ( (var_division + diff_term + logvar_division).sum(1) - self.num_topic)\n",
    "        # loss\n",
    "        loss = (NL + KLD)\n",
    "        \n",
    "        self.gamma()\n",
    "        \n",
    "        # in traiming mode, return averaged loss. In testing mode, return individual loss\n",
    "        if avg:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProdLDA(num_input, en1_units, en2_units, num_topic, drop_rate, init_mult)\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate, betas=(momentum, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wo_mean: tensor([0.2799, 0.2894, 0.4306], grad_fn=<SelectBackward>)\n",
      "gamma_mean: torch.Size([18073, 3])\n",
      "gamma_logvar: torch.Size([18073, 3])\n",
      "Epoch 0, loss=0.6498851013183594\n",
      "---------------Printing the Topics------------------\n",
      "staff steak outsid entre foodist cole hardcor monro boat amaron testicl lousi frenchwoman proffer jame whereupon deconstruct peynirli reat chindian at authit deferenti specjal windowsil lumpia pseudo huachinango kanzuri monther serrano fixin blond child pleasingli cutesi lanter savarin valrhona kassel atmo across sate place teow poppyse squab ragout ottaman pack\n",
      "occas girl ask mosaic waaaaaaay pumpkin fest strongest cobb repeat gianduja blase woodburn bogart banxiao decent elect morsel krystal snarlli chowder waitservic roti forno zarela mei modernist absoulut pointless wizard vp rel complai heidelberg robitussin je stormi aris bocca atrium ill spinkl lancast unrel wrought duct guayabera jd bib manhattan\n",
      "asparagu return yell ny bellavita papardel curv sea refer lconsist heart morass crawfish cojita proprietor cranberri pastitsio lattic marley surpriz spawn kenni lenni centuri pepeer daunt olieng aploget gianyotiko withachara turnov multitud fregola coe sopresata abli unrip corneliu appli obn althougth fiorello scattershot verg coax taro ooh snippi disappear frivol\n",
      "---------------End of Topics------------------\n",
      "wo_mean: tensor([0.3087, 0.1678, 0.5235], grad_fn=<SelectBackward>)\n",
      "gamma_mean: torch.Size([18073, 3])\n",
      "gamma_logvar: torch.Size([18073, 3])\n",
      "The approximated perplexity is:  20066.058027062012\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    loss_epoch = 0.0\n",
    "    model.train()                    # switch to training mode\n",
    "    for input_, _ in train_dl:\n",
    "        recon, loss = model(input_, compute_loss=True)\n",
    "        # optimize\n",
    "        optimizer.zero_grad()        # clear previous gradients\n",
    "        loss.backward()              # backprop\n",
    "        optimizer.step()             # update parameters\n",
    "        # report\n",
    "        loss_epoch += loss.item()    # add loss to loss_epoch\n",
    "        break\n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch {}, loss={}'.format(epoch, loss_epoch / len(input_)))\n",
    "        emb = model.de.linear.weight.data.detach().cpu().numpy().T\n",
    "        print_top_words(emb, vocab, 50)\n",
    "        print_perp(model)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Printing the Topics------------------\n",
      "coconut bland stew rib shrimp green tasti fresh yellowtail veget oyster wrap ravioli meati brioch simpl sashimi assort ingredi mackerel app rabbit ginger chewi salad carpaccio serv beef roll tast vanilla benedict foie grandma standout yuzu size spice cream pickl walnut naan rice stuf mixtur dri mango asparagu tart cheddar\n",
      "time look sens luger design arch abund nobl pub sop ambienc iron seven limit bistro cement squar suburb travel rule new chandeli tall scandinavian happili empir hesser mahogani eas mathur downtown brick reminisc wallac calm noisi bento eleg tablecloth slope dine predecessor dissappear frustat irish undul regardless swoop interior ivori\n",
      "us would romant next beverag kept came accomod unaccommod somewher sweater staf brusqu atleast crowd retro friend cozier paid waitress warm wew stool trunk left children great farmhous nobodi boutiq oxiou decemb bra endless event comfort heat bangalor half romanit impati ismoodi memori speacial school place count underground maitr self\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1993.9411619432524\n"
     ]
    }
   ],
   "source": [
    "emb = model.de.linear.weight.data.cpu().numpy().T\n",
    "print_top_words(emb, vocab, 50)\n",
    "print_perp(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avi",
   "language": "python",
   "name": "avi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
