{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters, imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "import pickle, gzip, math, torch, matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 2000\n",
    "maxlen = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 200\n",
    "en1_units=100\n",
    "en2_units=100\n",
    "num_topic=50\n",
    "num_input=max_words\n",
    "variance=0.995\n",
    "init_mult=1.0\n",
    "learning_rate=0.002\n",
    "batch_size=200\n",
    "momentum=0.99\n",
    "num_epoch=100\n",
    "nogpu=True\n",
    "drop_rate=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(o):\n",
    "    if o is None: return []\n",
    "    if isinstance(o, list): return o\n",
    "    if isinstance(o, str): return [o]\n",
    "    if isinstance(o, Iterable): return list(o)\n",
    "    return [o]\n",
    "def setify(o): return o if isinstance(o,set) else set(listify(o))\n",
    "def compose(x, funcs, *args, order_key='_order', **kwargs):\n",
    "    key = lambda o: getattr(o, order_key, 0)\n",
    "    for f in sorted(listify(funcs), key=key): x = f(x, **kwargs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_perp(model):\n",
    "    cost=[]\n",
    "    model.eval()                        # switch to testing mode\n",
    "    input_ = tensor_te\n",
    "    recon, loss = model(input_, compute_loss=True, avg_loss=False)\n",
    "    loss = loss.data\n",
    "    counts = tensor_te.sum(1)\n",
    "    avg = (loss / counts).mean()\n",
    "    print('The approximated perplexity is: ', math.exp(avg))\n",
    "    \n",
    "def print_perp(model):\n",
    "    cost=[]\n",
    "    model.eval()                        # switch to testing mode\n",
    "    input_, _ = next(iter(test_dl))\n",
    "    recon, loss = model(input_, compute_loss=True, avg_loss=False)\n",
    "    loss = loss.data\n",
    "    counts = input_.sum(1)\n",
    "    avg = (loss / counts).mean()\n",
    "    print('The approximated perplexity is: ', math.exp(avg))\n",
    "\n",
    "def print_top_words(beta, feature_names, n_top_words=10):\n",
    "    print ('---------------Printing the Topics------------------')\n",
    "    for i in range(len(beta)):\n",
    "        line = \" \".join([feature_names[j] \n",
    "                         for j in beta[i].argsort()[:-n_top_words - 1:-1]])\n",
    "        print('{}'.format(line))\n",
    "    print ('---------------End of Topics------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    x, y = zip(*b)\n",
    "    return torch.stack(x), torch.stack(y)\n",
    "\n",
    "class Numpyify():\n",
    "    _order=0\n",
    "    def __call__(self, item):\n",
    "        return np.array(item)\n",
    "\n",
    "class Onehotify():\n",
    "    _order=1\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "    def __call__(self, item):\n",
    "        return np.array(np.bincount(item.astype('int'), minlength=self.vocab_size))\n",
    "    \n",
    "class YToOnehot():\n",
    "    _order=1\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "    def __call__(self, item):\n",
    "        categorical = np.zeros((1, self.num_classes))\n",
    "        categorical[0, item] = 1\n",
    "        return categorical\n",
    "\n",
    "class Tensorify():\n",
    "    _order=2\n",
    "    def __call__(self, item):\n",
    "        return torch.from_numpy(item)\n",
    "\n",
    "class Floatify():\n",
    "    _order=3\n",
    "    def __call__(self, item):\n",
    "        return item.float()\n",
    "    \n",
    "class CheckAndCudify():\n",
    "    _order=100\n",
    "    def __init__(self):\n",
    "        self.ic = torch.cuda.is_available()\n",
    "    def __call__(self, item):\n",
    "        return item.cuda() if self.ic else item\n",
    "    \n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, x, y, tfms_x, tfms_y): \n",
    "        self.x, self.y = x, y\n",
    "        self.x_tfms = tfms_x\n",
    "        self.y_tfms = tfms_y\n",
    "    def __len__(self): \n",
    "        return len(self.x)\n",
    "    def __getitem__(self, i): \n",
    "        return compose(self.x[i], self.x_tfms), compose(self.y[i], self.y_tfms)\n",
    "    \n",
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_vocab(index_from=3):\n",
    "    word_to_id = imdb.get_word_index()\n",
    "    word_to_id = {k:(v + index_from) for k,v in word_to_id.items()}\n",
    "    word_to_id[\"<PAD>\"] = 0\n",
    "    word_to_id[\"<START>\"] = 1\n",
    "    word_to_id[\"<UNK>\"] = 2\n",
    "    word_to_id[\"<FCK>\"] = 3\n",
    "    \n",
    "    id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "\n",
    "    return id_to_word, word_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, id_vocab = imdb_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = imdb.load_data(num_words=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = np.max(train_y) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms_x = [Numpyify(), Onehotify(vocab_size=max_words), Tensorify(), Floatify(), CheckAndCudify()]\n",
    "tfms_y = [YToOnehot(num_classes=num_classes), Tensorify(), Floatify(), CheckAndCudify()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = IMDBDataset(train_x, train_y, tfms_x=tfms_x, tfms_y=tfms_y)\n",
    "test_ds = IMDBDataset(test_x, test_y, tfms_x=tfms_x, tfms_y=tfms_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = Sampler(train_ds, bs, shuffle=True)\n",
    "test_samp = Sampler(test_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n",
    "test_dl = DataLoader(test_ds, sampler=test_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(in_feature, hidden_feature1, hidden_feature2, drop_rate):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "                ('linear1', nn.Linear(in_feature, hidden_feature1)),\n",
    "                ('act1', nn.Softplus()),\n",
    "                ('linear2', nn.Linear(hidden_feature1, hidden_feature2)),\n",
    "                ('act2', nn.Softplus()),\n",
    "                ('dropout', nn.Dropout(drop_rate))\n",
    "            ]))\n",
    "\n",
    "def decoder(in_feature, out_feature, drop_rate):\n",
    "     return nn.Sequential(OrderedDict([\n",
    "                ('act1', nn.Softmax(dim=-1)),\n",
    "                ('dropout', nn.Dropout(drop_rate)),\n",
    "                ('linear', nn.Linear(in_feature, out_feature)),\n",
    "                ('batchnorm', nn.BatchNorm1d(out_feature)),\n",
    "                ('act2', nn.Softmax(dim=-1))\n",
    "            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden(in_feature, out_feature):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "                ('linear', nn.Linear(in_feature, out_feature)),\n",
    "                ('batchnorm', nn.BatchNorm1d(out_feature))\n",
    "            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProdLDA(nn.Module):\n",
    "    def __init__(self, num_input, en1_units, en2_units, num_topic, drop_rate, init_mult):\n",
    "        super(ProdLDA, self).__init__()\n",
    "        self.num_input, self.en1_units, self.en2_units, \\\n",
    "        self.num_topic, self.drop_rate, self.init_mult = num_input, en1_units, en2_units, \\\n",
    "                                                            num_topic, drop_rate, init_mult\n",
    "        # encoder\n",
    "        self.en = encoder(num_input, en1_units, en2_units, drop_rate)\n",
    "        self.mean = hidden(en2_units, num_topic)\n",
    "        self.logvar = hidden(en2_units, num_topic)\n",
    "        # decoder\n",
    "        self.de = decoder(num_topic, num_input, drop_rate)\n",
    "        # prior mean and variance as constant buffers\n",
    "        self.prior_mean   = torch.Tensor(1, num_topic).fill_(0)\n",
    "        self.prior_var    = torch.Tensor(1, num_topic).fill_(variance)\n",
    "        self.prior_mean   = nn.Parameter(self.prior_mean, requires_grad=False)\n",
    "        self.prior_var    = nn.Parameter(self.prior_var, requires_grad=False)\n",
    "        self.prior_logvar = nn.Parameter(self.prior_var.log(), requires_grad=False)\n",
    "        # initialize decoder weight\n",
    "        if init_mult != 0:\n",
    "            #std = 1. / math.sqrt( init_mult * (num_topic + num_input))\n",
    "            self.de.linear.weight.data.uniform_(0, init_mult)\n",
    "        # remove BN's scale parameters\n",
    "        for component in [self.mean, self.logvar, self.de]:\n",
    "            component.batchnorm.weight.requires_grad = False\n",
    "            component.batchnorm.weight.fill_(1.0)\n",
    "\n",
    "    def encode(self, input_):\n",
    "        encoded = self.en(input_)\n",
    "        posterior_mean = self.mean(encoded)\n",
    "        posterior_logvar = self.logvar(encoded)\n",
    "        return encoded, posterior_mean, posterior_logvar\n",
    "    \n",
    "    def decode(self, input_, posterior_mean, posterior_var):\n",
    "        # take sample\n",
    "        eps = input_.data.new().resize_as_(posterior_mean.data).normal_() # noise \n",
    "        z = posterior_mean + posterior_var.sqrt() * eps                   # reparameterization\n",
    "        # do reconstruction\n",
    "        recon = self.de(z)          # reconstructed distribution over vocabulary\n",
    "        return recon\n",
    "    \n",
    "    def forward(self, input_, compute_loss=False, avg_loss=True):\n",
    "        # compute posterior\n",
    "        en2, posterior_mean, posterior_logvar = self.encode(input_) \n",
    "        posterior_var    = posterior_logvar.exp()\n",
    "        \n",
    "        recon = self.decode(input_, posterior_mean, posterior_var)\n",
    "        if compute_loss:\n",
    "            return recon, self.loss(input_, recon, posterior_mean, posterior_logvar, posterior_var, avg_loss)\n",
    "        else:\n",
    "            return recon\n",
    "\n",
    "    def loss(self, input_, recon, posterior_mean, posterior_logvar, posterior_var, avg=True):\n",
    "        # NL\n",
    "        NL  = -(input_ * (recon + 1e-10).log()).sum(1)\n",
    "        # KLD, see Section 3.3 of Akash Srivastava and Charles Sutton, 2017, \n",
    "        # https://arxiv.org/pdf/1703.01488.pdf\n",
    "        prior_mean   = self.prior_mean.expand_as(posterior_mean)\n",
    "        prior_var    = self.prior_var.expand_as(posterior_mean)\n",
    "        prior_logvar = self.prior_logvar.expand_as(posterior_mean)\n",
    "        var_division    = posterior_var  / prior_var\n",
    "        diff            = posterior_mean - prior_mean\n",
    "        diff_term       = diff * diff / prior_var\n",
    "        logvar_division = prior_logvar - posterior_logvar\n",
    "        # put KLD together\n",
    "        KLD = 0.5 * ( (var_division + diff_term + logvar_division).sum(1) - self.num_topic)\n",
    "        # loss\n",
    "        loss = (NL + KLD)\n",
    "        # in traiming mode, return averaged loss. In testing mode, return individual loss\n",
    "        if avg:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProdLDA(num_input, en1_units, en2_units, num_topic, drop_rate, init_mult)\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate, betas=(momentum, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    optimizer = optimizer.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss=1124.9376000976563\n",
      "Epoch 5, loss=908.3554571533203\n",
      "Epoch 10, loss=843.8213909912109\n",
      "Epoch 15, loss=820.7206262207031\n",
      "Epoch 20, loss=811.3518713378907\n",
      "Epoch 25, loss=805.9802789306641\n",
      "Epoch 30, loss=803.7289654541015\n",
      "Epoch 35, loss=802.1388592529297\n",
      "Epoch 40, loss=801.1045501708984\n",
      "Epoch 45, loss=800.4813134765625\n",
      "Epoch 50, loss=800.144482421875\n",
      "Epoch 55, loss=799.8823211669921\n",
      "Epoch 60, loss=799.4978063964844\n",
      "Epoch 65, loss=799.3657940673828\n",
      "Epoch 70, loss=799.1760681152343\n",
      "Epoch 75, loss=798.948642578125\n",
      "Epoch 80, loss=798.9252777099609\n",
      "Epoch 85, loss=798.6046411132812\n",
      "Epoch 90, loss=798.714306640625\n",
      "Epoch 95, loss=798.5576092529296\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    loss_epoch = 0.0\n",
    "    model.train()                    # switch to training mode\n",
    "    for input_, _ in train_dl:\n",
    "        recon, loss = model(input_, compute_loss=True)\n",
    "        # optimize\n",
    "        optimizer.zero_grad()        # clear previous gradients\n",
    "        loss.backward()              # backprop\n",
    "        optimizer.step()             # update parameters\n",
    "        # report\n",
    "        loss_epoch += loss.item()    # add loss to loss_epoch\n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch {}, loss={}'.format(epoch, loss_epoch / len(input_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Printing the Topics------------------\n",
      "season episodes remake explanation happening twist series questions nightmare seven\n",
      "decides meets delightful her candy ex nudity kills husband noir\n",
      "decides mom dad meets sees her kills charlie finds wants\n",
      "jokes stupid hate show sucks crap naked thats kinda seriously\n",
      "disappointment disappointed cheesy expecting seemed bland wasn't budget weren't animation\n",
      "season episodes show shows jerry cartoon episode series adam batman\n",
      "zombie zombies arts martial fights crap scientist cop rent sucks\n",
      "supporting alan noir apartment arthur comedic finest cage he manages\n",
      "won stewart greatest jr excellent award outstanding kong martial don\n",
      "seemed adaptation unnecessary development historical hadn't disappointed depth failed developed\n",
      "he adam scared his cage serial actor film fan festival\n",
      "vhs memories season childhood episodes thank you favourite copy shows\n",
      "awesome likable batman supporting plus average bruce chemistry pace okay\n",
      "waste write you crap yourself please imdb comment your wasted\n",
      "her mom daughter dad she loved family mother i parents\n",
      "detective noir impact effective murders harry dirty <UNK> genre police\n",
      "slasher nudity pacing pointless werewolf badly development poorly unnecessary mediocre\n",
      "vhs christmas copy available castle jane version saw disney memories\n",
      "gene number studio musical numbers academy dick kelly won sing\n",
      "batman animation animated episodes cartoon disney villains superman series season\n",
      "laugh laughed funniest jokes eddie comedy tim funny comedies hilarious\n",
      "redeeming pathetic rating disappointment 5 lighting rated br directing 6\n",
      "score cinematography blonde sweet <UNK> lovely delightful winning nicely pace\n",
      "arts martial sci kong gore fu fi slasher flying horror\n",
      "davis her actress supporting ms she academy delightful won performance\n",
      "<UNK> the of a <FCK> is <PAD> and it in\n",
      "she's porn her tarzan she rape actress herself husband heroine\n",
      "fu arts martial bond guns cars game scientist mission soldiers\n",
      "father loss his died chris he south son jesus church\n",
      "superman here's cop screaming cops spoiler woods kills notice guy\n",
      "crap garbage copy bought jesus trash sat thank i my\n",
      "thats scared scary cause awesome it laugh cars rent rented\n",
      "adaptation reviews jane rarely complex difficult emotion reading disappointed stunning\n",
      "match vs win v jeff won kick team event partner\n",
      "fell hated laughed weren't died ok i bought disappointed saw\n",
      "jerry dancing fred kelly singing numbers jokes laugh musical dance\n",
      "zombies zombie flicks f p commentary werewolf featuring here's trash\n",
      "americans 11 documentary soldiers war media army government military historical\n",
      "empty gay filmmaker nonsense crap actions pretentious art trash pathetic\n",
      "laughed bought vhs christmas saw eddie favorite awesome rented laugh\n",
      "images filmmaker artist he human instance narrative his <UNK> questions\n",
      "images century historical themes culture beauty artistic relationships perspective masterpiece\n",
      "tarzan Â– forces steve office military army <UNK> water giant\n",
      "friendship fu lives father complex japan river dreams life relationships\n",
      "werewolf murders 80's creepy cult castle dr tarzan dressed slasher\n",
      "her herself husband mother affair she <UNK> ms french daughter\n",
      "waste pathetic redeeming crap garbage directing worst trash lighting horrible\n",
      "culture opinion adults drugs showed students am lives people eating\n",
      "sci fi season vhs episodes bought cartoon catch nightmare haven't\n",
      "werewolf gore character's zombies slasher zombie horror 000 scared castle\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  221.3056363226022\n"
     ]
    }
   ],
   "source": [
    "emb = model.de.linear.weight.data.cpu().numpy().T\n",
    "print_top_words(emb, vocab)\n",
    "print_perp(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt-avitm-1.0",
   "language": "python",
   "name": "pt-avitm-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
