{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import groupby\n",
    "from sklearn import datasets\n",
    "from numpy import random\n",
    "from scipy.stats import dirichlet, norm, poisson\n",
    "\n",
    "# from keras.datasets import reuters, imdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "import pickle, gzip, math, torch, matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URSA Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_ds_path = Path('../data/User Review Structure Analysis (URSA)/')\n",
    "xml_path = (folder_ds_path/'Classified_Corpus.xml')\n",
    "ds_path = (folder_ds_path/'10k')\n",
    "sentence_npy_path = (folder_ds_path/'sentence.npy')\n",
    "vocab_pkl_path = (ds_path/'vocab.pkl')\n",
    "seed_words_path = (ds_path/'seed_words.txt')\n",
    "train_filename = (ds_path/'train.txt.npy')\n",
    "\n",
    "# log words not pass\n",
    "aspect_tags = ['Food', 'Staff', 'Ambience']\n",
    "polatiry_tags = ['Positive', 'Negative', 'Neutral']\n",
    "xml_review_tag = './/Review'\n",
    "log_np = [[], [], []]\n",
    "\n",
    "# length allowed sentences\n",
    "# length_allowed = [11, 7, 4]\n",
    "# min_freq_allowed = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2id = pickle.load(open(vocab_pkl_path, 'rb'))\n",
    "vocab_size=len(vocab2id)\n",
    "train_data = np.load((train_filename), allow_pickle=True)\n",
    "p_sentence_list, label_list = train_data[:, 0], train_data[:, 1]\n",
    "vocab = dict(map(reversed, vocab2id.items()))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_, y_ = [], []\n",
    "for p_sentence, label_ in zip(p_sentence_list, label_list): \n",
    "    x_.append(p_sentence)\n",
    "    y_.append(label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_) == len(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y =  train_test_split(\n",
    "    x_, y_, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded\n",
      "Dim Training Data 29125 18073\n",
      "Dim Test Data 3237 18073\n"
     ]
    }
   ],
   "source": [
    "print ('Data Loaded')\n",
    "print ('Dim Training Data',len(train_x), vocab_size)\n",
    "print ('Dim Test Data', len(test_x), vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 200\n",
    "en1_units=100\n",
    "en2_units=100\n",
    "num_topic=3\n",
    "num_input=vocab_size\n",
    "variance=0.995\n",
    "init_mult=1.0\n",
    "learning_rate=0.0005\n",
    "batch_size=200\n",
    "momentum=0.99\n",
    "num_epoch=200\n",
    "nogpu=True\n",
    "drop_rate=0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_seed_words(fn):\n",
    "    with open(fn, \"r\") as fr:\n",
    "        def p_string_sw(l):\n",
    "            return l.replace('\\n','').split(',')\n",
    "        rl = [p_string_sw(l) for l in fr]\n",
    "    return rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_words = read_file_seed_words(seed_words_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['food', 'sauc', 'chicken', 'shrimp', 'chees', 'potato', 'fri', 'tomato', 'roast', 'onion', 'pork', 'goat', 'grill', 'tuna', 'salad', 'beef', 'tapa'], ['staff', 'servic', 'friendli', 'rude', 'hostess', 'waiter', 'bartend', 'waitress', 'help', 'polit', 'bar', 'courteou', 'member', 'waitstaff', 'attitud', 'reserv', 'tip'], ['atmospher', 'scene', 'place', 'tabl', 'outsid', 'area', 'ambianc', 'outdoor', 'romant', 'cozi', 'decor', 'sit', 'wall', 'light', 'window', 'area', 'ceil', 'floor']]\n"
     ]
    }
   ],
   "source": [
    "print (seed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(o):\n",
    "    if o is None: return []\n",
    "    if isinstance(o, list): return o\n",
    "    if isinstance(o, str): return [o]\n",
    "    if isinstance(o, Iterable): return list(o)\n",
    "    return [o]\n",
    "def setify(o): return o if isinstance(o,set) else set(listify(o))\n",
    "def compose(x, funcs, *args, order_key='_order', **kwargs):\n",
    "    key = lambda o: getattr(o, order_key, 0)\n",
    "    for f in sorted(listify(funcs), key=key): x = f(x, **kwargs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_prior(fn, n_k=3):\n",
    "    gamma = torch.zeros((len(vocab),n_k))\n",
    "    gamma_bin = torch.zeros((1, len(vocab),n_k))\n",
    "\n",
    "    full_vocab = read_file_seed_words(fn)\n",
    "    for k in range(len(full_vocab)):\n",
    "        for idx in range(len(full_vocab[k])):\n",
    "            ivocab = vocab2id[full_vocab[k][idx]]\n",
    "            gamma[ivocab, k] = 1.0\n",
    "            gamma_bin[:, ivocab, :] = 1.0\n",
    "\n",
    "    return (gamma, gamma_bin)\n",
    "\n",
    "def print_perp(model):\n",
    "    cost = []\n",
    "    model.eval()                        # switch to testing mode\n",
    "    for x_test, y_test in test_dl:\n",
    "        recon, loss = model(x_test, compute_loss=True, avg_loss=False)\n",
    "        loss = loss.data\n",
    "        counts = x_test.sum(1)\n",
    "        cost.extend((loss / counts).data.cpu().tolist())\n",
    "    print('The approximated perplexity is: ', (np.exp(np.mean(np.array(cost)))))\n",
    "\n",
    "def print_top_words(beta, feature_names, n_top_words=10):\n",
    "    print ('---------------Printing the Topics------------------')\n",
    "    for i in range(len(beta)):\n",
    "        line = \" \".join([feature_names[j] \n",
    "                         for j in beta[i].argsort()[:-n_top_words - 1:-1]])\n",
    "        print('{}'.format(line))\n",
    "    print ('---------------End of Topics------------------')\n",
    "    \n",
    "def print_gamma(gamma, seed_words, vocab, vocab2id):\n",
    "    sws = []        \n",
    "    for k in range(len(seed_words)):\n",
    "        for idx in range(len(seed_words[k])):\n",
    "            w = seed_words[k][idx]\n",
    "            sws.append((k, w))\n",
    "\n",
    "    for idx in range(len(sws)):\n",
    "        k, w = sws[idx]\n",
    "        ivocab = vocab2id[w]\n",
    "        mk = gamma[ivocab].argmax(-1)\n",
    "        print (ivocab, w, k, mk, gamma[ivocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    x, y = zip(*b)\n",
    "    return torch.stack(x), torch.stack(y)\n",
    "\n",
    "class IdifyAndLimitedVocab():\n",
    "    _order=-1\n",
    "    def __init__(self, vocab2id, limited_vocab):\n",
    "        self.vocab2id = vocab2id\n",
    "        self.limited_vocab = limited_vocab\n",
    "    def __call__(self, item):\n",
    "        idlist = [self.vocab2id[w] for w in item if self.vocab2id[w] < self.limited_vocab]\n",
    "        return np.array(idlist)\n",
    "    \n",
    "\n",
    "class Numpyify():\n",
    "    _order=0\n",
    "    def __call__(self, item):\n",
    "        return np.array(item)\n",
    "\n",
    "class Onehotify():\n",
    "    _order=1\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "    def __call__(self, item):\n",
    "        return np.array(np.bincount(item.astype('int'), minlength=self.vocab_size))\n",
    "    \n",
    "class YToOnehot():\n",
    "    _order=1\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "    def __call__(self, item):\n",
    "        categorical = np.zeros((1, self.num_classes))\n",
    "        categorical[0, item] = 1\n",
    "        return categorical\n",
    "\n",
    "class Tensorify():\n",
    "    _order=2\n",
    "    def __call__(self, item):\n",
    "        return torch.from_numpy(item)\n",
    "\n",
    "class Floatify():\n",
    "    _order=3\n",
    "    def __call__(self, item):\n",
    "        return item.float()\n",
    "    \n",
    "class CheckAndCudify():\n",
    "    _order=100\n",
    "    def __init__(self):\n",
    "        self.ic = torch.cuda.is_available()\n",
    "    def __call__(self, item):\n",
    "        return item.cuda() if self.ic else item\n",
    "    \n",
    "class URSADataset(Dataset):\n",
    "    def __init__(self, x, y, tfms_x, tfms_y): \n",
    "        self.x, self.y = x, y\n",
    "        self.x_tfms = tfms_x\n",
    "        self.y_tfms = tfms_y\n",
    "    def __len__(self): \n",
    "        return len(self.x)\n",
    "    def __getitem__(self, i): \n",
    "        return compose(self.x[i], self.x_tfms), compose(self.y[i], self.y_tfms)\n",
    "    \n",
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = np.max(train_y) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms_x = [Numpyify(), Onehotify(vocab_size=vocab_size), Tensorify(), Floatify(), CheckAndCudify()]\n",
    "tfms_y = [YToOnehot(num_classes=num_classes), Tensorify(), Floatify(), CheckAndCudify()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = URSADataset(train_x, train_y, tfms_x=tfms_x, tfms_y=tfms_y)\n",
    "test_ds = URSADataset(test_x, test_y, tfms_x=tfms_x, tfms_y=tfms_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = Sampler(train_ds, bs, shuffle=False)\n",
    "test_samp = Sampler(test_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n",
    "test_dl = DataLoader(test_ds, sampler=test_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_prior = setup_prior(seed_words_path, 3)\n",
    "\n",
    "gamma, gamma_bin = gamma_prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProdLDA(nn.Module):\n",
    "    def __init__(self, num_input, en1_units, en2_units, num_topic, drop_rate, init_mult, gamma_prior):\n",
    "        super(ProdLDA, self).__init__()\n",
    "        self.num_input, self.en1_units, self.en2_units, \\\n",
    "        self.num_topic, self.drop_rate, self.init_mult = num_input, en1_units, en2_units, \\\n",
    "                                                            num_topic, drop_rate, init_mult\n",
    "        # gamma prior\n",
    "        self.gamma_prior = gamma_prior\n",
    "        \n",
    "        # encoder\n",
    "        self.en1_fc = nn.Linear(num_input, en1_units)\n",
    "        self.en1_ac = nn.Softplus()\n",
    "        self.en2_fc     = nn.Linear(en1_units, en2_units)\n",
    "        self.en2_ac = nn.Softplus()\n",
    "        self.en2_dr   = nn.Dropout(drop_rate)\n",
    "        \n",
    "        # mean, logvar\n",
    "        self.mean_fc = nn.Linear(en2_units, num_topic)\n",
    "        self.mean_bn = nn.BatchNorm1d(num_topic)\n",
    "        self.logvar_fc = nn.Linear(en2_units, num_topic)\n",
    "        self.logvar_bn = nn.BatchNorm1d(num_topic)\n",
    "\n",
    "        # decoder\n",
    "        self.de_ac1 = nn.Softmax(dim=-1)\n",
    "        self.de_dr = nn.Dropout(drop_rate)\n",
    "        self.de_fc = nn.Linear(num_topic, num_input)\n",
    "        self.de_bn = nn.BatchNorm1d(num_input)\n",
    "        self.de_ac2 = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # prior mean and variance as constant buffers\n",
    "        self.prior_mean   = torch.Tensor(1, num_topic).fill_(0)\n",
    "        self.prior_var    = torch.Tensor(1, num_topic).fill_(variance)\n",
    "        self.prior_mean   = nn.Parameter(self.prior_mean, requires_grad=False)\n",
    "        self.prior_var    = nn.Parameter(self.prior_var, requires_grad=False)\n",
    "        self.prior_logvar = nn.Parameter(self.prior_var.log(), requires_grad=False)\n",
    "        # initialize decoder weight\n",
    "        if init_mult != 0:\n",
    "            #std = 1. / math.sqrt( init_mult * (num_topic + num_input))\n",
    "            self.de_fc.weight.data.uniform_(0, init_mult)\n",
    "        # remove BN's scale parameters\n",
    "        for component in [self.mean_bn, self.logvar_bn, self.de_bn]:\n",
    "            component.weight.requires_grad = False\n",
    "            component.weight.fill_(1.0)\n",
    "        \n",
    "    def gamma(self):\n",
    "        # this function have to run after self.encode\n",
    "        encoder_w1 = self.en1_fc.weight\n",
    "        encoder_b1 = self.en1_fc.bias\n",
    "        encoder_w2 = self.en2_fc.weight\n",
    "        encoder_b2 = self.en2_fc.bias\n",
    "        mean_w = self.mean_fc.weight\n",
    "        mean_b = self.mean_fc.bias\n",
    "        mean_running_mean = self.mean_bn.running_mean\n",
    "        mean_running_var = self.mean_bn.running_var\n",
    "        logvar_w = self.logvar_fc.weight\n",
    "        logvar_b = self.logvar_fc.bias\n",
    "        logvar_running_mean = self.logvar_bn.running_mean\n",
    "        logvar_running_var = self.logvar_bn.running_var\n",
    "        \n",
    "        w1 = F.softplus(encoder_w1.t() + encoder_b1)\n",
    "        w2 = F.softplus(F.linear(w1, encoder_w2, encoder_b2))\n",
    "        wdr = F.dropout(w2, self.drop_rate)\n",
    "        wo_mean = F.softmax(F.linear(wdr, mean_w, mean_b), dim=-1)\n",
    "        wo_logvar = F.softmax(F.batch_norm(F.linear(wdr, logvar_w, logvar_b), logvar_running_mean, logvar_running_var), dim=-1)\n",
    "        \n",
    "        return wo_mean, wo_logvar\n",
    "            \n",
    "    def encode(self, input_):\n",
    "        # encoder\n",
    "        encoded1 = self.en1_fc(input_)\n",
    "        encoded1_ac = self.en1_ac(encoded1)\n",
    "        encoded2 = self.en2_fc(encoded1_ac)\n",
    "        encoded2_ac = self.en2_ac(encoded2)\n",
    "        encoded2_dr = self.en2_dr(encoded2_ac)\n",
    "        \n",
    "        encoded = encoded2_dr\n",
    "        \n",
    "        # hidden => mean, logvar\n",
    "        mean_theta = self.mean_fc(encoded)\n",
    "        mean_theta_bn = self.mean_bn(mean_theta)\n",
    "        logvar_theta = self.logvar_fc(encoded)\n",
    "        logvar_theta_bn = self.logvar_bn(logvar_theta)\n",
    "        \n",
    "        posterior_mean = mean_theta_bn\n",
    "        posterior_logvar = logvar_theta_bn\n",
    "        return encoded, posterior_mean, posterior_logvar\n",
    "    \n",
    "    def decode(self, input_, posterior_mean, posterior_var):\n",
    "        # take sample\n",
    "        eps = input_.data.new().resize_as_(posterior_mean.data).normal_() # noise \n",
    "        z = posterior_mean + posterior_var.sqrt() * eps                   # reparameterization\n",
    "        # do reconstruction\n",
    "        # decoder\n",
    "        decoded1_ac = self.de_ac1(z)\n",
    "        decoded1_dr = self.de_dr(decoded1_ac)\n",
    "        decoded2 = self.de_fc(decoded1_dr)\n",
    "        decoded2_bn = self.de_bn(decoded2)\n",
    "        decoded2_ac = self.de_ac2(decoded2_bn)\n",
    "        recon = decoded2_ac          # reconstructed distribution over vocabulary\n",
    "        return recon\n",
    "    \n",
    "    def forward(self, input_, compute_loss=False, avg_loss=True):\n",
    "        # compute posterior\n",
    "        en2, posterior_mean, posterior_logvar = self.encode(input_) \n",
    "        posterior_var    = posterior_logvar.exp()\n",
    "        \n",
    "        recon = self.decode(input_, posterior_mean, posterior_var)\n",
    "        if compute_loss:\n",
    "            return recon, self.loss(input_, recon, posterior_mean, posterior_logvar, posterior_var, avg_loss)\n",
    "        else:\n",
    "            return recon\n",
    "\n",
    "    def loss(self, input_, recon, posterior_mean, posterior_logvar, posterior_var, avg=True):\n",
    "        # NL\n",
    "        NL  = -(input_ * (recon + 1e-10).log()).sum(1)\n",
    "        # KLD, see Section 3.3 of Akash Srivastava and Charles Sutton, 2017, \n",
    "        # https://arxiv.org/pdf/1703.01488.pdf\n",
    "        prior_mean   = self.prior_mean.expand_as(posterior_mean)\n",
    "        prior_var    = self.prior_var.expand_as(posterior_mean)\n",
    "        prior_logvar = self.prior_logvar.expand_as(posterior_mean)\n",
    "        var_division    = posterior_var  / prior_var\n",
    "        diff            = posterior_mean - prior_mean\n",
    "        diff_term       = diff * diff / prior_var\n",
    "        logvar_division = prior_logvar - posterior_logvar\n",
    "        # put KLD together\n",
    "        KLD = 0.5 * ( (var_division + diff_term + logvar_division).sum(1) - self.num_topic)\n",
    "        \n",
    "        # gamma\n",
    "        n, _ = input_.size()\n",
    "        gamma_mean, gamma_logvar = self.gamma()\n",
    "        gamma_prior, gammar_prior_bin = self.gamma_prior\n",
    "        input_t = (input_ > 0).unsqueeze(dim=-1)\n",
    "        input_bin = ((gammar_prior_bin.expand(n, -1, -1) == 1) & input_t)\n",
    "        lambda_c = 20.0\n",
    "        \n",
    "        gamma_prior = gamma_prior.expand(n, -1, -1)      \n",
    "        \n",
    "        GL = lambda_c * ((gamma_prior - (input_bin.int()*gamma_mean))**2).sum((1, 2))\n",
    "        \n",
    "        # loss\n",
    "        loss = (NL + KLD + GL)\n",
    "        \n",
    "        # in traiming mode, return averaged loss. In testing mode, return individual loss\n",
    "        if avg:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def compute_accuracy(y_pred, y_true):\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1_score, _ = metrics.precision_recall_fscore_support(y_true=y_true, \\\n",
    "                                                     y_pred=y_pred, \\\n",
    "                                                     average=None)\n",
    "\n",
    "    return (accuracy, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProdLDA(num_input, en1_units, en2_units, num_topic, drop_rate, init_mult, gamma_prior)\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate, betas=(momentum, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    loss_epoch = 0.0\n",
    "    model.train()                    # switch to training mode\n",
    "    for input_, label_ in train_dl:\n",
    "        recon, loss = model(input_, compute_loss=True)\n",
    "        # optimize\n",
    "        optimizer.zero_grad()        # clear previous gradients\n",
    "        loss.backward()              # backprop\n",
    "        optimizer.step()             # update parameters\n",
    "        # report\n",
    "        loss_epoch += loss.item()    # add loss to loss_epoch\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        # Test Model\n",
    "        pred_train = []\n",
    "        label_train = []\n",
    "        pred_test = []\n",
    "        label_test = []\n",
    "        \n",
    "        for x_train, y_train in train_dl:\n",
    "            encoded, theta_mean, theta_logvar = model.encode(x_train)\n",
    "            temp_theta_mean = theta_mean.argmax(-1).int().data.cpu().tolist()\n",
    "            temp_y_train = y_train.argmax(-1).flatten().data.cpu().tolist()\n",
    "            \n",
    "            pred_train.extend(temp_theta_mean)\n",
    "            label_train.extend(temp_y_train)\n",
    "        \n",
    "        accuracy_train, precision_train, recall_train, f1_score_train = compute_accuracy(pred_train, label_train)\n",
    "        \n",
    "        for x_test, y_test in test_dl:\n",
    "            encoded, theta_mean, theta_logvar = model.encode(x_test)\n",
    "            temp_theta_mean = theta_mean.argmax(-1).int().data.cpu().tolist()\n",
    "            temp_y_test = y_test.argmax(-1).flatten().data.cpu().tolist()\n",
    "            \n",
    "            pred_test.extend(temp_theta_mean)\n",
    "            label_test.extend(temp_y_test)\n",
    "        \n",
    "        accuracy_test, precision_test, recall_test, f1_score_test = compute_accuracy(pred_test, label_test)\n",
    "        print (\"##################################################\")\n",
    "        print('Epoch {}, loss={}, accuracy_train={}, accuracy_test={}'.format(epoch, loss_epoch / len(input_), accuracy_train, accuracy_test))\n",
    "        for k in range(num_topic):\n",
    "            print (\"precision_train{}\".format(k), \"=\" , \"{:.9f}\".format(precision_train[k]), \\\n",
    "                 \"recall_train{}\".format(k), \"=\" , \"{:.9f}\".format(recall_train[k]), \\\n",
    "                 \"f1_score_train{}\".format(k), \"=\" , \"{:.9f}\".format(f1_score_train[k]))\n",
    "            print (\"precision_te{}\".format(k), \"=\" , \"{:.9f}\".format(precision_test[k]), \\\n",
    "                 \"recall_te{}\".format(k), \"=\" , \"{:.9f}\".format(recall_test[k]), \\\n",
    "                 \"f1_score_te{}\".format(k), \"=\" , \"{:.9f}\".format(f1_score_test[k]))\n",
    "        emb = model.de_fc.weight.data.detach().cpu().numpy().T\n",
    "        print_top_words(emb, vocab, 50)\n",
    "        print_perp(model)\n",
    "        print (\"##################################################\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "935 food 0 0 [9.9816942e-01 7.8819349e-04 1.0423878e-03]\n",
      "88 sauc 0 0 [9.8719537e-01 5.6452770e-04 1.2240122e-02]\n",
      "2681 chicken 0 0 [9.9999261e-01 2.2231402e-06 5.0871781e-06]\n",
      "2414 shrimp 0 0 [9.9810696e-01 1.8730439e-03 1.9972667e-05]\n",
      "1381 chees 0 0 [9.9989939e-01 1.9063911e-05 8.1581544e-05]\n",
      "1496 potato 0 0 [9.9967277e-01 2.1902409e-04 1.0821931e-04]\n",
      "105 fri 0 0 [9.9992347e-01 2.9473993e-05 4.7106540e-05]\n",
      "546 tomato 0 0 [9.9965930e-01 6.7374240e-05 2.7342312e-04]\n",
      "1347 roast 0 0 [9.9998915e-01 7.4518891e-07 1.0149953e-05]\n",
      "642 onion 0 0 [9.9939394e-01 3.2875538e-04 2.7739661e-04]\n",
      "2272 pork 0 0 [9.99999046e-01 1.08499215e-07 7.79524385e-07]\n",
      "872 goat 0 0 [9.9991977e-01 2.8738012e-05 5.1523806e-05]\n",
      "1005 grill 0 0 [9.9506575e-01 6.2269263e-04 4.3116515e-03]\n",
      "124 tuna 0 0 [1.000000e+00 4.065984e-09 3.124280e-08]\n",
      "1159 salad 0 0 [9.9990308e-01 3.8168437e-05 5.8710895e-05]\n",
      "2188 beef 0 0 [9.9995565e-01 1.2659381e-06 4.3086537e-05]\n",
      "601 tapa 0 0 [9.99554574e-01 1.12453046e-04 3.32925294e-04]\n",
      "1991 staff 1 1 [8.5364336e-06 9.9980754e-01 1.8387046e-04]\n",
      "1425 servic 1 1 [2.5133838e-07 9.9999976e-01 3.7386730e-08]\n",
      "1137 friendli 1 1 [3.7456742e-03 9.9612111e-01 1.3322175e-04]\n",
      "1009 rude 1 1 [2.9501742e-05 9.9992132e-01 4.9241626e-05]\n",
      "452 hostess 1 1 [0.00325954 0.9914061  0.00533438]\n",
      "1592 waiter 1 1 [6.1562947e-05 9.9888140e-01 1.0570809e-03]\n",
      "584 bartend 1 1 [0.00163383 0.99724895 0.00111729]\n",
      "711 waitress 1 1 [4.8685938e-04 9.9949098e-01 2.2180839e-05]\n",
      "1012 help 1 1 [4.5043053e-06 9.9789596e-01 2.0994816e-03]\n",
      "216 polit 1 1 [3.0605964e-04 9.9951136e-01 1.8249791e-04]\n",
      "1401 bar 1 1 [9.5138256e-04 9.9714077e-01 1.9078857e-03]\n",
      "1962 courteou 1 1 [8.0115302e-04 9.9889225e-01 3.0664899e-04]\n",
      "2722 member 1 1 [6.7952198e-05 9.9982482e-01 1.0730359e-04]\n",
      "368 waitstaff 1 1 [0.00203446 0.99652135 0.00144421]\n",
      "685 attitud 1 1 [5.0672506e-06 9.9977463e-01 2.2027356e-04]\n",
      "1815 reserv 1 1 [0.01920364 0.96866924 0.01212711]\n",
      "2252 tip 1 1 [4.0685291e-06 9.9986994e-01 1.2602827e-04]\n",
      "2529 atmospher 2 2 [1.7322261e-03 4.2749970e-04 9.9784029e-01]\n",
      "579 scene 2 2 [7.5249943e-08 2.6577023e-09 9.9999988e-01]\n",
      "2318 place 2 2 [4.0433966e-04 8.9053909e-05 9.9950659e-01]\n",
      "2199 tabl 2 2 [4.1779371e-05 5.3498556e-04 9.9942327e-01]\n",
      "575 outsid 2 2 [1.1319224e-06 2.2183981e-04 9.9977702e-01]\n",
      "965 area 2 2 [1.7832146e-03 9.3944527e-06 9.9820733e-01]\n",
      "463 ambianc 2 2 [1.0712124e-04 1.0272979e-03 9.9886560e-01]\n",
      "1675 outdoor 2 2 [0.00275915 0.02935636 0.9678844 ]\n",
      "1589 romant 2 2 [8.2750125e-07 2.0803639e-06 9.9999714e-01]\n",
      "2616 cozi 2 2 [2.2754537e-04 2.6751350e-04 9.9950492e-01]\n",
      "457 decor 2 2 [4.5505152e-05 7.2527223e-04 9.9922919e-01]\n",
      "1942 sit 2 2 [7.9314013e-06 9.3533090e-05 9.9989855e-01]\n",
      "2705 wall 2 2 [4.3461303e-04 4.5968132e-05 9.9951947e-01]\n",
      "2643 light 2 2 [2.2994232e-05 3.5200169e-06 9.9997354e-01]\n",
      "864 window 2 2 [1.7211127e-05 8.8819763e-07 9.9998188e-01]\n",
      "965 area 2 2 [1.7832146e-03 9.3944527e-06 9.9820733e-01]\n",
      "85 ceil 2 2 [7.3147938e-05 2.7062959e-05 9.9989974e-01]\n",
      "161 floor 2 2 [3.9893737e-07 2.4896721e-07 9.9999940e-01]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "gamma_mean, gamma_logvar = model.gamma()\n",
    "gm, gl = gamma_mean.data.cpu().numpy(), gamma_logvar.data.cpu().numpy()\n",
    "print_gamma(gm, seed_words, vocab, vocab2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Printing the Topics------------------\n",
      "babi boomer green duck sweet stew sea liter starter zimbabw juggl z eggplant tart spinach mussel octopu slice foie cod tomato bass shrimp adult delic sauc mac piquant sour saute java crab sumatra pea corn pud fish deep assort satay horseradish like bread arugula line food short chorizo monkfish lentil\n",
      "even waiter n seat take said apolog peopl ask say terribl bottl friendli hour never bill later refus think phone glass min spoke though somewher substitut lose us rude mention manag help decid complaint p colleg complain suck attitud amount host smile ac behind particular worst parti member coffe warn\n",
      "decor color loung nquett window chandeli spot tablecloth cool modern music light scene intim jazz gray expos wall interior bar spaciou eleg ba feel stain area rustic dim asian fireplac rose upstair movi sleek gigant mismatch deep leather patron beam linen crowd mood print accent behind upper view franchis antiqu\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  4.518262611250598e+24\n"
     ]
    }
   ],
   "source": [
    "emb = model.de_fc.weight.data.cpu().numpy().T\n",
    "print_top_words(emb, vocab, 50)\n",
    "print_perp(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avi",
   "language": "python",
   "name": "avi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
