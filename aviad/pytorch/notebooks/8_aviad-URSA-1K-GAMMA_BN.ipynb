{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import groupby\n",
    "from sklearn import datasets\n",
    "from numpy import random\n",
    "from scipy.stats import dirichlet, norm, poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.datasets import reuters, imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "import pickle, gzip, math, torch, matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URSA Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_ds_path = Path('../data/User Review Structure Analysis (URSA)/')\n",
    "xml_path = (folder_ds_path/'Classified_Corpus.xml')\n",
    "ds_path = (folder_ds_path/'1k')\n",
    "sentence_npy_path = (folder_ds_path/'sentence.npy')\n",
    "vocab_pkl_path = (ds_path/'vocab.pkl')\n",
    "seed_words_path = (ds_path/'seed_words.txt')\n",
    "train_filename = (ds_path/'train.txt.npy')\n",
    "\n",
    "# log words not pass\n",
    "aspect_tags = ['Food', 'Staff', 'Ambience']\n",
    "polatiry_tags = ['Positive', 'Negative', 'Neutral']\n",
    "xml_review_tag = './/Review'\n",
    "log_np = [[], [], []]\n",
    "\n",
    "# length allowed sentences\n",
    "# length_allowed = [11, 7, 4]\n",
    "# min_freq_allowed = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2id = pickle.load(open(vocab_pkl_path, 'rb'))\n",
    "vocab_size=len(vocab2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load((train_filename), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sentence_list, label_list = train_data[:, 0], train_data[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict(map(reversed, vocab2id.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_, y_ = [], []\n",
    "for p_sentence, label_ in zip(p_sentence_list, label_list): \n",
    "    x_.append(p_sentence)\n",
    "    y_.append(label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "len(x_) == len(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y =  train_test_split(\n",
    "    x_, y_, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data Loaded\nDim Training Data 3095 2772\nDim Test Data 344 2772\n"
     ]
    }
   ],
   "source": [
    "print ('Data Loaded')\n",
    "print ('Dim Training Data',len(train_x), vocab_size)\n",
    "print ('Dim Test Data', len(test_x), vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 200\n",
    "en1_units=100\n",
    "en2_units=100\n",
    "num_topic=3\n",
    "num_input=vocab_size\n",
    "variance=0.995\n",
    "init_mult=1.0\n",
    "learning_rate=0.0005\n",
    "batch_size=200\n",
    "momentum=0.99\n",
    "num_epoch=200\n",
    "nogpu=True\n",
    "drop_rate=0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_seed_words(fn):\n",
    "    with open(fn, \"r\") as fr:\n",
    "        def p_string_sw(l):\n",
    "            return l.replace('\\n','').split(',')\n",
    "        rl = [p_string_sw(l) for l in fr]\n",
    "    return rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_words = read_file_seed_words(seed_words_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['food', 'sauc', 'chicken', 'shrimp', 'chees', 'potato', 'fri', 'tomato', 'roast', 'onion', 'pork', 'goat', 'grill', 'tuna', 'salad', 'beef', 'tapa'], ['staff', 'servic', 'friendli', 'rude', 'hostess', 'waiter', 'bartend', 'waitress', 'help', 'polit', 'bar', 'courteou', 'member', 'waitstaff', 'attitud', 'reserv', 'tip'], ['atmospher', 'scene', 'place', 'tabl', 'outsid', 'area', 'ambianc', 'outdoor', 'romant', 'cozi', 'decor', 'sit', 'wall', 'light', 'window', 'area', 'ceil', 'floor']]\n"
     ]
    }
   ],
   "source": [
    "print (seed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_prior(fn, n_k=3):\n",
    "    gamma = torch.zeros((len(vocab),n_k))\n",
    "    gamma_bin = torch.zeros((1, len(vocab),n_k))\n",
    "\n",
    "    full_vocab = read_file_seed_words(fn)\n",
    "    for k in range(len(full_vocab)):\n",
    "        for idx in range(len(full_vocab[k])):\n",
    "            ivocab = vocab2id[full_vocab[k][idx]]\n",
    "            gamma[ivocab, k] = 1.0\n",
    "            gamma_bin[:, ivocab, :] = 1.0\n",
    "\n",
    "    return (gamma, gamma_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(o):\n",
    "    if o is None: return []\n",
    "    if isinstance(o, list): return o\n",
    "    if isinstance(o, str): return [o]\n",
    "    if isinstance(o, Iterable): return list(o)\n",
    "    return [o]\n",
    "def setify(o): return o if isinstance(o,set) else set(listify(o))\n",
    "def compose(x, funcs, *args, order_key='_order', **kwargs):\n",
    "    key = lambda o: getattr(o, order_key, 0)\n",
    "    for f in sorted(listify(funcs), key=key): x = f(x, **kwargs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_perp(model):\n",
    "    cost = []\n",
    "    model.eval()                        # switch to testing mode\n",
    "    for x_test, y_test in test_dl:\n",
    "        recon, loss = model(x_test, compute_loss=True, avg_loss=False)\n",
    "        loss = loss.data\n",
    "        counts = x_test.sum(1)\n",
    "        cost.extend((loss / counts).data.cpu().tolist())\n",
    "    print('The approximated perplexity is: ', (np.exp(np.mean(np.array(cost)))))\n",
    "\n",
    "def print_top_words(beta, feature_names, n_top_words=10):\n",
    "    print ('---------------Printing the Topics------------------')\n",
    "    for i in range(len(beta)):\n",
    "        line = \" \".join([feature_names[j] \n",
    "                         for j in beta[i].argsort()[:-n_top_words - 1:-1]])\n",
    "        print('{}'.format(line))\n",
    "    print ('---------------End of Topics------------------')\n",
    "    \n",
    "def print_gamma(gamma, seed_words, vocab, vocab2id):\n",
    "    sws = []        \n",
    "    for k in range(len(seed_words)):\n",
    "        for idx in range(len(seed_words[k])):\n",
    "            w = seed_words[k][idx]\n",
    "            sws.append((k, w))\n",
    "\n",
    "    for idx in range(len(sws)):\n",
    "        k, w = sws[idx]\n",
    "        ivocab = vocab2id[w]\n",
    "        mk = gamma[ivocab].argmax(-1)\n",
    "        print (ivocab, w, k, mk, gamma[ivocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    x, y = zip(*b)\n",
    "    return torch.stack(x), torch.stack(y)\n",
    "\n",
    "class IdifyAndLimitedVocab():\n",
    "    _order=-1\n",
    "    def __init__(self, vocab2id, limited_vocab):\n",
    "        self.vocab2id = vocab2id\n",
    "        self.limited_vocab = limited_vocab\n",
    "    def __call__(self, item):\n",
    "        idlist = [self.vocab2id[w] for w in item if self.vocab2id[w] < self.limited_vocab]\n",
    "        return np.array(idlist)\n",
    "    \n",
    "\n",
    "class Numpyify():\n",
    "    _order=0\n",
    "    def __call__(self, item):\n",
    "        return np.array(item)\n",
    "\n",
    "class Onehotify():\n",
    "    _order=1\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "    def __call__(self, item):\n",
    "        return np.array(np.bincount(item.astype('int'), minlength=self.vocab_size))\n",
    "    \n",
    "class YToOnehot():\n",
    "    _order=1\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "    def __call__(self, item):\n",
    "        categorical = np.zeros((1, self.num_classes))\n",
    "        categorical[0, item] = 1\n",
    "        return categorical\n",
    "\n",
    "class Tensorify():\n",
    "    _order=2\n",
    "    def __call__(self, item):\n",
    "        return torch.from_numpy(item)\n",
    "\n",
    "class Floatify():\n",
    "    _order=3\n",
    "    def __call__(self, item):\n",
    "        return item.float()\n",
    "    \n",
    "class CheckAndCudify():\n",
    "    _order=100\n",
    "    def __init__(self):\n",
    "        self.ic = torch.cuda.is_available()\n",
    "    def __call__(self, item):\n",
    "        return item.cuda() if self.ic else item\n",
    "    \n",
    "class URSADataset(Dataset):\n",
    "    def __init__(self, x, y, tfms_x, tfms_y): \n",
    "        self.x, self.y = x, y\n",
    "        self.x_tfms = tfms_x\n",
    "        self.y_tfms = tfms_y\n",
    "    def __len__(self): \n",
    "        return len(self.x)\n",
    "    def __getitem__(self, i): \n",
    "        return compose(self.x[i], self.x_tfms), compose(self.y[i], self.y_tfms)\n",
    "    \n",
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = np.max(train_y) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms_x = [Numpyify(), Onehotify(vocab_size=vocab_size), Tensorify(), Floatify(), CheckAndCudify()]\n",
    "tfms_y = [YToOnehot(num_classes=num_classes), Tensorify(), Floatify(), CheckAndCudify()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = URSADataset(train_x, train_y, tfms_x=tfms_x, tfms_y=tfms_y)\n",
    "test_ds = URSADataset(test_x, test_y, tfms_x=tfms_x, tfms_y=tfms_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = Sampler(train_ds, bs, shuffle=False)\n",
    "test_samp = Sampler(test_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n",
    "test_dl = DataLoader(test_ds, sampler=test_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_prior = setup_prior(seed_words_path, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma, gamma_bin = gamma_prior\n",
    "if torch.cuda.is_available():\n",
    "    gamma, gamma_bin = gamma.cuda(), gamma_bin.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProdLDA(nn.Module):\n",
    "    def __init__(self, num_input, en1_units, en2_units, num_topic, drop_rate, init_mult, gamma_prior):\n",
    "        super(ProdLDA, self).__init__()\n",
    "        self.num_input, self.en1_units, self.en2_units, \\\n",
    "        self.num_topic, self.drop_rate, self.init_mult = num_input, en1_units, en2_units, \\\n",
    "                                                            num_topic, drop_rate, init_mult\n",
    "        # gamma prior\n",
    "        self.gamma_prior = gamma_prior\n",
    "        \n",
    "        # encoder\n",
    "        self.en1_fc = nn.Linear(num_input, en1_units)\n",
    "        self.en1_ac = nn.Softplus()\n",
    "        self.en2_fc     = nn.Linear(en1_units, en2_units)\n",
    "        self.en2_ac = nn.Softplus()\n",
    "        self.en2_dr   = nn.Dropout(drop_rate)\n",
    "        \n",
    "        # mean, logvar\n",
    "        self.mean_fc = nn.Linear(en2_units, num_topic)\n",
    "        self.mean_bn = nn.BatchNorm1d(num_topic)\n",
    "        self.logvar_fc = nn.Linear(en2_units, num_topic)\n",
    "        self.logvar_bn = nn.BatchNorm1d(num_topic)\n",
    "\n",
    "        # decoder\n",
    "        self.de_ac1 = nn.Softmax(dim=-1)\n",
    "        self.de_dr = nn.Dropout(drop_rate)\n",
    "        self.de_fc = nn.Linear(num_topic, num_input)\n",
    "        self.de_bn = nn.BatchNorm1d(num_input)\n",
    "        self.de_ac2 = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # prior mean and variance as constant buffers\n",
    "        self.prior_mean   = torch.Tensor(1, num_topic).fill_(0)\n",
    "        self.prior_var    = torch.Tensor(1, num_topic).fill_(variance)\n",
    "        self.prior_mean   = nn.Parameter(self.prior_mean, requires_grad=False)\n",
    "        self.prior_var    = nn.Parameter(self.prior_var, requires_grad=False)\n",
    "        self.prior_logvar = nn.Parameter(self.prior_var.log(), requires_grad=False)\n",
    "        # initialize decoder weight\n",
    "        if init_mult != 0:\n",
    "            #std = 1. / math.sqrt( init_mult * (num_topic + num_input))\n",
    "            self.de_fc.weight.data.uniform_(0, init_mult)\n",
    "        # remove BN's scale parameters\n",
    "        for component in [self.mean_bn, self.logvar_bn, self.de_bn]:\n",
    "            component.weight.requires_grad = False\n",
    "            component.weight.fill_(1.0)\n",
    "        \n",
    "    def gamma(self):\n",
    "        # this function have to run after self.encode\n",
    "        encoder_w1 = self.en1_fc.weight\n",
    "        encoder_b1 = self.en1_fc.bias\n",
    "        encoder_w2 = self.en2_fc.weight\n",
    "        encoder_b2 = self.en2_fc.bias\n",
    "        mean_w = self.mean_fc.weight\n",
    "        mean_b = self.mean_fc.bias\n",
    "        mean_running_mean = self.mean_bn.running_mean\n",
    "        mean_running_var = self.mean_bn.running_var\n",
    "        logvar_w = self.logvar_fc.weight\n",
    "        logvar_b = self.logvar_fc.bias\n",
    "        logvar_running_mean = self.logvar_bn.running_mean\n",
    "        logvar_running_var = self.logvar_bn.running_var\n",
    "        \n",
    "        w1 = F.softplus(encoder_w1.t() + encoder_b1)\n",
    "        w2 = F.softplus(F.linear(w1, encoder_w2, encoder_b2))\n",
    "        wdr = F.dropout(w2, self.drop_rate)\n",
    "        wo_mean = F.softmax(F.batch_norm(F.linear(wdr, mean_w, mean_b), mean_running_mean, mean_running_var), dim=-1)\n",
    "        wo_logvar = F.softmax(F.batch_norm(F.linear(wdr, logvar_w, logvar_b), logvar_running_mean, logvar_running_var), dim=-1)\n",
    "        \n",
    "        return wo_mean, wo_logvar\n",
    "            \n",
    "    def encode(self, input_):\n",
    "        # encoder\n",
    "        encoded1 = self.en1_fc(input_)\n",
    "        encoded1_ac = self.en1_ac(encoded1)\n",
    "        encoded2 = self.en2_fc(encoded1_ac)\n",
    "        encoded2_ac = self.en2_ac(encoded2)\n",
    "        encoded2_dr = self.en2_dr(encoded2_ac)\n",
    "        \n",
    "        encoded = encoded2_dr\n",
    "        \n",
    "        # hidden => mean, logvar\n",
    "        mean_theta = self.mean_fc(encoded)\n",
    "        mean_theta_bn = self.mean_bn(mean_theta)\n",
    "        logvar_theta = self.logvar_fc(encoded)\n",
    "        logvar_theta_bn = self.logvar_bn(logvar_theta)\n",
    "        \n",
    "        posterior_mean = mean_theta_bn\n",
    "        posterior_logvar = logvar_theta_bn\n",
    "        return encoded, posterior_mean, posterior_logvar\n",
    "    \n",
    "    def decode(self, input_, posterior_mean, posterior_var):\n",
    "        # take sample\n",
    "        eps = input_.data.new().resize_as_(posterior_mean.data).normal_() # noise \n",
    "        z = posterior_mean + posterior_var.sqrt() * eps                   # reparameterization\n",
    "        # do reconstruction\n",
    "        # decoder\n",
    "        decoded1_ac = self.de_ac1(z)\n",
    "        decoded1_dr = self.de_dr(decoded1_ac)\n",
    "        decoded2 = self.de_fc(decoded1_dr)\n",
    "        decoded2_bn = self.de_bn(decoded2)\n",
    "        decoded2_ac = self.de_ac2(decoded2_bn)\n",
    "        recon = decoded2_ac          # reconstructed distribution over vocabulary\n",
    "        return recon\n",
    "    \n",
    "    def forward(self, input_, compute_loss=False, avg_loss=True):\n",
    "        # compute posterior\n",
    "        en2, posterior_mean, posterior_logvar = self.encode(input_) \n",
    "        posterior_var    = posterior_logvar.exp()\n",
    "        \n",
    "        recon = self.decode(input_, posterior_mean, posterior_var)\n",
    "        if compute_loss:\n",
    "            return recon, self.loss(input_, recon, posterior_mean, posterior_logvar, posterior_var, avg_loss)\n",
    "        else:\n",
    "            return recon\n",
    "\n",
    "    def loss(self, input_, recon, posterior_mean, posterior_logvar, posterior_var, avg=True):\n",
    "        # NL\n",
    "        NL  = -(input_ * (recon + 1e-10).log()).sum(1)\n",
    "        # KLD, see Section 3.3 of Akash Srivastava and Charles Sutton, 2017, \n",
    "        # https://arxiv.org/pdf/1703.01488.pdf\n",
    "        prior_mean   = self.prior_mean.expand_as(posterior_mean)\n",
    "        prior_var    = self.prior_var.expand_as(posterior_mean)\n",
    "        prior_logvar = self.prior_logvar.expand_as(posterior_mean)\n",
    "        var_division    = posterior_var  / prior_var\n",
    "        diff            = posterior_mean - prior_mean\n",
    "        diff_term       = diff * diff / prior_var\n",
    "        logvar_division = prior_logvar - posterior_logvar\n",
    "        # put KLD together\n",
    "        KLD = 0.5 * ( (var_division + diff_term + logvar_division).sum(1) - self.num_topic)\n",
    "        \n",
    "        # gamma\n",
    "        n, _ = input_.size()\n",
    "        gamma_mean, gamma_logvar = self.gamma()\n",
    "        gamma_prior, gammar_prior_bin = self.gamma_prior\n",
    "        input_t = (input_ > 0).unsqueeze(dim=-1)\n",
    "        input_bin = ((gammar_prior_bin.expand(n, -1, -1) == 1) & input_t)\n",
    "        lambda_c = 20.0\n",
    "        \n",
    "        gamma_prior = gamma_prior.expand(n, -1, -1)      \n",
    "        \n",
    "        GL = lambda_c * ((gamma_prior - (input_bin.int()*gamma_mean))**2).sum((1, 2))\n",
    "        \n",
    "        # loss\n",
    "        loss = (NL + KLD + GL)\n",
    "        \n",
    "        # in traiming mode, return averaged loss. In testing mode, return individual loss\n",
    "        if avg:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_true):\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1_score, _ = metrics.precision_recall_fscore_support(y_true=y_true, \\\n",
    "                                                     y_pred=y_pred, \\\n",
    "                                                     average=None)\n",
    "\n",
    "    return (accuracy, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProdLDA(num_input, en1_units, en2_units, num_topic, drop_rate, init_mult, (gamma, gamma_bin))\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate, betas=(momentum, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-a6a72b114c8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                    \u001b[0;31m# switch to training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m# optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# clear previous gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt_aviad/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-bb89dbaad3ad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_, compute_loss, avg_loss)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mrecon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior_logvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrecon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-bb89dbaad3ad>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, input_, recon, posterior_mean, posterior_logvar, posterior_var, avg)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mgamma_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammar_prior_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma_prior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0minput_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0minput_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammar_prior_bin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0minput_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mlambda_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    loss_epoch = 0.0\n",
    "    model.train()                    # switch to training mode\n",
    "    for input_, label_ in train_dl:\n",
    "        recon, loss = model(input_, compute_loss=True)\n",
    "        # optimize\n",
    "        optimizer.zero_grad()        # clear previous gradients\n",
    "        loss.backward()              # backprop\n",
    "        optimizer.step()             # update parameters\n",
    "        # report\n",
    "        loss_epoch += loss.item()    # add loss to loss_epoch\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        # Test Model\n",
    "        pred_train = []\n",
    "        label_train = []\n",
    "        pred_test = []\n",
    "        label_test = []\n",
    "        \n",
    "        for x_train, y_train in train_dl:\n",
    "            encoded, theta_mean, theta_logvar = model.encode(x_train)\n",
    "            temp_theta_mean = theta_mean.argmax(-1).int().data.cpu().tolist()\n",
    "            temp_y_train = y_train.argmax(-1).flatten().data.cpu().tolist()\n",
    "            \n",
    "            pred_train.extend(temp_theta_mean)\n",
    "            label_train.extend(temp_y_train)\n",
    "        \n",
    "        accuracy_train, precision_train, recall_train, f1_score_train = compute_accuracy(pred_train, label_train)\n",
    "        \n",
    "        for x_test, y_test in test_dl:\n",
    "            encoded, theta_mean, theta_logvar = model.encode(x_test)\n",
    "            temp_theta_mean = theta_mean.argmax(-1).int().data.cpu().tolist()\n",
    "            temp_y_test = y_test.argmax(-1).flatten().data.cpu().tolist()\n",
    "            \n",
    "            pred_test.extend(temp_theta_mean)\n",
    "            label_test.extend(temp_y_test)\n",
    "        \n",
    "        accuracy_test, precision_test, recall_test, f1_score_test = compute_accuracy(pred_test, label_test)\n",
    "        print (\"##################################################\")\n",
    "        print('Epoch {}, loss={}, accuracy_train={}, accuracy_test={}'.format(epoch, loss_epoch / len(input_), accuracy_train, accuracy_test))\n",
    "        for k in range(num_topic):\n",
    "            print (\"precision_train{}\".format(k), \"=\" , \"{:.9f}\".format(precision_train[k]), \\\n",
    "                 \"recall_train{}\".format(k), \"=\" , \"{:.9f}\".format(recall_train[k]), \\\n",
    "                 \"f1_score_train{}\".format(k), \"=\" , \"{:.9f}\".format(f1_score_train[k]))\n",
    "            print (\"precision_te{}\".format(k), \"=\" , \"{:.9f}\".format(precision_test[k]), \\\n",
    "                 \"recall_te{}\".format(k), \"=\" , \"{:.9f}\".format(recall_test[k]), \\\n",
    "                 \"f1_score_te{}\".format(k), \"=\" , \"{:.9f}\".format(f1_score_test[k]))\n",
    "        emb = model.de_fc.weight.data.detach().cpu().numpy().T\n",
    "        print_top_words(emb, vocab, 50)\n",
    "        print_perp(model)\n",
    "        print (\"##################################################\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "935 food 0 1 [0.24964267 0.5249344  0.225423  ]\n",
      "88 sauc 0 0 [0.48584357 0.2489847  0.26517174]\n",
      "2681 chicken 0 0 [0.43324354 0.26412746 0.30262896]\n",
      "2414 shrimp 0 1 [0.35139   0.4001237 0.2484864]\n",
      "1381 chees 0 1 [0.31703988 0.37503672 0.30792344]\n",
      "1496 potato 0 1 [0.30937544 0.46532995 0.22529462]\n",
      "105 fri 0 2 [0.3354963  0.30596307 0.3585406 ]\n",
      "546 tomato 0 2 [0.35161608 0.29045445 0.35792953]\n",
      "1347 roast 0 0 [0.43419012 0.33292595 0.23288386]\n",
      "642 onion 0 1 [0.33218953 0.43471566 0.23309481]\n",
      "2272 pork 0 0 [0.35807046 0.35356387 0.2883656 ]\n",
      "872 goat 0 1 [0.32960615 0.3879351  0.28245872]\n",
      "1005 grill 0 1 [0.21808502 0.507333   0.27458197]\n",
      "124 tuna 0 1 [0.35623774 0.42428207 0.21948016]\n",
      "1159 salad 0 0 [0.4088579  0.31646934 0.27467275]\n",
      "2188 beef 0 1 [0.34266505 0.4064603  0.2508747 ]\n",
      "601 tapa 0 1 [0.29188886 0.39962614 0.30848497]\n",
      "1991 staff 1 1 [0.20411542 0.6123133  0.18357132]\n",
      "1425 servic 1 1 [0.16824763 0.595536   0.2362164 ]\n",
      "1137 friendli 1 1 [0.15206479 0.622277   0.22565818]\n",
      "1009 rude 1 1 [0.20166391 0.5513522  0.24698387]\n",
      "452 hostess 1 1 [0.16143951 0.6502595  0.18830103]\n",
      "1592 waiter 1 1 [0.17338167 0.63573366 0.19088468]\n",
      "584 bartend 1 1 [0.20229957 0.5568175  0.24088296]\n",
      "711 waitress 1 1 [0.17334919 0.6221424  0.20450847]\n",
      "1012 help 1 1 [0.19154179 0.5820153  0.22644293]\n",
      "216 polit 1 1 [0.17366503 0.5991291  0.22720596]\n",
      "1401 bar 1 1 [0.18699458 0.5814581  0.2315474 ]\n",
      "1962 courteou 1 1 [0.21969123 0.5428093  0.2374994 ]\n",
      "2722 member 1 1 [0.18620993 0.5529918  0.2607983 ]\n",
      "368 waitstaff 1 1 [0.16283242 0.6097605  0.22740704]\n",
      "685 attitud 1 1 [0.16738246 0.6350921  0.19752543]\n",
      "1815 reserv 1 1 [0.16259566 0.6288147  0.20858966]\n",
      "2252 tip 1 1 [0.14141501 0.66206527 0.19651969]\n",
      "2529 atmospher 2 2 [0.20298287 0.20418404 0.59283304]\n",
      "579 scene 2 1 [0.24829096 0.38971555 0.3619935 ]\n",
      "2318 place 2 2 [0.23461601 0.28782126 0.47756273]\n",
      "2199 tabl 2 2 [0.28861853 0.3216873  0.38969415]\n",
      "575 outsid 2 2 [0.20492329 0.35154232 0.4435344 ]\n",
      "965 area 2 2 [0.29272994 0.29559195 0.41167808]\n",
      "463 ambianc 2 1 [0.27255857 0.41984934 0.30759218]\n",
      "1675 outdoor 2 2 [0.20294131 0.29616514 0.50089353]\n",
      "1589 romant 2 2 [0.24336214 0.281197   0.47544086]\n",
      "2616 cozi 2 2 [0.24915206 0.34264758 0.4082004 ]\n",
      "457 decor 2 2 [0.2483938  0.35507017 0.39653602]\n",
      "1942 sit 2 2 [0.31199503 0.2087709  0.47923413]\n",
      "2705 wall 2 2 [0.24014834 0.2992698  0.46058193]\n",
      "2643 light 2 2 [0.22777754 0.263664   0.5085584 ]\n",
      "864 window 2 2 [0.33741096 0.2161001  0.44648898]\n",
      "965 area 2 2 [0.29272994 0.29559195 0.41167808]\n",
      "85 ceil 2 2 [0.18231079 0.30119443 0.51649475]\n",
      "161 floor 2 2 [0.27353564 0.21783596 0.50862837]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "gamma_mean, gamma_logvar = model.gamma()\n",
    "gm, gl = gamma_mean.data.cpu().numpy(), gamma_logvar.data.cpu().numpy()\n",
    "print_gamma(gm, seed_words, vocab, vocab2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Printing the Topics------------------\n",
      "chicken sausag scallop tender roll grill roast menu sear eel kobe pineappl filet beef hearti lettuc citru loaf creami brais shrimp salt dumpl duck tofu root nut octopu either homemad greasi oil foie die gener miso kielbasa great dessert egg veget salmon chipotl perfectli rich pepper sichuan peke highlight curri\n",
      "n waitress java take even reserv terribl away min without need staff charg attent tell smooth us constantli host bad effici drink order hour inattent sincer problem came mediocr waiter bold patient got spill go bother diet poor embarrass though confirm ruin anoth wo last wrong bill e forgotten kept\n",
      "feel chair dark wooden lit paint color tone franchis soar cozi wood place deco atmospher mirror shini midtown tier accent ba mod lamp dim outdoor strike look stylish area silver ceil mismatch weather overhead enclos inspir spaciou carpet band neon chines rear villag bakeri cater rail poster drape vintag environ\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1.0212110196456305e+25\n"
     ]
    }
   ],
   "source": [
    "emb = model.de_fc.weight.data.cpu().numpy().T\n",
    "print_top_words(emb, vocab, 50)\n",
    "print_perp(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('pt_aviad': conda)",
   "metadata": {
    "interpreter": {
     "hash": "6eda30a233d25edfe69621bdff23df4a00ae6f8312cf75300c9219586cb4b0de"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}