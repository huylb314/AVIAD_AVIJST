{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import groupby\n",
    "from sklearn import datasets\n",
    "from numpy import random\n",
    "from scipy.stats import dirichlet, norm, poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.datasets import reuters, imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "import pickle, gzip, math, torch, matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URSA Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_ds_path = Path('../data/User Review Structure Analysis (URSA)/')\n",
    "xml_path = (folder_ds_path/'Classified_Corpus.xml')\n",
    "ds_path = (folder_ds_path/'1k')\n",
    "sentence_npy_path = (folder_ds_path/'sentence.npy')\n",
    "vocab_pkl_path = (ds_path/'vocab.pkl')\n",
    "seed_words_path = (ds_path/'seed_words.txt')\n",
    "train_filename = (ds_path/'train.txt.npy')\n",
    "\n",
    "# log words not pass\n",
    "aspect_tags = ['Food', 'Staff', 'Ambience']\n",
    "polatiry_tags = ['Positive', 'Negative', 'Neutral']\n",
    "xml_review_tag = './/Review'\n",
    "log_np = [[], [], []]\n",
    "\n",
    "# length allowed sentences\n",
    "# length_allowed = [11, 7, 4]\n",
    "# min_freq_allowed = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2id = pickle.load(open(vocab_pkl_path, 'rb'))\n",
    "vocab_size=len(vocab2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load((train_filename), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sentence_list, label_list = train_data[:, 0], train_data[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict(map(reversed, vocab2id.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_, y_ = [], []\n",
    "for p_sentence, label_ in zip(p_sentence_list, label_list): \n",
    "    x_.append(p_sentence)\n",
    "    y_.append(label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "len(x_) == len(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y =  train_test_split(\n",
    "    x_, y_, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data Loaded\nDim Training Data 3095 2772\nDim Test Data 344 2772\n"
     ]
    }
   ],
   "source": [
    "print ('Data Loaded')\n",
    "print ('Dim Training Data',len(train_x), vocab_size)\n",
    "print ('Dim Test Data', len(test_x), vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 200\n",
    "en1_units=100\n",
    "en2_units=100\n",
    "num_topic=3\n",
    "num_input=vocab_size\n",
    "variance=0.995\n",
    "init_mult=1.0\n",
    "learning_rate=0.0005\n",
    "batch_size=200\n",
    "momentum=0.99\n",
    "num_epoch=200\n",
    "nogpu=True\n",
    "drop_rate=0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_seed_words(fn):\n",
    "    with open(fn, \"r\") as fr:\n",
    "        def p_string_sw(l):\n",
    "            return l.replace('\\n','').split(',')\n",
    "        rl = [p_string_sw(l) for l in fr]\n",
    "    return rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_words = read_file_seed_words(seed_words_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['food', 'sauc', 'chicken', 'shrimp', 'chees', 'potato', 'fri', 'tomato', 'roast', 'onion', 'pork', 'goat', 'grill', 'tuna', 'salad', 'beef', 'tapa'], ['staff', 'servic', 'friendli', 'rude', 'hostess', 'waiter', 'bartend', 'waitress', 'help', 'polit', 'bar', 'courteou', 'member', 'waitstaff', 'attitud', 'reserv', 'tip'], ['atmospher', 'scene', 'place', 'tabl', 'outsid', 'area', 'ambianc', 'outdoor', 'romant', 'cozi', 'decor', 'sit', 'wall', 'light', 'window', 'area', 'ceil', 'floor']]\n"
     ]
    }
   ],
   "source": [
    "print (seed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_prior(fn, n_k=3):\n",
    "    gamma = torch.zeros((len(vocab),n_k))\n",
    "    gamma_bin = torch.zeros((1, len(vocab),n_k))\n",
    "\n",
    "    full_vocab = read_file_seed_words(fn)\n",
    "    for k in range(len(full_vocab)):\n",
    "        for idx in range(len(full_vocab[k])):\n",
    "            ivocab = vocab2id[full_vocab[k][idx]]\n",
    "            gamma[ivocab, k] = 1.0\n",
    "            gamma_bin[:, ivocab, :] = 1.0\n",
    "\n",
    "    return (gamma, gamma_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(o):\n",
    "    if o is None: return []\n",
    "    if isinstance(o, list): return o\n",
    "    if isinstance(o, str): return [o]\n",
    "    if isinstance(o, Iterable): return list(o)\n",
    "    return [o]\n",
    "def setify(o): return o if isinstance(o,set) else set(listify(o))\n",
    "def compose(x, funcs, *args, order_key='_order', **kwargs):\n",
    "    key = lambda o: getattr(o, order_key, 0)\n",
    "    for f in sorted(listify(funcs), key=key): x = f(x, **kwargs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_perp(model):\n",
    "    cost = []\n",
    "    model.eval()                        # switch to testing mode\n",
    "    for x_test, y_test in test_dl:\n",
    "        recon, loss = model(x_test, compute_loss=True, avg_loss=False)\n",
    "        loss = loss.data\n",
    "        counts = x_test.sum(1)\n",
    "        cost.extend((loss / counts).data.cpu().tolist())\n",
    "    print('The approximated perplexity is: ', (np.exp(np.mean(np.array(cost)))))\n",
    "\n",
    "def print_top_words(beta, feature_names, n_top_words=10):\n",
    "    print ('---------------Printing the Topics------------------')\n",
    "    for i in range(len(beta)):\n",
    "        line = \" \".join([feature_names[j] \n",
    "                         for j in beta[i].argsort()[:-n_top_words - 1:-1]])\n",
    "        print('{}'.format(line))\n",
    "    print ('---------------End of Topics------------------')\n",
    "    \n",
    "def print_gamma(gamma, seed_words, vocab, vocab2id):\n",
    "    sws = []        \n",
    "    for k in range(len(seed_words)):\n",
    "        for idx in range(len(seed_words[k])):\n",
    "            w = seed_words[k][idx]\n",
    "            sws.append((k, w))\n",
    "\n",
    "    for idx in range(len(sws)):\n",
    "        k, w = sws[idx]\n",
    "        ivocab = vocab2id[w]\n",
    "        mk = gamma[ivocab].argmax(-1)\n",
    "        print (ivocab, w, k, mk, gamma[ivocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    x, y = zip(*b)\n",
    "    return torch.stack(x), torch.stack(y)\n",
    "\n",
    "class IdifyAndLimitedVocab():\n",
    "    _order=-1\n",
    "    def __init__(self, vocab2id, limited_vocab):\n",
    "        self.vocab2id = vocab2id\n",
    "        self.limited_vocab = limited_vocab\n",
    "    def __call__(self, item):\n",
    "        idlist = [self.vocab2id[w] for w in item if self.vocab2id[w] < self.limited_vocab]\n",
    "        return np.array(idlist)\n",
    "    \n",
    "\n",
    "class Numpyify():\n",
    "    _order=0\n",
    "    def __call__(self, item):\n",
    "        return np.array(item)\n",
    "\n",
    "class Onehotify():\n",
    "    _order=1\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "    def __call__(self, item):\n",
    "        return np.array(np.bincount(item.astype('int'), minlength=self.vocab_size))\n",
    "    \n",
    "class YToOnehot():\n",
    "    _order=1\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "    def __call__(self, item):\n",
    "        categorical = np.zeros((1, self.num_classes))\n",
    "        categorical[0, item] = 1\n",
    "        return categorical\n",
    "\n",
    "class Tensorify():\n",
    "    _order=2\n",
    "    def __call__(self, item):\n",
    "        return torch.from_numpy(item)\n",
    "\n",
    "class Floatify():\n",
    "    _order=3\n",
    "    def __call__(self, item):\n",
    "        return item.float()\n",
    "    \n",
    "class CheckAndCudify():\n",
    "    _order=100\n",
    "    def __init__(self):\n",
    "        self.ic = torch.cuda.is_available()\n",
    "    def __call__(self, item):\n",
    "        return item.cuda() if self.ic else item\n",
    "    \n",
    "class URSADataset(Dataset):\n",
    "    def __init__(self, x, y, tfms_x, tfms_y): \n",
    "        self.x, self.y = x, y\n",
    "        self.x_tfms = tfms_x\n",
    "        self.y_tfms = tfms_y\n",
    "    def __len__(self): \n",
    "        return len(self.x)\n",
    "    def __getitem__(self, i): \n",
    "        return compose(self.x[i], self.x_tfms), compose(self.y[i], self.y_tfms)\n",
    "    \n",
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = np.max(train_y) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms_x = [Numpyify(), Onehotify(vocab_size=vocab_size), Tensorify(), Floatify(), CheckAndCudify()]\n",
    "tfms_y = [YToOnehot(num_classes=num_classes), Tensorify(), Floatify(), CheckAndCudify()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = URSADataset(train_x, train_y, tfms_x=tfms_x, tfms_y=tfms_y)\n",
    "test_ds = URSADataset(test_x, test_y, tfms_x=tfms_x, tfms_y=tfms_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = Sampler(train_ds, bs, shuffle=False)\n",
    "test_samp = Sampler(test_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n",
    "test_dl = DataLoader(test_ds, sampler=test_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_prior = setup_prior(seed_words_path, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma, gamma_bin = gamma_prior\n",
    "if torch.cuda.is_available():\n",
    "    gamma, gamma_bin = gamma.cuda(), gamma_bin.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProdLDA(nn.Module):\n",
    "    def __init__(self, num_input, en1_units, en2_units, num_topic, drop_rate, init_mult, gamma_prior):\n",
    "        super(ProdLDA, self).__init__()\n",
    "        self.num_input, self.en1_units, self.en2_units, \\\n",
    "        self.num_topic, self.drop_rate, self.init_mult = num_input, en1_units, en2_units, \\\n",
    "                                                            num_topic, drop_rate, init_mult\n",
    "        # gamma prior\n",
    "        self.gamma_prior = gamma_prior\n",
    "        \n",
    "        # encoder\n",
    "        self.en1_fc = nn.Linear(num_input, en1_units)\n",
    "        self.en1_ac = nn.Softplus()\n",
    "        self.en2_fc     = nn.Linear(en1_units, en2_units)\n",
    "        self.en2_ac = nn.Softplus()\n",
    "        self.en2_dr   = nn.Dropout(drop_rate)\n",
    "        \n",
    "        # mean, logvar\n",
    "        self.mean_fc = nn.Linear(en2_units, num_topic)\n",
    "        self.mean_bn = nn.BatchNorm1d(num_topic)\n",
    "        self.logvar_fc = nn.Linear(en2_units, num_topic)\n",
    "        self.logvar_bn = nn.BatchNorm1d(num_topic)\n",
    "\n",
    "        # decoder\n",
    "        self.de_ac1 = nn.Softmax(dim=-1)\n",
    "        self.de_dr = nn.Dropout(drop_rate)\n",
    "        self.de_fc = nn.Linear(num_topic, num_input)\n",
    "        self.de_bn = nn.BatchNorm1d(num_input)\n",
    "        self.de_ac2 = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # prior mean and variance as constant buffers\n",
    "        self.prior_mean   = torch.Tensor(1, num_topic).fill_(0)\n",
    "        self.prior_var    = torch.Tensor(1, num_topic).fill_(variance)\n",
    "        self.prior_mean   = nn.Parameter(self.prior_mean, requires_grad=False)\n",
    "        self.prior_var    = nn.Parameter(self.prior_var, requires_grad=False)\n",
    "        self.prior_logvar = nn.Parameter(self.prior_var.log(), requires_grad=False)\n",
    "        # initialize decoder weight\n",
    "        if init_mult != 0:\n",
    "            #std = 1. / math.sqrt( init_mult * (num_topic + num_input))\n",
    "            self.de_fc.weight.data.uniform_(0, init_mult)\n",
    "        # remove BN's scale parameters\n",
    "        for component in [self.mean_bn, self.logvar_bn, self.de_bn]:\n",
    "            component.weight.requires_grad = False\n",
    "            component.weight.fill_(1.0)\n",
    "        \n",
    "    def gamma(self):\n",
    "        # this function have to run after self.encode\n",
    "        encoder_w1 = self.en1_fc.weight\n",
    "        encoder_b1 = self.en1_fc.bias\n",
    "        encoder_w2 = self.en2_fc.weight\n",
    "        encoder_b2 = self.en2_fc.bias\n",
    "        mean_w = self.mean_fc.weight\n",
    "        mean_b = self.mean_fc.bias\n",
    "        mean_running_mean = self.mean_bn.running_mean\n",
    "        mean_running_var = self.mean_bn.running_var\n",
    "        logvar_w = self.logvar_fc.weight\n",
    "        logvar_b = self.logvar_fc.bias\n",
    "        logvar_running_mean = self.logvar_bn.running_mean\n",
    "        logvar_running_var = self.logvar_bn.running_var\n",
    "        \n",
    "        w1 = F.softplus(encoder_w1.t() + encoder_b1)\n",
    "        w2 = F.softplus(F.linear(w1, encoder_w2, encoder_b2))\n",
    "        wdr = F.dropout(w2, self.drop_rate)\n",
    "        wo_mean = F.softmax(F.linear(wdr, mean_w, mean_b), dim=-1)\n",
    "        wo_logvar = F.softmax(F.batch_norm(F.linear(wdr, logvar_w, logvar_b), logvar_running_mean, logvar_running_var), dim=-1)\n",
    "        \n",
    "        return wo_mean, wo_logvar\n",
    "            \n",
    "    def encode(self, input_):\n",
    "        # encoder\n",
    "        encoded1 = self.en1_fc(input_)\n",
    "        encoded1_ac = self.en1_ac(encoded1)\n",
    "        encoded2 = self.en2_fc(encoded1_ac)\n",
    "        encoded2_ac = self.en2_ac(encoded2)\n",
    "        encoded2_dr = self.en2_dr(encoded2_ac)\n",
    "        \n",
    "        encoded = encoded2_dr\n",
    "        \n",
    "        # hidden => mean, logvar\n",
    "        mean_theta = self.mean_fc(encoded)\n",
    "        mean_theta_bn = self.mean_bn(mean_theta)\n",
    "        logvar_theta = self.logvar_fc(encoded)\n",
    "        logvar_theta_bn = self.logvar_bn(logvar_theta)\n",
    "        \n",
    "        posterior_mean = mean_theta_bn\n",
    "        posterior_logvar = logvar_theta_bn\n",
    "        return encoded, posterior_mean, posterior_logvar\n",
    "    \n",
    "    def decode(self, input_, posterior_mean, posterior_var):\n",
    "        # take sample\n",
    "        eps = input_.data.new().resize_as_(posterior_mean.data).normal_() # noise \n",
    "        z = posterior_mean + posterior_var.sqrt() * eps                   # reparameterization\n",
    "        # do reconstruction\n",
    "        # decoder\n",
    "        decoded1_ac = self.de_ac1(z)\n",
    "        decoded1_dr = self.de_dr(decoded1_ac)\n",
    "        decoded2 = self.de_fc(decoded1_dr)\n",
    "        decoded2_bn = self.de_bn(decoded2)\n",
    "        decoded2_ac = self.de_ac2(decoded2_bn)\n",
    "        recon = decoded2_ac          # reconstructed distribution over vocabulary\n",
    "        return recon\n",
    "    \n",
    "    def forward(self, input_, compute_loss=False, avg_loss=True):\n",
    "        # compute posterior\n",
    "        en2, posterior_mean, posterior_logvar = self.encode(input_) \n",
    "        posterior_var    = posterior_logvar.exp()\n",
    "        \n",
    "        recon = self.decode(input_, posterior_mean, posterior_var)\n",
    "        if compute_loss:\n",
    "            return recon, self.loss(input_, recon, posterior_mean, posterior_logvar, posterior_var, avg_loss)\n",
    "        else:\n",
    "            return recon\n",
    "\n",
    "    def loss(self, input_, recon, posterior_mean, posterior_logvar, posterior_var, avg=True):\n",
    "        # NL\n",
    "        NL  = -(input_ * (recon + 1e-10).log()).sum(1)\n",
    "        # KLD, see Section 3.3 of Akash Srivastava and Charles Sutton, 2017, \n",
    "        # https://arxiv.org/pdf/1703.01488.pdf\n",
    "        prior_mean   = self.prior_mean.expand_as(posterior_mean)\n",
    "        prior_var    = self.prior_var.expand_as(posterior_mean)\n",
    "        prior_logvar = self.prior_logvar.expand_as(posterior_mean)\n",
    "        var_division    = posterior_var  / prior_var\n",
    "        diff            = posterior_mean - prior_mean\n",
    "        diff_term       = diff * diff / prior_var\n",
    "        logvar_division = prior_logvar - posterior_logvar\n",
    "        # put KLD together\n",
    "        KLD = 0.5 * ( (var_division + diff_term + logvar_division).sum(1) - self.num_topic)\n",
    "        \n",
    "        # gamma\n",
    "        n, _ = input_.size()\n",
    "        gamma_mean, gamma_logvar = self.gamma()\n",
    "        gamma_prior, gammar_prior_bin = self.gamma_prior\n",
    "        input_t = (input_ > 0).unsqueeze(dim=-1)\n",
    "        input_bin = ((gammar_prior_bin.expand(n, -1, -1) == 1) & input_t)\n",
    "        lambda_c = 20.0\n",
    "        \n",
    "        gamma_prior = gamma_prior.expand(n, -1, -1)      \n",
    "        \n",
    "        GL = lambda_c * ((gamma_prior - (input_bin.int()*gamma_mean))**2).sum((1, 2))\n",
    "        \n",
    "        # loss\n",
    "        loss = (NL + KLD + GL)\n",
    "        \n",
    "        # in traiming mode, return averaged loss. In testing mode, return individual loss\n",
    "        if avg:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_true):\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1_score, _ = metrics.precision_recall_fscore_support(y_true=y_true, \\\n",
    "                                                     y_pred=y_pred, \\\n",
    "                                                     average=None)\n",
    "\n",
    "    return (accuracy, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProdLDA(num_input, en1_units, en2_units, num_topic, drop_rate, init_mult, (gamma, gamma_bin))\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate, betas=(momentum, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "an lead charm soundtrack nois wood solo film russian find look fixtur lantern iron otherwis sign watch pink friday tall inde overlook fine sister\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  6.10514201587197e+24\n",
      "##################################################\n",
      "##################################################\n",
      "Epoch 99, loss=191.54560032894736, accuracy_train=0.8190630048465266, accuracy_test=0.8255813953488372\n",
      "precision_train0 = 0.865580448 recall_train0 = 0.801886792 f1_score_train0 = 0.832517140\n",
      "precision_te0 = 0.866071429 recall_te0 = 0.795081967 f1_score_te0 = 0.829059829\n",
      "precision_train1 = 0.812617702 recall_train1 = 0.798334875 f1_score_train1 = 0.805412972\n",
      "precision_te1 = 0.829457364 recall_te1 = 0.862903226 f1_score_te1 = 0.845849802\n",
      "precision_train2 = 0.782112274 recall_train2 = 0.861635220 f1_score_train2 = 0.819950125\n",
      "precision_te2 = 0.776699029 recall_te2 = 0.816326531 f1_score_te2 = 0.796019900\n",
      "---------------Printing the Topics------------------\n",
      "bread boomer mash serv spici jolt babi steak miso carrot burger green ring tuna lamb rice pot soft veggi meat mustard cream drizzl highlight food steam calamari tast mushroom multipl garlic pancak salt gra line duck pepper succul salti tomato execut moist chilli adult sugar sublim arugula prosciutto lemoni finger\n",
      "wait us wine care host go attitud away would want rude problem next extrem dessert need said charg one phone guac happen complimentari told probabl got plate member person hostess weird layout came half height water browni chocol last angri staff heard inde boss servic card assur shot maitr mistak\n",
      "atmospher dark lit scene expos eleg light ba deco room street banquett layout cafe wall west leather ceil hotel trendi loud wood great difficult lead fan south height soundtrack nois charm solo otherwis russian look watch film lantern romant fixtur iron pink find sign sister tabl tall visitor downtown overlook\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  5.855146303341838e+24\n",
      "##################################################\n",
      "##################################################\n",
      "Epoch 109, loss=191.3955322265625, accuracy_train=0.8297253634894992, accuracy_test=0.8226744186046512\n",
      "precision_train0 = 0.877049180 recall_train0 = 0.807547170 f1_score_train0 = 0.840864440\n",
      "precision_te0 = 0.872727273 recall_te0 = 0.786885246 f1_score_te0 = 0.827586207\n",
      "precision_train1 = 0.817761332 recall_train1 = 0.817761332 f1_score_train1 = 0.817761332\n",
      "precision_te1 = 0.816793893 recall_te1 = 0.862903226 f1_score_te1 = 0.839215686\n",
      "precision_train2 = 0.797687861 recall_train2 = 0.867924528 f1_score_train2 = 0.831325301\n",
      "precision_te2 = 0.776699029 recall_te2 = 0.816326531 f1_score_te2 = 0.796019900\n",
      "---------------Printing the Topics------------------\n",
      "bread boomer mash spici serv jolt steak babi miso burger carrot green ring lamb tuna rice meat pot veggi drizzl soft mustard cream highlight steam multipl calamari mushroom garlic tast gra duck pancak salt food pepper succul line salti execut tomato adult crunchi stew prosciutto snow lemoni chilli moist arugula\n",
      "wait us wine care go host attitud away want rude would problem next extrem need charg said dessert one phone guac got happen complimentari told probabl hostess came member plate water person weird half servic final layout staff angri height inde boss inform heard maitr shot chocol card last assur\n",
      "atmospher dark lit scene expos eleg light ba room deco banquett street layout leather cafe wood west ceil hotel trendi wall loud great lead difficult fan south soundtrack height romant otherwis nois solo look watch film russian charm lantern iron sign pink tall sister chair find overlook fixtur ambianc air\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  5.612943558562404e+24\n",
      "##################################################\n",
      "##################################################\n",
      "Epoch 119, loss=191.25298237047699, accuracy_train=0.8358642972536349, accuracy_test=0.8401162790697675\n",
      "precision_train0 = 0.893796004 recall_train0 = 0.801886792 f1_score_train0 = 0.845350572\n",
      "precision_te0 = 0.880733945 recall_te0 = 0.786885246 f1_score_te0 = 0.831168831\n",
      "precision_train1 = 0.817028986 recall_train1 = 0.834412581 f1_score_train1 = 0.825629291\n",
      "precision_te1 = 0.824817518 recall_te1 = 0.911290323 f1_score_te1 = 0.865900383\n",
      "precision_train2 = 0.802884615 recall_train2 = 0.875262055 f1_score_train2 = 0.837512538\n",
      "precision_te2 = 0.816326531 recall_te2 = 0.816326531 f1_score_te2 = 0.816326531\n",
      "---------------Printing the Topics------------------\n",
      "bread boomer mash spici jolt serv miso steak babi carrot burger green lamb ring rice tuna meat pot veggi drizzl mustard soft highlight cream steam multipl calamari mushroom garlic duck gra pancak pepper tast salt succul line salti execut crunchi tomato adult chicken stew prosciutto toro snow lemoni food chilli\n",
      "wait us wine go care want attitud away host rude problem would next extrem charg need said one got dessert phone guac happen complimentari told probabl came hostess water final member plate servic person weird staff angri half waiter inform inde boss assur shot layout maitr height card knew heard\n",
      "atmospher dark lit expos eleg scene room ba light deco banquett street wood leather layout west cafe ceil hotel trendi wall great lead loud difficult romant soundtrack south fan otherwis nois height look watch russian film solo iron lantern sign air chair bustl pink charm tall ambianc overlook find sister\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  5.4627659745033e+24\n",
      "##################################################\n",
      "##################################################\n",
      "Epoch 129, loss=191.18432103207238, accuracy_train=0.8420032310177706, accuracy_test=0.8459302325581395\n",
      "precision_train0 = 0.898520085 recall_train0 = 0.801886792 f1_score_train0 = 0.847457627\n",
      "precision_te0 = 0.880733945 recall_te0 = 0.786885246 f1_score_te0 = 0.831168831\n",
      "precision_train1 = 0.816163410 recall_train1 = 0.850138760 f1_score_train1 = 0.832804712\n",
      "precision_te1 = 0.821428571 recall_te1 = 0.927419355 f1_score_te1 = 0.871212121\n",
      "precision_train2 = 0.818181818 recall_train2 = 0.877358491 f1_score_train2 = 0.846737481\n",
      "precision_te2 = 0.842105263 recall_te2 = 0.816326531 f1_score_te2 = 0.829015544\n",
      "---------------Printing the Topics------------------\n",
      "bread boomer mash jolt spici miso steak serv babi carrot burger lamb green ring rice meat pot tuna drizzl highlight veggi mustard soft cream steam multipl calamari mushroom garlic duck gra pepper pancak succul salt salti chicken tast crunchi execut stew line adult tomato toro prosciutto snow standout sear moist\n",
      "wait us wine go care want away attitud problem host rude would need charg extrem got next said one phone guac dessert complimentari came happen told final hostess water probabl plate member servic person angri waiter inform staff half weird maitr assur card boss shot height upset mistak heard inde\n",
      "atmospher dark lit expos eleg ba room banquett deco scene light wood leather street west layout hotel ceil cafe trendi lead great wall loud romant soundtrack difficult fan otherwis look nois south bustl watch iron air lantern height film loung russian sign chair solo pink tall sister overlook bamboo ambianc\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  5.284486492634889e+24\n",
      "##################################################\n",
      "##################################################\n",
      "Epoch 139, loss=191.06345728824013, accuracy_train=0.8432956381260097, accuracy_test=0.8459302325581395\n",
      "precision_train0 = 0.903088392 recall_train0 = 0.800000000 f1_score_train0 = 0.848424212\n",
      "precision_te0 = 0.879629630 recall_te0 = 0.778688525 f1_score_te0 = 0.826086957\n",
      "precision_train1 = 0.807093426 recall_train1 = 0.863089732 f1_score_train1 = 0.834152883\n",
      "precision_te1 = 0.805555556 recall_te1 = 0.935483871 f1_score_te1 = 0.865671642\n",
      "precision_train2 = 0.829000000 recall_train2 = 0.868972746 f1_score_train2 = 0.848515865\n",
      "precision_te2 = 0.869565217 recall_te2 = 0.816326531 f1_score_te2 = 0.842105263\n",
      "---------------Printing the Topics------------------\n",
      "bread boomer mash jolt spici miso steak carrot babi serv burger lamb rice ring green drizzl pot meat highlight veggi mustard tuna cream steam multipl soft calamari mushroom garlic duck gra pepper succul pancak salti chicken salt crunchi execut stew adult prosciutto toro snow tomato line tast seafood sear moist\n",
      "wait us wine go want care problem attitud away rude host would need got charg extrem said one next phone final came guac happen told complimentari water hostess dessert probabl member servic plate inform waiter angri card person staff half weird assur upset boss maitr heard mistak shot height inde\n",
      "atmospher dark lit expos eleg ba banquett room deco wood leather scene light street west hotel layout ceil cafe lead trendi great wall romant loud soundtrack bustl difficult fan loung iron watch air nois look lantern otherwis south film chair height russian pink solo sign bamboo sister fixtur overlook tall\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  5.087635173274073e+24\n",
      "##################################################\n",
      "##################################################\n",
      "Epoch 149, loss=190.98512798108553, accuracy_train=0.8462035541195476, accuracy_test=0.8488372093023255\n",
      "precision_train0 = 0.912147505 recall_train0 = 0.793396226 f1_score_train0 = 0.848637740\n",
      "precision_te0 = 0.879629630 recall_te0 = 0.778688525 f1_score_te0 = 0.826086957\n",
      "precision_train1 = 0.804255319 recall_train1 = 0.874190564 f1_score_train1 = 0.837765957\n",
      "precision_te1 = 0.806896552 recall_te1 = 0.943548387 f1_score_te1 = 0.869888476\n",
      "precision_train2 = 0.834669339 recall_train2 = 0.873165618 f1_score_train2 = 0.853483607\n",
      "precision_te2 = 0.879120879 recall_te2 = 0.816326531 f1_score_te2 = 0.846560847\n",
      "---------------Printing the Topics------------------\n",
      "bread boomer mash jolt spici miso steak carrot babi serv rice lamb burger ring green drizzl highlight veggi meat pot mustard tuna cream steam multipl soft calamari garlic mushroom duck gra succul pepper pancak salti chicken execut crunchi stew salt toro prosciutto adult snow seafood tomato sear moist standout line\n",
      "wait us wine go want care problem attitud away rude host would got charg need said one extrem final phone next came happen water guac told hostess complimentari probabl inform member servic dessert plate card angri waiter assur half weird staff upset heard person maitr boss mistak height shot knew\n",
      "atmospher dark expos lit eleg ba banquett wood deco room leather street scene west light hotel ceil lead layout great trendi cafe romant wall bustl soundtrack loung iron loud difficult air lantern watch south nois look film otherwis fan bamboo pink chair antiqu russian solo sign downtown height fixtur sexi\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  5.011460889410105e+24\n",
      "##################################################\n",
      "##################################################\n",
      "Epoch 159, loss=190.91393914473684, accuracy_train=0.8474959612277867, accuracy_test=0.8517441860465116\n",
      "precision_train0 = 0.915125136 recall_train0 = 0.793396226 f1_score_train0 = 0.849924204\n",
      "precision_te0 = 0.888888889 recall_te0 = 0.786885246 f1_score_te0 = 0.834782609\n",
      "precision_train1 = 0.800000000 recall_train1 = 0.880666050 f1_score_train1 = 0.838397182\n",
      "precision_te1 = 0.806896552 recall_te1 = 0.943548387 f1_score_te1 = 0.869888476\n",
      "precision_train2 = 0.841784990 recall_train2 = 0.870020964 f1_score_train2 = 0.855670103\n",
      "precision_te2 = 0.879120879 recall_te2 = 0.816326531 f1_score_te2 = 0.846560847\n",
      "---------------Printing the Topics------------------\n",
      "bread boomer mash jolt miso spici carrot steak babi serv rice burger ring lamb drizzl highlight veggi meat green pot mustard cream steam multipl tuna soft calamari garlic mushroom duck gra pepper succul pancak chicken salti execut crunchi stew toro prosciutto adult salt seafood snow sear moist tomato standout combo\n",
      "wait us wine go want care problem attitud away rude host would got charg need said one final extrem came phone next happen water hostess complimentari told guac probabl inform member servic angri plate card assur waiter weird upset heard person maitr mistak half staff boss shot total show last\n",
      "atmospher dark expos eleg lit ba banquett wood deco leather room street west scene hotel lead light ceil layout great trendi cafe romant bustl loung air difficult iron wall soundtrack south lantern watch loud bamboo film otherwis nois pink chair antiqu look fan downtown sign russian fixtur sister sexi solo\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  4.842658412494512e+24\n",
      "##################################################\n",
      "##################################################\n",
      "Epoch 169, loss=190.82288240131578, accuracy_train=0.8481421647819063, accuracy_test=0.8517441860465116\n",
      "precision_train0 = 0.916849015 recall_train0 = 0.790566038 f1_score_train0 = 0.849037487\n",
      "precision_te0 = 0.888888889 recall_te0 = 0.786885246 f1_score_te0 = 0.834782609\n",
      "precision_train1 = 0.794701987 recall_train1 = 0.888066605 f1_score_train1 = 0.838794233\n",
      "precision_te1 = 0.808219178 recall_te1 = 0.951612903 f1_score_te1 = 0.874074074\n",
      "precision_train2 = 0.849948613 recall_train2 = 0.866876310 f1_score_train2 = 0.858329009\n",
      "precision_te2 = 0.877777778 recall_te2 = 0.806122449 f1_score_te2 = 0.840425532\n",
      "---------------Printing the Topics------------------\n",
      "bread boomer mash jolt miso spici carrot babi steak rice serv ring lamb burger drizzl highlight veggi meat pot mustard green cream steam multipl tuna soft calamari duck mushroom garlic gra succul pepper pancak execut salti chicken crunchi stew toro prosciutto adult salt sear moist snow seafood standout combo tomato\n",
      "wait us wine go want care problem attitud away rude got would charg host need final said one came phone extrem happen water hostess next told probabl complimentari inform guac member angri card servic assur plate weird upset waiter mistak heard person boss maitr total shot staff half show point\n",
      "atmospher expos eleg dark ba lit banquett wood deco leather room street west lead hotel scene layout ceil trendi great light bustl cafe romant loung air difficult iron soundtrack south watch lantern wall bamboo film nois loud otherwis antiqu chair pink look downtown fan russian sister rail overlook solo fixtur\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  4.777421741498484e+24\n",
      "##################################################\n",
      "##################################################\n",
      "Epoch 179, loss=190.76948756167764, accuracy_train=0.8481421647819063, accuracy_test=0.8546511627906976\n",
      "precision_train0 = 0.919426049 recall_train0 = 0.785849057 f1_score_train0 = 0.847405900\n",
      "precision_te0 = 0.897196262 recall_te0 = 0.786885246 f1_score_te0 = 0.838427948\n",
      "precision_train1 = 0.788492707 recall_train1 = 0.900092507 f1_score_train1 = 0.840604752\n",
      "precision_te1 = 0.804054054 recall_te1 = 0.959677419 f1_score_te1 = 0.875000000\n",
      "precision_train2 = 0.857591623 recall_train2 = 0.858490566 f1_score_train2 = 0.858040859\n",
      "precision_te2 = 0.887640449 recall_te2 = 0.806122449 f1_score_te2 = 0.844919786\n",
      "---------------Printing the Topics------------------\n",
      "bread boomer mash jolt miso spici carrot babi steak rice serv drizzl ring lamb highlight burger veggi pot meat mustard steam green multipl cream tuna calamari soft duck gra mushroom garlic succul pepper pancak crunchi execut salti toro chicken stew prosciutto adult moist sear salt combo standout snow seafood curri\n",
      "wait us wine go want problem care attitud away got rude charg would host final said need one came phone extrem water hostess happen probabl inform told complimentari guac angri member next card assur servic upset total weird mistak heard plate show boss maitr waiter person point shot half though\n",
      "atmospher eleg expos dark ba lit banquett wood deco leather street room west lead hotel layout ceil scene trendi bustl romant great cafe loung light air difficult soundtrack south iron bamboo watch lantern film nois wall otherwis antiqu loud chair sexi downtown pink rail solo russian overlook bright sister look\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  4.716412318628493e+24\n",
      "##################################################\n",
      "##################################################\n",
      "Epoch 189, loss=190.71568410773025, accuracy_train=0.8497576736672051, accuracy_test=0.8546511627906976\n",
      "precision_train0 = 0.918591859 recall_train0 = 0.787735849 f1_score_train0 = 0.848146267\n",
      "precision_te0 = 0.898148148 recall_te0 = 0.795081967 f1_score_te0 = 0.843478261\n",
      "precision_train1 = 0.788025890 recall_train1 = 0.901017576 f1_score_train1 = 0.840742339\n",
      "precision_te1 = 0.798657718 recall_te1 = 0.959677419 f1_score_te1 = 0.871794872\n",
      "precision_train2 = 0.864210526 recall_train2 = 0.860587002 f1_score_train2 = 0.862394958\n",
      "precision_te2 = 0.896551724 recall_te2 = 0.795918367 f1_score_te2 = 0.843243243\n",
      "---------------Printing the Topics------------------\n",
      "bread boomer mash miso jolt carrot spici babi drizzl steak rice ring lamb serv highlight burger veggi pot mustard meat multipl steam cream green calamari tuna soft duck gra succul garlic mushroom pepper pancak toro execut crunchi salti stew chicken prosciutto moist adult sear combo standout seafood salt snow asparagu\n",
      "wait us wine go want problem care attitud away got rude charg final would host said need one came phone extrem hostess water happen probabl inform guac angri member card told complimentari assur total upset servic next boss mistak weird maitr heard show waiter plate though knew person point shot\n",
      "eleg atmospher expos ba dark lit banquett leather deco wood street room west lead hotel layout ceil scene trendi bustl romant loung cafe great air soundtrack difficult iron bamboo film south watch nois lantern light antiqu wall chair otherwis sexi downtown rail overlook bright solo pink candlelit loud fan russian\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  4.6507762039095733e+24\n",
      "##################################################\n",
      "##################################################\n",
      "Epoch 199, loss=190.69674907483554, accuracy_train=0.8494345718901454, accuracy_test=0.8488372093023255\n",
      "precision_train0 = 0.919514884 recall_train0 = 0.786792453 f1_score_train0 = 0.847991866\n",
      "precision_te0 = 0.896226415 recall_te0 = 0.778688525 f1_score_te0 = 0.833333333\n",
      "precision_train1 = 0.786290323 recall_train1 = 0.901942646 f1_score_train1 = 0.840155106\n",
      "precision_te1 = 0.793333333 recall_te1 = 0.959677419 f1_score_te1 = 0.868613139\n",
      "precision_train2 = 0.864978903 recall_train2 = 0.859538784 f1_score_train2 = 0.862250263\n",
      "precision_te2 = 0.886363636 recall_te2 = 0.795918367 f1_score_te2 = 0.838709677\n",
      "---------------Printing the Topics------------------\n",
      "boomer bread mash miso jolt carrot spici babi drizzl rice ring highlight lamb steak serv burger veggi mustard multipl pot steam cream meat green calamari soft tuna duck gra succul garlic pepper mushroom execut toro crunchi pancak salti stew chicken moist prosciutto sear adult combo standout seafood snow asparagu salt\n",
      "wait us wine go want problem care attitud got away rude charg final said host would one need came phone hostess happen extrem water probabl inform total angri member assur card guac complimentari maitr upset told mistak heard boss weird servic next though knew show waiter plate point shot turn\n",
      "eleg expos ba atmospher lit banquett dark leather deco wood street west hotel lead room layout trendi ceil bustl scene loung romant soundtrack air cafe iron bamboo film great difficult south sexi watch lantern antiqu nois wall downtown overlook chair candlelit rail otherwis solo bright pink fixtur russian cool fan\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  4.568877115953359e+24\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    loss_epoch = 0.0\n",
    "    model.train()                    # switch to training mode\n",
    "    for input_, label_ in train_dl:\n",
    "        recon, loss = model(input_, compute_loss=True)\n",
    "        # optimize\n",
    "        optimizer.zero_grad()        # clear previous gradients\n",
    "        loss.backward()              # backprop\n",
    "        optimizer.step()             # update parameters\n",
    "        # report\n",
    "        loss_epoch += loss.item()    # add loss to loss_epoch\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        # Test Model\n",
    "        pred_train = []\n",
    "        label_train = []\n",
    "        pred_test = []\n",
    "        label_test = []\n",
    "        \n",
    "        for x_train, y_train in train_dl:\n",
    "            encoded, theta_mean, theta_logvar = model.encode(x_train)\n",
    "            temp_theta_mean = theta_mean.argmax(-1).int().data.cpu().tolist()\n",
    "            temp_y_train = y_train.argmax(-1).flatten().data.cpu().tolist()\n",
    "            \n",
    "            pred_train.extend(temp_theta_mean)\n",
    "            label_train.extend(temp_y_train)\n",
    "        \n",
    "        accuracy_train, precision_train, recall_train, f1_score_train = compute_accuracy(pred_train, label_train)\n",
    "        \n",
    "        for x_test, y_test in test_dl:\n",
    "            encoded, theta_mean, theta_logvar = model.encode(x_test)\n",
    "            temp_theta_mean = theta_mean.argmax(-1).int().data.cpu().tolist()\n",
    "            temp_y_test = y_test.argmax(-1).flatten().data.cpu().tolist()\n",
    "            \n",
    "            pred_test.extend(temp_theta_mean)\n",
    "            label_test.extend(temp_y_test)\n",
    "        \n",
    "        accuracy_test, precision_test, recall_test, f1_score_test = compute_accuracy(pred_test, label_test)\n",
    "        print (\"##################################################\")\n",
    "        print('Epoch {}, loss={}, accuracy_train={}, accuracy_test={}'.format(epoch, loss_epoch / len(input_), accuracy_train, accuracy_test))\n",
    "        for k in range(num_topic):\n",
    "            print (\"precision_train{}\".format(k), \"=\" , \"{:.9f}\".format(precision_train[k]), \\\n",
    "                 \"recall_train{}\".format(k), \"=\" , \"{:.9f}\".format(recall_train[k]), \\\n",
    "                 \"f1_score_train{}\".format(k), \"=\" , \"{:.9f}\".format(f1_score_train[k]))\n",
    "            print (\"precision_te{}\".format(k), \"=\" , \"{:.9f}\".format(precision_test[k]), \\\n",
    "                 \"recall_te{}\".format(k), \"=\" , \"{:.9f}\".format(recall_test[k]), \\\n",
    "                 \"f1_score_te{}\".format(k), \"=\" , \"{:.9f}\".format(f1_score_test[k]))\n",
    "        emb = model.de_fc.weight.data.detach().cpu().numpy().T\n",
    "        print_top_words(emb, vocab, 50)\n",
    "        print_perp(model)\n",
    "        print (\"##################################################\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "935 food 0 0 [9.9993610e-01 5.9545473e-05 4.2492393e-06]\n88 sauc 0 0 [9.9975437e-01 1.9608298e-04 4.9528349e-05]\n2681 chicken 0 0 [9.9868745e-01 4.1305946e-04 8.9944241e-04]\n2414 shrimp 0 0 [9.9954140e-01 3.0691590e-04 1.5171748e-04]\n1381 chees 0 0 [9.9982101e-01 1.4468734e-04 3.4268822e-05]\n1496 potato 0 0 [9.9999523e-01 4.4848007e-06 2.1311011e-07]\n105 fri 0 0 [9.9997866e-01 1.4435831e-05 6.9094390e-06]\n546 tomato 0 0 [9.89861608e-01 1.01157725e-02 2.26201664e-05]\n1347 roast 0 0 [9.9928826e-01 6.7012839e-04 4.1705141e-05]\n642 onion 0 0 [9.9879932e-01 9.4182766e-04 2.5879298e-04]\n2272 pork 0 0 [9.9769419e-01 2.2634293e-03 4.2320753e-05]\n872 goat 0 0 [9.9753100e-01 5.1972579e-04 1.9492413e-03]\n1005 grill 0 0 [9.9999738e-01 2.6810369e-07 2.3462303e-06]\n124 tuna 0 0 [9.9999344e-01 6.4748265e-06 1.6232381e-07]\n1159 salad 0 0 [9.9972481e-01 2.4227082e-04 3.2863019e-05]\n2188 beef 0 0 [9.9998665e-01 3.1575648e-06 1.0298515e-05]\n601 tapa 0 0 [9.9961567e-01 3.8180308e-04 2.5090389e-06]\n1991 staff 1 1 [1.2349648e-06 9.9999738e-01 1.4400084e-06]\n1425 servic 1 1 [2.2208712e-06 9.9999762e-01 1.0531265e-07]\n1137 friendli 1 1 [4.0976561e-04 9.9957675e-01 1.3444174e-05]\n1009 rude 1 1 [3.3418307e-04 9.9139005e-01 8.2757454e-03]\n452 hostess 1 1 [1.7905839e-04 9.9901772e-01 8.0324634e-04]\n1592 waiter 1 1 [1.0129451e-06 9.9960524e-01 3.9378420e-04]\n584 bartend 1 1 [5.0929742e-04 9.9944514e-01 4.5590012e-05]\n711 waitress 1 1 [5.2951476e-05 9.9993944e-01 7.5656626e-06]\n1012 help 1 1 [1.0380212e-06 9.9995816e-01 4.0804822e-05]\n216 polit 1 1 [2.0786521e-03 9.9727935e-01 6.4203056e-04]\n1401 bar 1 1 [7.1285386e-04 9.9584949e-01 3.4376518e-03]\n1962 courteou 1 1 [5.5544879e-05 9.9948007e-01 4.6443625e-04]\n2722 member 1 1 [1.6702210e-04 9.9941218e-01 4.2078225e-04]\n368 waitstaff 1 1 [1.7385461e-04 9.9978667e-01 3.9434184e-05]\n685 attitud 1 1 [1.4707715e-03 9.9759549e-01 9.3373383e-04]\n1815 reserv 1 1 [6.0001644e-06 9.9998939e-01 4.6577738e-06]\n2252 tip 1 1 [0.00210684 0.9964868  0.00140642]\n2529 atmospher 2 2 [4.5914567e-04 4.2922325e-03 9.9524862e-01]\n579 scene 2 2 [1.8743625e-05 7.9525402e-05 9.9990177e-01]\n2318 place 2 2 [1.8770717e-05 6.1361789e-04 9.9936754e-01]\n2199 tabl 2 2 [5.1345622e-05 8.8595778e-05 9.9986005e-01]\n575 outsid 2 2 [4.167313e-05 8.453671e-06 9.999498e-01]\n965 area 2 2 [1.1590972e-06 1.9278665e-05 9.9997950e-01]\n463 ambianc 2 2 [4.4221179e-05 2.7766742e-04 9.9967813e-01]\n1675 outdoor 2 2 [9.9328645e-06 5.3229963e-04 9.9945778e-01]\n1589 romant 2 2 [6.2071756e-08 1.6390828e-07 9.9999976e-01]\n2616 cozi 2 2 [6.7542010e-06 6.4207575e-06 9.9998677e-01]\n457 decor 2 2 [1.7466233e-04 7.6369225e-04 9.9906164e-01]\n1942 sit 2 2 [2.0151449e-05 5.3741856e-05 9.9992609e-01]\n2705 wall 2 2 [3.7812304e-06 3.0367066e-05 9.9996579e-01]\n2643 light 2 2 [2.4277639e-05 3.8833794e-05 9.9993682e-01]\n864 window 2 2 [2.8499482e-05 9.0344190e-07 9.9997056e-01]\n965 area 2 2 [1.1590972e-06 1.9278665e-05 9.9997950e-01]\n85 ceil 2 2 [3.2277778e-06 1.7211713e-04 9.9982470e-01]\n161 floor 2 2 [2.6201482e-07 1.8373423e-05 9.9998140e-01]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "gamma_mean, gamma_logvar = model.gamma()\n",
    "gm, gl = gamma_mean.data.cpu().numpy(), gamma_logvar.data.cpu().numpy()\n",
    "print_gamma(gm, seed_words, vocab, vocab2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------------Printing the Topics------------------\nboomer bread mash miso jolt carrot spici babi drizzl rice ring highlight lamb steak serv burger veggi mustard multipl pot steam cream meat green calamari soft tuna duck gra succul garlic pepper mushroom execut toro crunchi pancak salti stew chicken moist prosciutto sear adult combo standout seafood snow asparagu salt\nwait us wine go want problem care attitud got away rude charg final said host would one need came phone hostess happen extrem water probabl inform total angri member assur card guac complimentari maitr upset told mistak heard boss weird servic next though knew show waiter plate point shot turn\neleg expos ba atmospher lit banquett dark leather deco wood street west hotel lead room layout trendi ceil bustl scene loung romant soundtrack air cafe iron bamboo film great difficult south sexi watch lantern antiqu nois wall downtown overlook chair candlelit rail otherwis solo bright pink fixtur russian cool fan\n---------------End of Topics------------------\nThe approximated perplexity is:  4.5465039394472366e+24\n"
     ]
    }
   ],
   "source": [
    "emb = model.de_fc.weight.data.cpu().numpy().T\n",
    "print_top_words(emb, vocab, 50)\n",
    "print_perp(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('pt_aviad': conda)",
   "metadata": {
    "interpreter": {
     "hash": "6eda30a233d25edfe69621bdff23df4a00ae6f8312cf75300c9219586cb4b0de"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}