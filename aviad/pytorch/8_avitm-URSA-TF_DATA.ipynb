{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import groupby\n",
    "from sklearn import datasets\n",
    "from numpy import random\n",
    "from scipy.stats import dirichlet, norm, poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters, imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "import pickle, gzip, math, torch, matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URSA Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_ds_path = Path('./data/User Review Structure Analysis (URSA)/')\n",
    "xml_path = (folder_ds_path/'Classified_Corpus.xml')\n",
    "ds_path = (folder_ds_path/'10k')\n",
    "sentence_npy_path = (folder_ds_path/'sentence.npy')\n",
    "vocab_pkl_path = (folder_ds_path/'vocab.pkl')\n",
    "\n",
    "# log words not pass\n",
    "aspect_tags = ['Food', 'Staff', 'Ambience']\n",
    "polatiry_tags = ['Positive', 'Negative', 'Neutral']\n",
    "xml_review_tag = './/Review'\n",
    "log_np = [[], [], []]\n",
    "\n",
    "# length allowed sentences\n",
    "length_allowed = [-1, -1, -1]\n",
    "min_freq_allowed = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'train.txt.npy'\n",
    "vocab_filename = 'vocab.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_onehot = np.load((ds_path/train_filename), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pickle.load(open((ds_path/vocab_filename), 'rb'))\n",
    "vocab_size=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32362, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_onehot[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data_onehot[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32362,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32362,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URSA Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_nested_xml(axml):\n",
    "    return ' '.join([the_aiter for the_aiter in axml.itertext()])\n",
    "\n",
    "def get_firstchild(axml):\n",
    "    try:\n",
    "        if len(axml.getchildren()) > 0:\n",
    "            return axml.getchildren()[0].tag\n",
    "        else:\n",
    "            raise (Exception('ListIndex', 'aXmlElement input has no children.'))\n",
    "    except Exception as e:\n",
    "        print (str(e))\n",
    "\n",
    "def xml_unique_valid(axml, alist_tag_allowed):\n",
    "    return (len(axml.getchildren()) == 0) or (get_firstchild(axml) in alist_tag_allowed)\n",
    "\n",
    "def xml_name_valid(axml, atag_name):\n",
    "    return axml.tag == atag_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listsentence_unique(alist_xml, alist_tag_allowed):\n",
    "    the_listsentence = []\n",
    "    for the_axml in alist_xml:\n",
    "        if xml_unique_valid(the_axml, alist_tag_allowed):\n",
    "            the_listsentence.append(string_nested_xml(the_axml))\n",
    "\n",
    "    return the_listsentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listxml_child(list_xml, tag):\n",
    "    return_ = []\n",
    "    for xml_ in list_xml:\n",
    "        for xml_child in xml_:\n",
    "            if xml_name_valid(xml_child, tag):\n",
    "                return_.append(xml_child)\n",
    "\n",
    "    return return_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listxml_child_list(document_list, tag_list):\n",
    "    return_ = []\n",
    "    for tag in tag_list:\n",
    "        xml_children = get_listxml_child(document_list, tag)\n",
    "        return_.append(xml_children)\n",
    "    \n",
    "    return return_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xml_unique_list(xml_children_list, polatiry_tags):\n",
    "    return_ = []\n",
    "    for xml_children in xml_children_list:\n",
    "        xml_unique = get_listsentence_unique(xml_children, polatiry_tags)\n",
    "        return_.append(xml_unique)\n",
    "    \n",
    "    return return_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tree = ET.parse(xml_path)\n",
    "corpus_root = corpus_tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_list = corpus_root.findall(xml_review_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96235\n",
      "32892\n",
      "16803\n"
     ]
    }
   ],
   "source": [
    "xml_children_list = get_listxml_child_list(document_list, aspect_tags)\n",
    "for idx in range(0, len(xml_children_list)): print (len(xml_children_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62348\n",
      "23730\n",
      "13385\n"
     ]
    }
   ],
   "source": [
    "xml_unique_list = get_xml_unique_list(xml_children_list, polatiry_tags)\n",
    "for idx in range(0, len(xml_unique_list)): print (len(xml_unique_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The food is a melding of Moroccan comfort food and Spanish tapas fare : tagines , stews and salads , with surprises like baby eggplants and olives where you might not expect them . '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_unique_list[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/huylb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/huylb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabet(atext):\n",
    "    return re.sub(\"[^a-zA-Z]\", \" \", atext)\n",
    "\n",
    "def liststopword():\n",
    "    en_stopwords = list(nltk.corpus.stopwords.words(\"english\"))\n",
    "    additional_list = [\"'s\",\"...\",\"'ve\",\"``\",\"''\",\"'m\",'--',\"'ll\",\"'d\"]\n",
    "    stopwords_ = set(en_stopwords + additional_list)\n",
    "    return stopwords_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(sentence, sw):\n",
    "    alphabet_ = alphabet(sentence)\n",
    "    tokenized_ = nltk.word_tokenize(alphabet_.lower())\n",
    "    stemmed_ = [st.stem(word) for word in tokenized_ if word not in sw]\n",
    "\n",
    "    return (stemmed_, len(stemmed_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence_list(sentence_list, allowed_length, sw):\n",
    "    np_log_ = []\n",
    "    return_ = []\n",
    "    for idx_, sentence_ in enumerate(sentence_list):\n",
    "        processed_, length_ = process_sentence(sentence_, sw)\n",
    "        if length_ > allowed_length:\n",
    "            return_.append(processed_)\n",
    "        else:\n",
    "            np_log_.append(processed_)\n",
    "\n",
    "    return return_, np_log_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_process_sentence_list(xml_list, length_allowed):\n",
    "    return_ = []\n",
    "    np_ = []\n",
    "    sw = liststopword()\n",
    "    for xml_, la_ in zip(xml_list, length_allowed):\n",
    "        processed_, np_log_ = process_sentence_list(xml_, la_, sw)\n",
    "        return_.append(processed_)\n",
    "        np_.append(np_log_)\n",
    "    \n",
    "    return return_, np_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sentence_list, log_np = get_process_sentence_list(xml_unique_list, length_allowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62348\n",
      "23730\n",
      "13385\n"
     ]
    }
   ],
   "source": [
    "for idx in range(0, len(p_sentence_list)): print (len(p_sentence_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "for idx in range(0, len(p_sentence_list)): label_list.append([idx] * len(p_sentence_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62348\n",
      "23730\n",
      "13385\n"
     ]
    }
   ],
   "source": [
    "for idx in range(0, len(label_list)): print (len(label_list[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_valid(aword):\n",
    "    return aword not in [\"\",\" \"]\n",
    "\n",
    "def create_vocab_listsentence(alist_sentence, amin_freq_allowed):\n",
    "    the_words = []\n",
    "    for sentence_list_ in alist_sentence:\n",
    "        for the_asentence in sentence_list_:\n",
    "            for the_aword in the_asentence:\n",
    "                the_words.append(the_aword)\n",
    "        the_words_freq = nltk.FreqDist(the_words)\n",
    "        the_vocab = []\n",
    "        for the_aword, the_afreq in the_words_freq.items():\n",
    "            if the_afreq > amin_freq_allowed:\n",
    "                if word_valid(the_aword):\n",
    "                    the_vocab.append(the_aword)\n",
    "\n",
    "    the_vocab_sorted = sorted(the_vocab)\n",
    "    #Assign a number corresponding to each word. Makes counting easier.\n",
    "    the_vocab_sorted_dict = dict(zip(the_vocab_sorted, range(len(the_vocab_sorted))))\n",
    "    return the_vocab_sorted, the_vocab_sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, vocab2id = create_vocab_listsentence(p_sentence_list, min_freq_allowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31902"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.9, random_state=0)\n",
    "_, index_train_test = sss.split(data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_, y_ = [], []\n",
    "# for p_sentence, label_ in zip(p_sentence_list, label_list): \n",
    "#     x_.extend(p_sentence)\n",
    "#     y_.extend(label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(x_) == len(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x, test_x, train_y, test_y =  train_test_split(\n",
    "#     x_, y_, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = data[index_train_test[0]], label[index_train_test[0]]\n",
    "test_x, test_y = data[index_train_test[1]], label[index_train_test[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded\n",
      "Dim Training Data 3236 18073\n",
      "Dim Test Data 29126 18073\n"
     ]
    }
   ],
   "source": [
    "print ('Data Loaded')\n",
    "print ('Dim Training Data',train_x.shape[0], vocab_size)\n",
    "print ('Dim Test Data',test_x.shape[0], vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 200\n",
    "en1_units=100\n",
    "en2_units=100\n",
    "num_topic=3\n",
    "num_input=vocab_size\n",
    "variance=0.995\n",
    "init_mult=1.0\n",
    "learning_rate=0.002\n",
    "batch_size=200\n",
    "momentum=0.99\n",
    "num_epoch=100\n",
    "nogpu=True\n",
    "drop_rate=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(o):\n",
    "    if o is None: return []\n",
    "    if isinstance(o, list): return o\n",
    "    if isinstance(o, str): return [o]\n",
    "    if isinstance(o, Iterable): return list(o)\n",
    "    return [o]\n",
    "def setify(o): return o if isinstance(o,set) else set(listify(o))\n",
    "def compose(x, funcs, *args, order_key='_order', **kwargs):\n",
    "    key = lambda o: getattr(o, order_key, 0)\n",
    "    for f in sorted(listify(funcs), key=key): x = f(x, **kwargs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_perp(model):\n",
    "    cost=[]\n",
    "    model.eval()                        # switch to testing mode\n",
    "    input_ = tensor_te\n",
    "    recon, loss = model(input_, compute_loss=True, avg_loss=False)\n",
    "    loss = loss.data\n",
    "    counts = tensor_te.sum(1)\n",
    "    avg = (loss / counts).mean()\n",
    "    print('The approximated perplexity is: ', math.exp(avg))\n",
    "    \n",
    "def print_perp(model):\n",
    "    cost=[]\n",
    "    model.eval()                        # switch to testing mode\n",
    "    input_, _ = next(iter(test_dl))\n",
    "    recon, loss = model(input_, compute_loss=True, avg_loss=False)\n",
    "    loss = loss.data\n",
    "    counts = input_.sum(1)\n",
    "    avg = (loss / counts).mean()\n",
    "    print('The approximated perplexity is: ', math.exp(avg))\n",
    "\n",
    "def print_top_words(beta, feature_names, n_top_words=10):\n",
    "    print ('---------------Printing the Topics------------------')\n",
    "    for i in range(len(beta)):\n",
    "        line = \" \".join([feature_names[j] \n",
    "                         for j in beta[i].argsort()[:-n_top_words - 1:-1]])\n",
    "        print('{}'.format(line))\n",
    "    print ('---------------End of Topics------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    x, y = zip(*b)\n",
    "    return torch.stack(x), torch.stack(y)\n",
    "\n",
    "class IdifyAndLimitedVocab():\n",
    "    _order=-1\n",
    "    def __init__(self, vocab2id, limited_vocab):\n",
    "        self.vocab2id = vocab2id\n",
    "        self.limited_vocab = limited_vocab\n",
    "    def __call__(self, item):\n",
    "        idlist = [self.vocab2id[w] for w in item if self.vocab2id[w] < limited_vocab]\n",
    "        return np.array(idlist)\n",
    "    \n",
    "\n",
    "class Numpyify():\n",
    "    _order=0\n",
    "    def __call__(self, item):\n",
    "        return np.array(item)\n",
    "\n",
    "class Onehotify():\n",
    "    _order=1\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "    def __call__(self, item):\n",
    "        return np.array(np.bincount(item.astype('int'), minlength=self.vocab_size))\n",
    "    \n",
    "class YToOnehot():\n",
    "    _order=1\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "    def __call__(self, item):\n",
    "        categorical = np.zeros((1, self.num_classes))\n",
    "        categorical[0, item] = 1\n",
    "        return categorical\n",
    "\n",
    "class Tensorify():\n",
    "    _order=2\n",
    "    def __call__(self, item):\n",
    "        return torch.from_numpy(item)\n",
    "\n",
    "class Floatify():\n",
    "    _order=3\n",
    "    def __call__(self, item):\n",
    "        return item.float()\n",
    "    \n",
    "class CheckAndCudify():\n",
    "    _order=100\n",
    "    def __init__(self):\n",
    "        self.ic = torch.cuda.is_available()\n",
    "    def __call__(self, item):\n",
    "        return item.cuda() if self.ic else item\n",
    "    \n",
    "class URSADataset(Dataset):\n",
    "    def __init__(self, x, y, tfms_x, tfms_y): \n",
    "        self.x, self.y = x, y\n",
    "        self.x_tfms = tfms_x\n",
    "        self.y_tfms = tfms_y\n",
    "    def __len__(self): \n",
    "        return len(self.x)\n",
    "    def __getitem__(self, i): \n",
    "        return compose(self.x[i], self.x_tfms), compose(self.y[i], self.y_tfms)\n",
    "    \n",
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2id={v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vocab2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = np.max(train_y) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfms_x = [IdifyAndLimitedVocab(vocab2id, vocab_size), Numpyify(), Onehotify(vocab_size=vocab_size), Tensorify(), Floatify(), CheckAndCudify()]\n",
    "tfms_x = [Numpyify(), Onehotify(vocab_size=vocab_size), Tensorify(), Floatify(), CheckAndCudify()]\n",
    "tfms_y = [YToOnehot(num_classes=num_classes), Tensorify(), Floatify(), CheckAndCudify()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = URSADataset(train_x, train_y, tfms_x=tfms_x, tfms_y=tfms_y)\n",
    "test_ds = URSADataset(test_x, test_y, tfms_x=tfms_x, tfms_y=tfms_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = Sampler(train_ds, bs, shuffle=False)\n",
    "test_samp = Sampler(test_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n",
    "test_dl = DataLoader(test_ds, sampler=test_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(in_feature, hidden_feature1, hidden_feature2, drop_rate):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "                ('linear1', nn.Linear(in_feature, hidden_feature1)),\n",
    "                ('act1', nn.Softplus()),\n",
    "                ('linear2', nn.Linear(hidden_feature1, hidden_feature2)),\n",
    "                ('act2', nn.Softplus()),\n",
    "                ('dropout', nn.Dropout(drop_rate))\n",
    "            ]))\n",
    "\n",
    "def decoder(in_feature, out_feature, drop_rate):\n",
    "     return nn.Sequential(OrderedDict([\n",
    "                ('act1', nn.Softmax(dim=-1)),\n",
    "                ('dropout', nn.Dropout(drop_rate)),\n",
    "                ('linear', nn.Linear(in_feature, out_feature)),\n",
    "                ('batchnorm', nn.BatchNorm1d(out_feature)),\n",
    "                ('act2', nn.Softmax(dim=-1))\n",
    "            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden(in_feature, out_feature):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "                ('linear', nn.Linear(in_feature, out_feature)),\n",
    "                ('batchnorm', nn.BatchNorm1d(out_feature))\n",
    "            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProdLDA(nn.Module):\n",
    "    def __init__(self, num_input, en1_units, en2_units, num_topic, drop_rate, init_mult):\n",
    "        super(ProdLDA, self).__init__()\n",
    "        self.num_input, self.en1_units, self.en2_units, \\\n",
    "        self.num_topic, self.drop_rate, self.init_mult = num_input, en1_units, en2_units, \\\n",
    "                                                            num_topic, drop_rate, init_mult\n",
    "        # encoder\n",
    "        self.en = encoder(num_input, en1_units, en2_units, drop_rate)\n",
    "        self.mean = hidden(en2_units, num_topic)\n",
    "        self.logvar = hidden(en2_units, num_topic)\n",
    "        # decoder\n",
    "        self.de = decoder(num_topic, num_input, drop_rate)\n",
    "        # prior mean and variance as constant buffers\n",
    "        self.prior_mean   = torch.Tensor(1, num_topic).fill_(0)\n",
    "        self.prior_var    = torch.Tensor(1, num_topic).fill_(variance)\n",
    "        self.prior_mean   = nn.Parameter(self.prior_mean, requires_grad=False)\n",
    "        self.prior_var    = nn.Parameter(self.prior_var, requires_grad=False)\n",
    "        self.prior_logvar = nn.Parameter(self.prior_var.log(), requires_grad=False)\n",
    "        # initialize decoder weight\n",
    "        if init_mult != 0:\n",
    "            #std = 1. / math.sqrt( init_mult * (num_topic + num_input))\n",
    "            self.de.linear.weight.data.uniform_(0, init_mult)\n",
    "        # remove BN's scale parameters\n",
    "        for component in [self.mean, self.logvar, self.de]:\n",
    "            component.batchnorm.weight.requires_grad = False\n",
    "            component.batchnorm.weight.fill_(1.0)\n",
    "\n",
    "    def encode(self, input_):\n",
    "        encoded = self.en(input_)\n",
    "        posterior_mean = self.mean(encoded)\n",
    "        posterior_logvar = self.logvar(encoded)\n",
    "        return encoded, posterior_mean, posterior_logvar\n",
    "    \n",
    "    def decode(self, input_, posterior_mean, posterior_var):\n",
    "        # take sample\n",
    "        eps = input_.data.new().resize_as_(posterior_mean.data).normal_() # noise \n",
    "        z = posterior_mean + posterior_var.sqrt() * eps                   # reparameterization\n",
    "        # do reconstruction\n",
    "        recon = self.de(z)          # reconstructed distribution over vocabulary\n",
    "        return recon\n",
    "    \n",
    "    def forward(self, input_, compute_loss=False, avg_loss=True):\n",
    "        # compute posterior\n",
    "        en2, posterior_mean, posterior_logvar = self.encode(input_) \n",
    "        posterior_var    = posterior_logvar.exp()\n",
    "        \n",
    "        recon = self.decode(input_, posterior_mean, posterior_var)\n",
    "        if compute_loss:\n",
    "            return recon, self.loss(input_, recon, posterior_mean, posterior_logvar, posterior_var, avg_loss)\n",
    "        else:\n",
    "            return recon\n",
    "\n",
    "    def loss(self, input_, recon, posterior_mean, posterior_logvar, posterior_var, avg=True):\n",
    "        # NL\n",
    "        NL  = -(input_ * (recon + 1e-10).log()).sum(1)\n",
    "        # KLD, see Section 3.3 of Akash Srivastava and Charles Sutton, 2017, \n",
    "        # https://arxiv.org/pdf/1703.01488.pdf\n",
    "        prior_mean   = self.prior_mean.expand_as(posterior_mean)\n",
    "        prior_var    = self.prior_var.expand_as(posterior_mean)\n",
    "        prior_logvar = self.prior_logvar.expand_as(posterior_mean)\n",
    "        var_division    = posterior_var  / prior_var\n",
    "        diff            = posterior_mean - prior_mean\n",
    "        diff_term       = diff * diff / prior_var\n",
    "        logvar_division = prior_logvar - posterior_logvar\n",
    "        # put KLD together\n",
    "        KLD = 0.5 * ( (var_division + diff_term + logvar_division).sum(1) - self.num_topic)\n",
    "        # loss\n",
    "        loss = (NL + KLD)\n",
    "        # in traiming mode, return averaged loss. In testing mode, return individual loss\n",
    "        if avg:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProdLDA(num_input, en1_units, en2_units, num_topic, drop_rate, init_mult)\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate, betas=(momentum, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss=62.29949972364638\n",
      "---------------Printing the Topics------------------\n",
      "hip accomplish deliveri robot roam load throughout treat u light pollack swallow includ goa xeo earn blame timer save pie dedic num huckleberri round honest throw head hoomoo cancel flank standbi lemoni brim array addict tast oeu ett buoy cram someli store pequena attitud chewi shortag bland birthday cabernet sure\n",
      "bag strictli chosen evil ice broken danc amend driver restaur buco around normal ugh spoil surround vic made picki thailand novelti cram thick guess ethnic fukin manchego dinnerbrok long charg dissapoint dine sniffli skit buncha comfort meanwhil burbur finger capabl session fondli membran pc ele swordfish separetli folklor lemongrass suprem\n",
      "breakfast horribl project outdoor jacqu rocco honest slice hipster antipasto wore skirt fruit dd forno transplant anyon h knudi understand shabu pair unexpect gamey duti jelli parmesean slow pack nowher scratch tomato advis line alon driven buttermilk casa vip pitch chowfun confid satisif typic hangawi leek rank bounc qualin bento\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  19802.514909401514\n",
      "Epoch 5, loss=59.73249774509006\n",
      "---------------Printing the Topics------------------\n",
      "includ light music top honest flavor tast treat throughout goa attitud hip overpow cancel throw layer round albeit deliveri greek easili accomplish oliv modern owner blame seaport array timer load num u poo store monkey chewi sure del roam strictli name loudli cram scream bland celeb forno chain pie shortag\n",
      "restaur strictli broken dine great ethnic comfort evil fukin paint manchego charg surround bag sniffli buoy thailand cram someli pequena driver skyrocket foundat keep danc ice fetish offer space closest long buco descript crepe sancho around dinnerbrok tao griddl finger folklor panel sangaria chain ele breakfast amend citarella dd inger\n",
      "horribl understand slow outdoor breakfast duti jelli knudi n pitch skirt pearl antipasto buttermilk wore empti pack brought jacqu slice lock hipster whenev right hangawi shrimp parmesean water typic find point peach moon line forno buddah confid site lip behind without thta anyon bounc driven handsom project snack bday casa\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  14662.75985924199\n",
      "Epoch 10, loss=57.79016664293077\n",
      "---------------Printing the Topics------------------\n",
      "music flavor top includ light tast veget oliv modern goa overpow del boomer easili honest greek chewi forno monkey spici scallion round throughout hip ravioli sure heaven save layer sauc non patron poo fleck pie bland overli upstair attitud load line cancel owner num arroz moist ether treat brais forev\n",
      "restaur great dine strictli ethnic space broken paint charg keep surround comfort level chain panel skyrocket locat wooden offer cram unfortun select sit danc thailand energi anyhow manchego around evil dinnerbrok descript fukin tao defint floor noodl sniffli cholesterol someth driver whatsoev buco boast oklahoma typic emerg copper casa send\n",
      "horribl understand slow minut n brought shrimp empti knudi pack water right point anyon without duti parmesean antipasto second find pitch jelli ful outdoor entre slice lip behind thta countri wore site peach whenev beer cutlet lock receiv peopl deliv mid bother assum confid pequena pearl lemoni everi foundat buoy\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  11638.445100646853\n",
      "Epoch 15, loss=56.29256841871474\n",
      "---------------Printing the Topics------------------\n",
      "music flavor light top includ veget modern tast del oliv boomer sauc spici ravioli easili goa scallion overpow chewi beauti perfect heaven greek delici non insid forno line soft monkey save hous num upstair moist patron seafood crispi poo arroz overli hip span appl pie fleck layer bland potpi round\n",
      "restaur great space dine sit strictli ethnic paint broken keep level locat chain offer someth panel typic unfortun charg tell comfort around cram floor event wooden skyrocket sushi surround emerg two energi noodl anyhow amaz select dinnerbrok dumpl boast small defint cholesterol tao style decor send menu thailand beam evil\n",
      "minut horribl understand slow n brought shrimp empti point entre without ful right second water anyon receiv deliv manag knudi peopl go find parmesean everi behind pack made pitch bother antipasto cutlet countri assum beer peach lip wore thta back duti whenev mid wine outdoor venison confid wait lemoni custom\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  9412.425861561516\n",
      "Epoch 20, loss=54.93141280280219\n",
      "---------------Printing the Topics------------------\n",
      "music flavor light top veget boomer modern del includ oliv sauc ravioli spici perfect tast beauti scallion easili insid delici chewi non heaven crispi hous goa appl larg soft line seafood rib greek overpow num tender bread span forno steak delic cheddar save roll monkey addict upstair creat moist balanc\n",
      "restaur great space sit dine locat keep strictli someth chain paint level ethnic typic broken two offer panel sushi style unfortun tell event dumpl cram around menu amaz floor small decor wooden comfort emerg charg beam energi select skyrocket boast anyhow mix la cholesterol trek surround stranger noodl send frite\n",
      "minut horribl slow understand n brought shrimp manag point entre empti without go ful peopl right receiv deliv water made second everi find assum behind anyon back bother wait knudi order beer lip pitch wore countri parmesean deliveri thta custom hostess cutlet antipasto wine peach pack whenev mid ensur duti\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  7721.93588226834\n",
      "Epoch 25, loss=53.704487058851456\n",
      "---------------Printing the Topics------------------\n",
      "flavor music top veget light boomer modern ravioli del perfect oliv sauc spici includ beauti tender crispi scallion insid delici appl tast roll rib easili larg hous non soft chewi seafood heaven cheddar delic steak line num goa span bread chocol filet ice rich chili addict fall balanc overpow forno\n",
      "restaur great space sit locat dine someth keep chain typic paint two style level strictli panel sushi ethnic dumpl menu event broken tell decor unfortun offer cram floor small around amaz comfort beam wooden energi emerg trek mix la diner select stranger bistro jackson size boast parti skyrocket cholesterol anyhow\n",
      "minut horribl slow understand brought manag n point go empti peopl without ful shrimp entre made receiv right deliv water back second everi hostess assum order behind wait bother find attent deliveri lip beer knudi custom bartend wine wore pitch staff unattent ask ensur countri guest thta anyon whenev move\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  6518.84559532689\n",
      "Epoch 30, loss=52.59666845533583\n",
      "---------------Printing the Topics------------------\n",
      "flavor music veget top boomer light perfect ravioli tender modern del crispi oliv roll sauc spici beauti includ scallion rib insid appl delici larg cheddar delic seafood hous soft non easili steak ice rich chewi filet tast chili heaven num chocol line thick span cook goat bean balanc fall bread\n",
      "restaur great space sit locat someth typic chain keep dine paint style menu level two strictli sushi panel dumpl floor ethnic event small decor cram tell unfortun trek beam broken diner around energi amaz parti offer comfort food mix emerg size la jackson wooden copper select friend tao stranger last\n",
      "minut horribl slow manag understand brought peopl point empti without made go ful n water receiv right back deliv entre attent hostess shrimp second bother everi order assum wait behind find deliveri bartend lip ask custom beer min wine unattent ensur knudi guest staff pitch reserv wore spent move replac\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  5492.4346147058395\n",
      "Epoch 35, loss=51.596008724636505\n",
      "---------------Printing the Topics------------------\n",
      "flavor veget music top boomer tender roll perfect crispi ravioli modern light del oliv beauti spici sauc rib includ scallion appl cheddar delic seafood insid delici ice larg rich chili filet soft steak non hous goat easili thick bean bass num cook chewi homemad chocol heaven balanc tasti fall span\n",
      "restaur great space sit locat typic someth chain keep paint dine style menu dumpl food floor panel strictli small level sushi two cram decor ethnic trek event beam parti diner tell size energi around come friend unfortun copper broken mix jackson emerg amaz clad last dish la stranger tao comfort\n",
      "minut horribl manag slow brought understand peopl made point without empti go ful water back n receiv right attent hostess bother deliv bartend second order assum everi deliveri ask wait entre behind lip min find custom unattent reserv beer ensur guest replac wine treat knudi shrimp move staff pitch spent\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  4773.675944504079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, loss=50.72814581129286\n",
      "---------------Printing the Topics------------------\n",
      "flavor veget roll top crispi boomer tender perfect music ravioli modern del oliv light beauti includ rib cheddar scallion seafood spici delic appl sauc ice rich chili filet insid larg delici bass soft goat steak bean thick gra homemad non cook noodl tasti hous num balanc easili chewi fall prosciutto\n",
      "restaur great space sit locat typic someth chain paint food style keep dine menu small panel dumpl floor cram strictli decor level sushi ethnic diner parti beam two trek come size friend copper clad event energi jackson citi tell ba last around broken ehhhh mix emerg dish unfortun ambianc stranger\n",
      "minut horribl manag slow brought peopl made point understand without empti back ful go water receiv n right attent hostess bartend bother deliv ask deliveri second order assum lip min everi wait custom behind treat reserv replac unattent guest find ensur wine move knudi beer promptli hour liquor entre duti\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  4180.301199929799\n",
      "Epoch 45, loss=49.94643232557509\n",
      "---------------Printing the Topics------------------\n",
      "roll veget flavor crispi top boomer tender perfect ravioli includ music oliv beauti del cheddar seafood modern rib delic scallion chili appl rich filet light ice gra spici bass goat bean soft delici larg insid noodl sauc homemad thick steak tasti cook brais non num prosciutto balanc curri fall spring\n",
      "restaur great space sit locat typic food someth chain paint dine style keep small menu panel floor dumpl cram decor strictli come diner parti beam level ethnic clad trek size sushi friend copper ba two energi citi event jackson ehhhh hip last ambianc throw rugbi broken burger upper tao stranger\n",
      "minut manag horribl slow peopl made brought back without point ful understand empti water go receiv n attent right bartend hostess bother ask deliveri deliv second lip hour treat min reserv order custom assum replac wait unattent behind everi guest promptli wine ensur littl move knudi host ten duti mid\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  3677.8337138427573\n",
      "Epoch 50, loss=49.30565219455295\n",
      "---------------Printing the Topics------------------\n",
      "roll veget crispi flavor top boomer tender perfect ravioli includ cheddar seafood scallion delic rib chili beauti oliv del gra filet rich bass appl ice noodl goat bean soft spici modern homemad delici light thick music tasti larg cook insid brais steak mushroom sauc prosciutto num spring nut curri balanc\n",
      "restaur great space sit food locat typic someth chain dine paint style keep small menu panel floor decor cram come strictli dumpl diner beam clad ethnic trek parti ba size level friend copper energi citi hip ehhhh jackson sushi event throw ambianc rugbi cozi two last tao upper comfort burger\n",
      "minut manag horribl peopl slow made back without brought ful point water empti understand go receiv n attent bartend right hour ask hostess bother deliveri treat lip reserv deliv second min custom promptli replac order unattent wait assum wine guest littl ensur behind move readi ten host polit mid knudi\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  3400.8984629507013\n",
      "Epoch 55, loss=48.68127335442437\n",
      "---------------Printing the Topics------------------\n",
      "roll veget crispi flavor top boomer perfect tender ravioli includ cheddar chili scallion gra seafood delic rib noodl del oliv filet bass beauti ice rich appl bean goat soft homemad mushroom thick brais spici delici tasti cook steak sauc spring larg prosciutto insid num light slab bed balanc butteri nut\n",
      "restaur great space sit food locat typic dine chain someth paint small decor style keep panel menu cram floor ba come strictli clad beam diner ethnic trek hip dumpl energi level citi ehhhh copper size friend parti event throw jackson ambianc cozi rugbi comfort tao thailand hang garden date bar\n",
      "minut manag peopl back horribl made slow without brought ful point water empti understand go n receiv bartend attent hour right ask bother hostess treat deliveri lip reserv promptli deliv min custom replac second unattent wine littl order guest wait ensur readi assum ten mid move polit host behind knudi\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  3076.0912202285454\n",
      "Epoch 60, loss=48.13664881388346\n",
      "---------------Printing the Topics------------------\n",
      "roll crispi veget flavor top boomer tender perfect ravioli includ cheddar gra chili scallion noodl seafood delic rib filet bass oliv ice del rich appl goat bean mushroom soft brais homemad thick beauti spici delici tasti spring sauc cook prosciutto butteri steak num slab bed balanc nut crepe breakfast tangi\n",
      "restaur great space food sit locat typic dine chain paint someth decor small panel ba clad cram hip style floor keep menu beam ethnic strictli diner come trek ehhhh citi energi copper level friend size dumpl event jackson throw comfort tao cozi ambianc thailand bar rugbi date garden hang u\n",
      "minut back manag peopl horribl made slow without water point brought ful empti n understand go hour bartend receiv attent ask bother right hostess treat promptli lip reserv deliveri custom replac min deliv wine unattent littl guest readi second order ten mid ensur wait polit assum move host duti sniffli\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  2814.3316483330414\n",
      "Epoch 65, loss=47.744188732571075\n",
      "---------------Printing the Topics------------------\n",
      "roll crispi veget flavor boomer top tender perfect includ ravioli cheddar gra chili noodl scallion seafood delic filet rib bass ice goat brais mushroom oliv bean rich appl soft homemad del thick spici delici tasti butteri spring prosciutto sauc bed slab num crepe steak balanc cook tangi nut polenta good\n",
      "restaur great food space sit locat typic dine chain paint decor small someth panel ba clad hip cram floor ethnic beam strictli style diner ehhhh citi trek copper energi keep menu level come friend comfort jackson event throw tao cozi date bar thailand u rugbi size ambianc garden hang upper\n",
      "minut back manag peopl horribl made slow without water point ful n brought empti understand hour bartend go receiv attent ask bother promptli hostess treat lip right reserv deliveri custom wine unattent replac readi guest littl min ten mid deliv second ensur wait order polit duti bday host assum section\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  2660.320434239915\n",
      "Epoch 70, loss=47.29426087273492\n",
      "---------------Printing the Topics------------------\n",
      "roll crispi veget boomer flavor top tender perfect includ ravioli cheddar gra noodl scallion chili seafood delic filet rib bass brais mushroom goat ice bean soft rich oliv appl homemad thick butteri spici delici spring del prosciutto bed tasti sauc slab num crepe tangi balanc polenta good steak nut breakfast\n",
      "restaur great food space sit locat dine typic chain decor paint clad ba hip small panel floor someth cram ethnic beam strictli ehhhh diner copper energi citi trek style comfort level event jackson friend u date rugbi tao throw cozi bar thailand ambianc come upper real garden keep neat hang\n",
      "back minut manag horribl peopl slow made n water ful point without empti brought understand hour receiv attent bartend go ask bother promptli lip treat hostess reserv deliveri wine right readi guest custom ten unattent replac mid second littl min owner ensur wait name polit bday deliv duti section book\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  2478.9417732550996\n",
      "Epoch 75, loss=46.95052507188585\n",
      "---------------Printing the Topics------------------\n",
      "roll crispi veget boomer flavor top tender includ gra cheddar scallion noodl perfect ravioli chili seafood delic filet rib brais bass mushroom goat soft bean ice butteri rich appl thick homemad oliv spici bed delici prosciutto crepe spring slab num sauc tangi tasti balanc polenta del mignon good nut steak\n",
      "restaur food great space sit locat dine chain decor typic paint hip clad ba panel small floor beam ethnic strictli cram ehhhh copper someth energi diner trek citi comfort event style level wooden u date jackson rugbi real cozi tao friend bar ambianc hang neat thailand upper throw sophist bounc\n",
      "back minut manag horribl slow n made peopl ful water point without empti brought understand hour receiv attent go ask bartend promptli bother lip hostess treat wine reserv ten readi deliveri guest owner mid unattent custom replac wait second ensur right name bday polit min duti book littl section offer\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  2316.77405518051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, loss=46.62566799587674\n",
      "---------------Printing the Topics------------------\n",
      "roll crispi boomer veget flavor top tender includ scallion gra cheddar noodl chili ravioli seafood filet perfect delic rib brais bass mushroom goat butteri soft bean thick appl oliv rich ice homemad spici crepe bed prosciutto sauc num slab spring delici tangi mignon polenta balanc good bland sausag tasti nut\n",
      "food restaur great space sit locat dine chain decor paint hip clad ba typic panel small beam ethnic ehhhh floor strictli cram copper energi wooden trek diner comfort event citi level u someth real rugbi date cozi neat jackson ambianc student thailand sophist hang upper style bar tao throw bounc\n",
      "back minut manag n horribl slow ful water made point empti without understand brought peopl hour receiv attent promptli ask bother bartend go lip wine hostess owner ten reserv readi mid treat guest wait unattent replac custom deliveri book ensur name second bday duti polit offer rd min equip regularli\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  2190.677327209781\n",
      "Epoch 85, loss=46.335666020711265\n",
      "---------------Printing the Topics------------------\n",
      "roll crispi boomer veget flavor top tender scallion noodl gra cheddar includ chili ravioli seafood filet delic brais rib mushroom bass butteri goat bean soft thick sauc perfect spici appl homemad crepe bed num rich prosciutto oliv slab ice spring mignon tangi delici polenta balanc bland sausag char good nut\n",
      "food restaur great space locat sit dine paint decor hip chain clad ba panel small typic ethnic beam ehhhh wooden strictli copper energi floor cram trek comfort diner event u citi level rugbi sophist real neat student cozi thailand date music upper ambianc hang bounc tao throw bar jackson someth\n",
      "back minut manag n horribl slow ful water point empti made understand without brought hour receiv promptli attent ask bother peopl lip wine bartend owner ten readi hostess go mid wait reserv guest replac unattent book custom ensur deliveri treat name duti polit second rd bday equip section repli regularli\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  2127.842280448682\n",
      "Epoch 90, loss=46.163561291164825\n",
      "---------------Printing the Topics------------------\n",
      "roll crispi boomer veget flavor top cheddar noodl gra scallion tender includ chili ravioli seafood filet brais delic butteri rib mushroom bass goat sauc thick bean num soft spici crepe bed prosciutto slab appl homemad rich oliv mignon tangi spring polenta ice delici balanc char bland sausag overpow span nut\n",
      "food restaur great space locat dine sit paint hip decor clad ba chain panel wooden ethnic ehhhh small beam strictli energi copper typic comfort cram floor trek music u event diner rugbi student sophist neat citi cozi bounc upper level thailand real date ambianc hang tao pitch garden throw bar\n",
      "back minut n manag slow horribl ful water understand empti point made brought without hour receiv promptli attent ask bother lip wine owner readi ten wait mid hostess bartend guest reserv book replac ensur unattent custom name duti rd go polit deliveri bday equip repli peopl second regularli treat section\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  2058.2021276329106\n",
      "Epoch 95, loss=45.89967558119032\n",
      "---------------Printing the Topics------------------\n",
      "roll crispi boomer veget flavor top cheddar noodl gra scallion includ tender chili ravioli brais seafood filet delic butteri mushroom sauc rib bass goat num crepe soft thick spici bed bean slab prosciutto appl homemad tangi mignon rich spring polenta oliv balanc char delici ice bland sausag overpow wasabi span\n",
      "food restaur great space locat dine paint hip clad decor ba sit chain wooden panel ehhhh beam ethnic energi small strictli copper comfort music cram trek floor typic student u event rugbi sophist neat bounc diner upper cozi date thailand pitch tao level citi hang real ambianc garden asia throw\n",
      "n minut back manag slow ful horribl understand water empti point brought hour made receiv promptli without attent ask bother lip wine readi ten owner wait mid guest hostess book reserv bartend replac ensur rd duti unattent name bday polit custom equip deliveri repli regularli second sniffli suggest section treat\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1990.2318001703006\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    loss_epoch = 0.0\n",
    "    model.train()                    # switch to training mode\n",
    "    for input_, _ in train_dl:\n",
    "        recon, loss = model(input_, compute_loss=True)\n",
    "        # optimize\n",
    "        optimizer.zero_grad()        # clear previous gradients\n",
    "        loss.backward()              # backprop\n",
    "        optimizer.step()             # update parameters\n",
    "        # report\n",
    "        loss_epoch += loss.item()    # add loss to loss_epoch\n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch {}, loss={}'.format(epoch, loss_epoch / len(input_)))\n",
    "        emb = model.de.linear.weight.data.cpu().numpy().T\n",
    "        print_top_words(emb, vocab, 50)\n",
    "        print_perp(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Printing the Topics------------------\n",
      "beat bao bore breakfast annoyed camouflage blatantly bimbimbop adobada afternoon\n",
      "adve aestethically ayurveda appeteasers accosted calamarie adventure appertizer aday beautyfull\n",
      "bellini caribbean broaden bangersmash caters blander breasaola cercle alice admit\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  inf\n"
     ]
    }
   ],
   "source": [
    "emb = model.de.linear.weight.data.cpu().numpy().T\n",
    "print_top_words(emb, vocab)\n",
    "print_perp(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avi",
   "language": "python",
   "name": "avi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
