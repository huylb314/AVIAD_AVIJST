{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import groupby\n",
    "from sklearn import datasets\n",
    "from numpy import random\n",
    "from scipy.stats import dirichlet, norm, poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.datasets import reuters, imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "import pickle, gzip, math, torch, matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URSA Datasets 10K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_ds_path = Path('./data/User Review Structure Analysis (URSA)/')\n",
    "xml_path = (folder_ds_path/'Classified_Corpus.xml')\n",
    "ds_path = (folder_ds_path/'10k')\n",
    "sentence_npy_path = (folder_ds_path/'sentence.npy')\n",
    "vocab_pkl_path = (folder_ds_path/'vocab.pkl')\n",
    "\n",
    "# log words not pass\n",
    "aspect_tags = ['Food', 'Staff', 'Ambience']\n",
    "polatiry_tags = ['Positive', 'Negative', 'Neutral']\n",
    "xml_review_tag = './/Review'\n",
    "log_np = [[], [], []]\n",
    "\n",
    "# length allowed sentences\n",
    "length_allowed = [11, 7, 4]\n",
    "min_freq_allowed = -1\n",
    "\n",
    "test_size = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URSA Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_nested_xml(axml):\n",
    "    return ' '.join([the_aiter for the_aiter in axml.itertext()])\n",
    "\n",
    "def get_firstchild(axml):\n",
    "    try:\n",
    "        if len(axml.getchildren()) > 0:\n",
    "            return axml.getchildren()[0].tag\n",
    "        else:\n",
    "            raise (Exception('ListIndex', 'aXmlElement input has no children.'))\n",
    "    except Exception as e:\n",
    "        print (str(e))\n",
    "\n",
    "def xml_unique_valid(axml, alist_tag_allowed):\n",
    "    return (len(axml.getchildren()) == 0) or (get_firstchild(axml) in alist_tag_allowed)\n",
    "\n",
    "def xml_name_valid(axml, atag_name):\n",
    "    return axml.tag == atag_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listsentence_unique(alist_xml, alist_tag_allowed):\n",
    "    the_listsentence = []\n",
    "    for the_axml in alist_xml:\n",
    "        if xml_unique_valid(the_axml, alist_tag_allowed):\n",
    "            the_listsentence.append(string_nested_xml(the_axml))\n",
    "\n",
    "    return the_listsentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listxml_child(list_xml, tag):\n",
    "    return_ = []\n",
    "    for xml_ in list_xml:\n",
    "        for xml_child in xml_:\n",
    "            if xml_name_valid(xml_child, tag):\n",
    "                return_.append(xml_child)\n",
    "\n",
    "    return return_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listxml_child_list(document_list, tag_list):\n",
    "    return_ = []\n",
    "    for tag in tag_list:\n",
    "        xml_children = get_listxml_child(document_list, tag)\n",
    "        return_.append(xml_children)\n",
    "    \n",
    "    return return_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xml_unique_list(xml_children_list, polatiry_tags):\n",
    "    return_ = []\n",
    "    for xml_children in xml_children_list:\n",
    "        xml_unique = get_listsentence_unique(xml_children, polatiry_tags)\n",
    "        return_.append(xml_unique)\n",
    "    \n",
    "    return return_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tree = ET.parse(xml_path)\n",
    "corpus_root = corpus_tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_list = corpus_root.findall(xml_review_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96235\n",
      "32892\n",
      "16803\n"
     ]
    }
   ],
   "source": [
    "xml_children_list = get_listxml_child_list(document_list, aspect_tags)\n",
    "for idx in range(0, len(xml_children_list)): print (len(xml_children_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62348\n",
      "23730\n",
      "13385\n"
     ]
    }
   ],
   "source": [
    "xml_unique_list = get_xml_unique_list(xml_children_list, polatiry_tags)\n",
    "for idx in range(0, len(xml_unique_list)): print (len(xml_unique_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The food is a melding of Moroccan comfort food and Spanish tapas fare : tagines , stews and salads , with surprises like baby eggplants and olives where you might not expect them . '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_unique_list[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/huylb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/huylb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabet(atext):\n",
    "    return re.sub(\"[^a-zA-Z]\", \" \", atext)\n",
    "\n",
    "def liststopword():\n",
    "    en_stopwords = list(nltk.corpus.stopwords.words(\"english\"))\n",
    "    additional_list = [\"'s\",\"...\",\"'ve\",\"``\",\"''\",\"'m\",'--',\"'ll\",\"'d\"]\n",
    "    stopwords_ = set(en_stopwords + additional_list)\n",
    "    return stopwords_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(sentence, sw):\n",
    "    alphabet_ = alphabet(sentence)\n",
    "    tokenized_ = nltk.word_tokenize(alphabet_.lower())\n",
    "    stemmed_ = [st.stem(word) for word in tokenized_ if word not in sw]\n",
    "\n",
    "    return (stemmed_, len(stemmed_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence_list(sentence_list, allowed_length, sw):\n",
    "    np_log_ = []\n",
    "    return_ = []\n",
    "    for idx_, sentence_ in enumerate(sentence_list):\n",
    "        processed_, length_ = process_sentence(sentence_, sw)\n",
    "        if length_ > allowed_length:\n",
    "            return_.append(processed_)\n",
    "        else:\n",
    "            np_log_.append(processed_)\n",
    "\n",
    "    return return_, np_log_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_process_sentence_list(xml_list, length_allowed):\n",
    "    return_ = []\n",
    "    np_ = []\n",
    "    sw = liststopword()\n",
    "    for xml_, la_ in zip(xml_list, length_allowed):\n",
    "        processed_, np_log_ = process_sentence_list(xml_, la_, sw)\n",
    "        return_.append(processed_)\n",
    "        np_.append(np_log_)\n",
    "    \n",
    "    return return_, np_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sentence_list, log_np = get_process_sentence_list(xml_unique_list, length_allowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11547\n",
      "10175\n",
      "10640\n"
     ]
    }
   ],
   "source": [
    "for idx in range(0, len(p_sentence_list)): print (len(p_sentence_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "for idx in range(0, len(p_sentence_list)): label_list.append([idx] * len(p_sentence_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11547\n",
      "10175\n",
      "10640\n"
     ]
    }
   ],
   "source": [
    "for idx in range(0, len(label_list)): print (len(label_list[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_valid(aword):\n",
    "    return aword not in [\"\",\" \"]\n",
    "\n",
    "def create_vocab_listsentence(alist_sentence, amin_freq_allowed):\n",
    "    the_words = []\n",
    "    for sentence_list_ in alist_sentence:\n",
    "        for the_asentence in sentence_list_:\n",
    "            for the_aword in the_asentence:\n",
    "                the_words.append(the_aword)\n",
    "        the_words_freq = nltk.FreqDist(the_words)\n",
    "        the_vocab = []\n",
    "        for the_aword, the_afreq in the_words_freq.items():\n",
    "            if the_afreq > amin_freq_allowed:\n",
    "                if word_valid(the_aword):\n",
    "                    the_vocab.append(the_aword)\n",
    "\n",
    "    the_vocab_sorted = sorted(the_vocab)\n",
    "    #Assign a number corresponding to each word. Makes counting easier.\n",
    "    the_vocab_sorted_dict = dict(zip(the_vocab_sorted, range(len(the_vocab_sorted))))\n",
    "    return the_vocab_sorted, the_vocab_sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, vocab2id = create_vocab_listsentence(p_sentence_list, min_freq_allowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_, y_ = [], []\n",
    "for p_sentence, label_ in zip(p_sentence_list, label_list): \n",
    "    x_.extend(p_sentence)\n",
    "    y_.extend(label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_) == len(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y =  train_test_split(\n",
    "    x_, y_, test_size=test_size, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded\n",
      "Dim Training Data 29125 18073\n",
      "Dim Test Data 3237 18073\n"
     ]
    }
   ],
   "source": [
    "print ('Data Loaded')\n",
    "print ('Dim Training Data',len(train_x), vocab_size)\n",
    "print ('Dim Test Data', len(test_x), vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 200\n",
    "en1_units=100\n",
    "en2_units=100\n",
    "num_topic=3\n",
    "num_input=vocab_size\n",
    "variance=0.995\n",
    "init_mult=1.0\n",
    "learning_rate=0.002\n",
    "batch_size=200\n",
    "momentum=0.99\n",
    "num_epoch=100\n",
    "nogpu=True\n",
    "drop_rate=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(o):\n",
    "    if o is None: return []\n",
    "    if isinstance(o, list): return o\n",
    "    if isinstance(o, str): return [o]\n",
    "    if isinstance(o, Iterable): return list(o)\n",
    "    return [o]\n",
    "def setify(o): return o if isinstance(o,set) else set(listify(o))\n",
    "def compose(x, funcs, *args, order_key='_order', **kwargs):\n",
    "    key = lambda o: getattr(o, order_key, 0)\n",
    "    for f in sorted(listify(funcs), key=key): x = f(x, **kwargs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_perp(model):\n",
    "    cost=[]\n",
    "    model.eval()                        # switch to testing mode\n",
    "    input_ = tensor_te\n",
    "    recon, loss = model(input_, compute_loss=True, avg_loss=False)\n",
    "    loss = loss.data\n",
    "    counts = tensor_te.sum(1)\n",
    "    avg = (loss / counts).mean()\n",
    "    print('The approximated perplexity is: ', math.exp(avg))\n",
    "    \n",
    "def print_perp(model):\n",
    "    cost=[]\n",
    "    model.eval()                        # switch to testing mode\n",
    "    input_, _ = next(iter(test_dl))\n",
    "    recon, loss = model(input_, compute_loss=True, avg_loss=False)\n",
    "    loss = loss.data\n",
    "    counts = input_.sum(1)\n",
    "    avg = (loss / counts).mean()\n",
    "    print('The approximated perplexity is: ', math.exp(avg))\n",
    "\n",
    "def print_top_words(beta, feature_names, n_top_words=10):\n",
    "    print ('---------------Printing the Topics------------------')\n",
    "    for i in range(len(beta)):\n",
    "        line = \" \".join([feature_names[j] \n",
    "                         for j in beta[i].argsort()[:-n_top_words - 1:-1]])\n",
    "        print('{}'.format(line))\n",
    "    print ('---------------End of Topics------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    x, y = zip(*b)\n",
    "    return torch.stack(x), torch.stack(y)\n",
    "\n",
    "class IdifyAndLimitedVocab():\n",
    "    _order=-1\n",
    "    def __init__(self, vocab2id, limited_vocab):\n",
    "        self.vocab2id = vocab2id\n",
    "        self.limited_vocab = limited_vocab\n",
    "    def __call__(self, item):\n",
    "        idlist = [self.vocab2id[w] for w in item if self.vocab2id[w] < self.limited_vocab]\n",
    "        return np.array(idlist)\n",
    "    \n",
    "\n",
    "class Numpyify():\n",
    "    _order=0\n",
    "    def __call__(self, item):\n",
    "        return np.array(item)\n",
    "\n",
    "class Onehotify():\n",
    "    _order=1\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "    def __call__(self, item):\n",
    "        return np.array(np.bincount(item.astype('int'), minlength=self.vocab_size))\n",
    "    \n",
    "class YToOnehot():\n",
    "    _order=1\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "    def __call__(self, item):\n",
    "        categorical = np.zeros((1, self.num_classes))\n",
    "        categorical[0, item] = 1\n",
    "        return categorical\n",
    "\n",
    "class Tensorify():\n",
    "    _order=2\n",
    "    def __call__(self, item):\n",
    "        return torch.from_numpy(item)\n",
    "\n",
    "class Floatify():\n",
    "    _order=3\n",
    "    def __call__(self, item):\n",
    "        return item.float()\n",
    "    \n",
    "class CheckAndCudify():\n",
    "    _order=100\n",
    "    def __init__(self):\n",
    "        self.ic = torch.cuda.is_available()\n",
    "    def __call__(self, item):\n",
    "        return item.cuda() if self.ic else item\n",
    "    \n",
    "class URSADataset(Dataset):\n",
    "    def __init__(self, x, y, tfms_x, tfms_y): \n",
    "        self.x, self.y = x, y\n",
    "        self.x_tfms = tfms_x\n",
    "        self.y_tfms = tfms_y\n",
    "    def __len__(self): \n",
    "        return len(self.x)\n",
    "    def __getitem__(self, i): \n",
    "        return compose(self.x[i], self.x_tfms), compose(self.y[i], self.y_tfms)\n",
    "    \n",
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = np.max(train_y) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms_x = [IdifyAndLimitedVocab(vocab2id, vocab_size), Numpyify(), Onehotify(vocab_size=vocab_size), Tensorify(), Floatify(), CheckAndCudify()]\n",
    "tfms_y = [YToOnehot(num_classes=num_classes), Tensorify(), Floatify(), CheckAndCudify()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = URSADataset(train_x, train_y, tfms_x=tfms_x, tfms_y=tfms_y)\n",
    "test_ds = URSADataset(test_x, test_y, tfms_x=tfms_x, tfms_y=tfms_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = Sampler(train_ds, bs, shuffle=False)\n",
    "test_samp = Sampler(test_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n",
    "test_dl = DataLoader(test_ds, sampler=test_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(in_feature, hidden_feature1, hidden_feature2, drop_rate):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "                ('linear1', nn.Linear(in_feature, hidden_feature1)),\n",
    "                ('act1', nn.Softplus()),\n",
    "                ('linear2', nn.Linear(hidden_feature1, hidden_feature2)),\n",
    "                ('act2', nn.Softplus()),\n",
    "                ('dropout', nn.Dropout(drop_rate))\n",
    "            ]))\n",
    "\n",
    "def decoder(in_feature, out_feature, drop_rate):\n",
    "     return nn.Sequential(OrderedDict([\n",
    "                ('act1', nn.Softmax(dim=-1)),\n",
    "                ('dropout', nn.Dropout(drop_rate)),\n",
    "                ('linear', nn.Linear(in_feature, out_feature)),\n",
    "                ('batchnorm', nn.BatchNorm1d(out_feature)),\n",
    "                ('act2', nn.Softmax(dim=-1))\n",
    "            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden(in_feature, out_feature):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "                ('linear', nn.Linear(in_feature, out_feature)),\n",
    "                ('batchnorm', nn.BatchNorm1d(out_feature))\n",
    "            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProdLDA(nn.Module):\n",
    "    def __init__(self, num_input, en1_units, en2_units, num_topic, drop_rate, init_mult):\n",
    "        super(ProdLDA, self).__init__()\n",
    "        self.num_input, self.en1_units, self.en2_units, \\\n",
    "        self.num_topic, self.drop_rate, self.init_mult = num_input, en1_units, en2_units, \\\n",
    "                                                            num_topic, drop_rate, init_mult\n",
    "        # encoder\n",
    "        self.en = encoder(num_input, en1_units, en2_units, drop_rate)\n",
    "        self.mean = hidden(en2_units, num_topic)\n",
    "        self.logvar = hidden(en2_units, num_topic)\n",
    "        # decoder\n",
    "        self.de = decoder(num_topic, num_input, drop_rate)\n",
    "        # prior mean and variance as constant buffers\n",
    "        self.prior_mean   = torch.Tensor(1, num_topic).fill_(0)\n",
    "        self.prior_var    = torch.Tensor(1, num_topic).fill_(variance)\n",
    "        self.prior_mean   = nn.Parameter(self.prior_mean, requires_grad=False)\n",
    "        self.prior_var    = nn.Parameter(self.prior_var, requires_grad=False)\n",
    "        self.prior_logvar = nn.Parameter(self.prior_var.log(), requires_grad=False)\n",
    "        # initialize decoder weight\n",
    "        if init_mult != 0:\n",
    "            #std = 1. / math.sqrt( init_mult * (num_topic + num_input))\n",
    "            self.de.linear.weight.data.uniform_(0, init_mult)\n",
    "        # remove BN's scale parameters\n",
    "        for component in [self.mean, self.logvar, self.de]:\n",
    "            component.batchnorm.weight.requires_grad = False\n",
    "            component.batchnorm.weight.fill_(1.0)\n",
    "\n",
    "    def encode(self, input_):\n",
    "        encoded = self.en(input_)\n",
    "        posterior_mean = self.mean(encoded)\n",
    "        posterior_logvar = self.logvar(encoded)\n",
    "        return encoded, posterior_mean, posterior_logvar\n",
    "    \n",
    "    def decode(self, input_, posterior_mean, posterior_var):\n",
    "        # take sample\n",
    "        eps = input_.data.new().resize_as_(posterior_mean.data).normal_() # noise \n",
    "        z = posterior_mean + posterior_var.sqrt() * eps                   # reparameterization\n",
    "        # do reconstruction\n",
    "        recon = self.de(z)          # reconstructed distribution over vocabulary\n",
    "        return recon\n",
    "    \n",
    "    def forward(self, input_, compute_loss=False, avg_loss=True):\n",
    "        # compute posterior\n",
    "        en2, posterior_mean, posterior_logvar = self.encode(input_) \n",
    "        posterior_var    = posterior_logvar.exp()\n",
    "        \n",
    "        recon = self.decode(input_, posterior_mean, posterior_var)\n",
    "        if compute_loss:\n",
    "            return recon, self.loss(input_, recon, posterior_mean, posterior_logvar, posterior_var, avg_loss)\n",
    "        else:\n",
    "            return recon\n",
    "\n",
    "    def loss(self, input_, recon, posterior_mean, posterior_logvar, posterior_var, avg=True):\n",
    "        # NL\n",
    "        NL  = -(input_ * (recon + 1e-10).log()).sum(1)\n",
    "        # KLD, see Section 3.3 of Akash Srivastava and Charles Sutton, 2017, \n",
    "        # https://arxiv.org/pdf/1703.01488.pdf\n",
    "        prior_mean   = self.prior_mean.expand_as(posterior_mean)\n",
    "        prior_var    = self.prior_var.expand_as(posterior_mean)\n",
    "        prior_logvar = self.prior_logvar.expand_as(posterior_mean)\n",
    "        var_division    = posterior_var  / prior_var\n",
    "        diff            = posterior_mean - prior_mean\n",
    "        diff_term       = diff * diff / prior_var\n",
    "        logvar_division = prior_logvar - posterior_logvar\n",
    "        # put KLD together\n",
    "        KLD = 0.5 * ( (var_division + diff_term + logvar_division).sum(1) - self.num_topic)\n",
    "        # loss\n",
    "        loss = (NL + KLD)\n",
    "        # in traiming mode, return averaged loss. In testing mode, return individual loss\n",
    "        if avg:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProdLDA(num_input, en1_units, en2_units, num_topic, drop_rate, init_mult)\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate, betas=(momentum, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss=149.2250062866211\n",
      "---------------Printing the Topics------------------\n",
      "villag light three type mushi honey near babbo cold recommend dessert issu deliv shock augment collag fillet push lu paul bbq mia ari room piec oop cartoon wake rental fort stylish secret breadstick popul soup understand advanc pleasur verd piglet jamey asparagu big strick plea tune downsid noodl veget stool\n",
      "websit smaller custom locat sensat feet yogurt hour piec man marin plate esp chef unslic r brine be brought gon much name lousi bouillabaiss wasnt pai suzann somewher tight elmhurst calmli disapp strick sweetbread art lobster sett couch tv prawn hover koi sake kid aforement po flown coupl mumbl eaten\n",
      "food next nd mustard laid pair sleek fresh bowl better make storm breast plain din etc crazi intern spaghetti whip test sett chees c brazilian domin rub remind scandinavian sake bistro goat pleas yorker one appear wok awsom addam recommend cake sweetbread wore pig chose booth mignon easili delici galleri\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  13337.43625573127\n",
      "Epoch 5, loss=126.02558679199218\n",
      "---------------Printing the Topics------------------\n",
      "dessert room green like soup noodl plate potato chocol egg chees cold apolog medium almond creme roast garlic fri never gave near honey steak tart stuf bbq side includ cooki calamari peanut tomato beef pie cream crispi dumpl breast veget diner villag piec spici chop huge lamb smoke tabl walk\n",
      "custom roll chef barista get fresh think smooth new lobster juggl rude piec expect much z know marin meal complet servic whole smaller bad shock bun websit oil tri aw yorker sake hour extrem ridden normal imagin season prawn spend collag histrion babi staff durat oth noth sensat issu terribl\n",
      "food place list make play peopl love cool sit authent one next better good romant atmospher garden quiet laid nyc eat anyon iciou locat stop recommend italian night ambienc sett well time beauti pretti ta amaz suggest everi insid mark experic sing beer long defin allow watch websit teriibl cramp\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  3533.87667953326\n",
      "Epoch 10, loss=116.67907916259766\n",
      "---------------Printing the Topics------------------\n",
      "chees green potato marin dessert beef chocol plate egg noodl calamari almond honey breast creme cream salad lamb soup roast stuf garlic dumpl medium tomato whip peanut cooki steak includ tender mushroom fri spici like crispi piec fruit tart smoke chop bbq fresh iciou sweetbread bun cake veget vanilla freshli\n",
      "custom never barista chef juggl one get smooth waitress owner know menu z would question shock unfriendli hostess yorker min rude extrem aw terribl servic websit new issu nyc better imagin much emerg complet waitstaff gratuiti overbook spend oth think ridden snooti strick juli mia amogst apolog remi hour inconsider\n",
      "sit play place food cool good locat atmospher quiet area loud list lit ambienc laid garden villag romant peopl jazz stop go patio anyon sing sleek love sett cramp make nyc touristi watch insid flamenco invit near eat stranger authent durat sexi classi nicer trattoria dancer waterfal redecor swanki relax\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1962.6638819199197\n",
      "Epoch 15, loss=113.37002178955078\n",
      "---------------Printing the Topics------------------\n",
      "marin chees green beef almond calamari breast egg honey potato noodl creme chocol dumpl stuf lamb piec fresh whip sweetbread peanut salad cream garlic cooki fruit roast mushroom tender bun spaghetti pate medium iciou soup tomato bbq spici gravi freshli eggplant crispi vanilla chop pie lobster swiss tart beet atop\n",
      "custom never question min apolog barista waitress gratuiti juggl told issu hostess one terribl shock unfriendli overbook z websit smooth mia annoy understand know servic guest blame snooti approach gave aw waiter rudest hover taken cell rude waitstaff comment inconsider oop extrem strick owner spend offens greet oth arriv refil\n",
      "play place lit sit locat sleek villag laid cool area invit space loud room jazz near stranger atmospher patio color quiet touristi garden ambienc rail cramp paint meet pub older sing backyard narrow roomi classi flamenco trattoria nicer booth dancer sexi insid downstair wise waterfal collag dimli minimalist nyc artsi\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1636.0095493497295\n",
      "Epoch 20, loss=112.58635192871094\n",
      "---------------Printing the Topics------------------\n",
      "marin almond chees honey calamari beef egg breast potato creme dumpl green noodl lamb sweetbread stuf piec chocol bun whip pate peanut spaghetti fresh iciou gravi cooki fruit bbq mushroom garlic cream tender roast eggplant swiss salad freshli blueberri casserol venison vanilla pie chop soup beet heavenli assort pancak atop\n",
      "custom question never min apolog gratuiti barista shock mia unfriendli juggl issu overbook waitress told terribl hostess annoy blame approach oop rudest guest hover understand silverwar websit gave z comment mumbl storm aw cell inconsider snooti offens refil spend wrong condescend taken oth displeas plate constantli paid spoke brought ditzi\n",
      "sleek lit play villag locat laid invit color stranger rail area space jazz cool touristi sit roomi patio loud near backyard narrow twinkl paint pub minimalist stylish sing flamenco beig dancer sexi older calm trattoria artsi collag room soundtrack wise transport quiet chandeli circa waterfal hunch dimli sprawl project candlelight\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1582.0609714974587\n",
      "Epoch 25, loss=112.31082330322266\n",
      "---------------Printing the Topics------------------\n",
      "marin almond honey calamari egg breast chees dumpl sweetbread beef creme potato pate spaghetti lamb noodl whip green bun stuf piec peanut gravi fruit chocol fresh iciou freshli eggplant swiss venison bbq casserol mushroom cooki blueberri garlic heavenli tender roast assort atop vanilla beet pancak cream chop salad bolognes po\n",
      "custom question min never gratuiti apolog barista issu shock unfriendli mia overbook annoy oop hover juggl websit blame mumbl told snooti comment hostess storm rudest offens terribl z approach waitress silverwar allergi understand guest inconsider cell displeas paid constantli comput complai condescend gave spend refil unwelcom oth abus spoke strick\n",
      "sleek villag lit play locat invit color rail laid stranger beig backyard twinkl stylish roomi minimalist collag area space touristi artsi sprawl flamenco circa contemporari paint patio narrow soundtrack pub hunch dancer shade trattoria transport older calm vault jazz near sit upholst neat neo sexi glow project chandeli bohemian musician\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1553.085100412249\n",
      "Epoch 30, loss=112.17609118652344\n",
      "---------------Printing the Topics------------------\n",
      "marin almond honey sweetbread calamari egg breast dumpl creme beef bun chees potato spaghetti pate whip noodl lamb peanut iciou stuf green gravi freshli cooki piec fruit swiss casserol venison bbq chocol blueberri heavenli assort eggplant fresh bolognes mushroom po vanilla atop cream beet tender garlic alaskan roast fillet enchilada\n",
      "custom question min gratuiti never shock apolog mia unfriendli barista issu overbook oop mumbl annoy snooti blame storm rudest websit terribl comment offens hover juggl unwelcom allergi approach told hostess waitress displeas cell paid understand z silverwar guest shortli confront ditzi atroci gave condescend suddendli comput blatantli constantli strick abus\n",
      "sleek villag lit locat play rail laid beig stranger twinkl invit color neo collag minimalist roomi circa sprawl stylish backyard touristi flamenco vault upholst dancer soundtrack contemporari transport hunch calm older area pub artsi musician shade causal project plant narrow paint cool seren bohemian synchron carrol space jazz waterfal iovin\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1576.0291175975685\n",
      "Epoch 35, loss=112.12254901123048\n",
      "---------------Printing the Topics------------------\n",
      "marin almond sweetbread honey calamari breast creme dumpl chees spaghetti bun egg whip beef pate potato noodl freshli lamb fruit gravi venison peanut swiss casserol stuf blueberri heavenli assort piec cooki po bbq iciou alaskan chocol green eggplant beet bolognes masala mushroom shiitak enchilada vanilla cream fillet atop garlic fresh\n",
      "min custom question gratuiti never shock overbook mia unfriendli issu snooti apolog oop barista mumbl unwelcom storm allergi rudest comment websit annoy offens hover blame displeas juggl cell shortli ditzi terribl told paid atroci blatantli strick waitress approach suddendli z understand ingridi verbal abus uncontrol comput hostess silverwar confront constantli\n",
      "sleek villag rail lit locat beig play twinkl invit color collag circa stranger roomi sprawl laid backyard neo minimalist touristi upholst bohemian dancer pub stylish contemporari iovin hunch flamenco artsi vault synchron soundtrack transport fenc shade project musician carrol calm trattoria narrow seren histrion causal befit older loungey plant neat\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1564.6728405342578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, loss=112.0651830444336\n",
      "---------------Printing the Topics------------------\n",
      "marin almond sweetbread honey creme breast calamari dumpl spaghetti bun whip pate chees beef egg noodl potato peanut po swiss freshli gravi lamb blueberri venison casserol heavenli fruit stuf cooki alaskan bbq iciou piec chocol assort shiitak enchilada atop masala eggplant fillet beet bolognes vanilla sorbet pie mushroom green pancak\n",
      "min question gratuiti custom mia overbook shock oop unfriendli never snooti mumbl unwelcom barista apolog issu websit storm offens rudest hover allergi annoy blame ditzi displeas shortli atroci suddendli juggl told comment strick verbal inabl approach constantli abus cell understand ingridi blatantli terribl uncontrol substitu paid teriibl comput charbon disapp\n",
      "sleek rail villag lit locat beig twinkl play roomi circa collag invit sprawl touristi neo color soundtrack backyard minimalist artsi shade upholst fenc carrol dancer flamenco iovin laid contemporari causal bohemian stranger vault project calm synchron hunch redecor histrion stylish hottest swanki trattoria pub musician neat birch plant worn overtli\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1561.7441808843616\n",
      "Epoch 45, loss=112.04263391113281\n",
      "---------------Printing the Topics------------------\n",
      "almond marin sweetbread honey spaghetti breast calamari creme dumpl pate beef bun swiss chees whip blueberri po egg venison casserol noodl potato lamb stuf peanut fruit bolognes gravi heavenli cooki alaskan assort masala freshli piec iciou shiitak sorbet rigatoni atop bbq garlic eggplant green fillet patti vanilla chocol enchilada beet\n",
      "min question gratuiti never mia overbook oop custom unfriendli snooti unwelcom shock mumbl barista rudest storm apolog issu offens hover displeas blame annoy shortli websit allergi suddendli ingridi strick atroci oth ditzi charbon verbal confront comment juggl constantli abus z uncontrol disapp cell rectifi waitress comb juli telephon ineffici deliicia\n",
      "sleek villag twinkl rail play roomi beig lit locat collag circa soundtrack color artsi stylish sprawl invit touristi causal minimalist carrol neo upholst iovin flamenco vault bohemian shade hunch fenc dancer redecor backyard revolut histrion birch stranger plant synchron worn fanni bedeck pub befit contemporari sceneri overtli hottest loungey musician\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1589.3408929145414\n",
      "Epoch 50, loss=111.96641162109376\n",
      "---------------Printing the Topics------------------\n",
      "marin almond sweetbread honey spaghetti calamari bun whip breast dumpl pate creme beef egg swiss chees blueberri potato peanut venison noodl bolognes lamb freshli casserol alaskan heavenli assort stuf po fruit gravi chocol masala shiitak piec rigatoni fillet enchilada eggplant patti sirloin cooki iciou bbq green mapl mushi atop edamam\n",
      "min question mia overbook gratuiti custom unwelcom oop snooti barista apolog mumbl offens issu unfriendli shock displeas storm never rudest allergi strick atroci websit ditzi annoy hover suddendli blatantli shortli verbal blame ingridi charbon comb abus comput juggl oth disapp calmli wolfgang uncontrol confront sta ineffici z understand rectifi cell\n",
      "sleek twinkl villag rail collag roomi beig lit soundtrack locat play upholst circa carrol touristi causal bohemian sprawl iovin neo stylish minimalist color redecor flamenco revolut invit hunch vault fenc tulip shade artsi plant birch bedeck contemporari histrion overtli project swanki dancer hottest loungey calm worn hillsid pub fasic ricketi\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1580.5393538550309\n",
      "Epoch 55, loss=111.91999847412109\n",
      "---------------Printing the Topics------------------\n",
      "almond marin sweetbread honey spaghetti calamari bun pate creme blueberri whip beef dumpl breast egg chees peanut swiss alaskan potato lamb stuf casserol venison bolognes assort gravi fruit enchilada po iciou freshli heavenli masala patti shiitak rigatoni mapl chocol eggplant garlic fillet piec green noodl sirloin edamam quesadilla beet julienn\n",
      "question overbook mia gratuiti min oop snooti custom unfriendli barista mumbl offens rudest displeas never apolog allergi unwelcom issu storm blame shock strick shortli ditzi disapp annoy suddendli oth charbon atroci websit ingridi comput confront hover comb teriibl uncontrol verbal calmli juggl blatantli sta abus patent comment substitu waitress approach\n",
      "sleek twinkl collag beig villag roomi rail circa lit causal soundtrack locat flamenco upholst carrol bohemian iovin artsi redecor neo play sprawl histrion minimalist touristi overtli ricketi hottest bedeck revolut vault fanni hunch loungey synchron shini purportedli hillsid tulip spotless tobacco project dancer invit blocki romat color shade contemporari plant\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1609.2836885612776\n",
      "Epoch 60, loss=111.91761639404297\n",
      "---------------Printing the Topics------------------\n",
      "marin sweetbread almond breast honey spaghetti creme calamari beef pate bun chees whip blueberri swiss stuf egg alaskan potato venison gravi dumpl freshli bolognes lamb casserol peanut mapl rigatoni shiitak heavenli patti assort langoustin masala fruit po eggplant sirloin noodl enchilada cooki fillet sorbet piec beet kale garlic iciou undrink\n",
      "overbook mia gratuiti min question oop unfriendli snooti offens unwelcom custom barista mumbl apolog disapp never displeas strick rudest allergi blame suddendli shock comb ingridi verbal atroci charbon confront shortli uncontrol hover websit comput issu oth ditzi calmli sta blatantli substitu abus gratitu juggl comment nonstop tearoom patent storm annoy\n",
      "sleek twinkl collag beig roomi villag rail circa artsi overtli sprawl upholst carrol neo iovin play soundtrack redecor hottest histrion lit ricketi flamenco revolut locat bedeck causal bohemian synchron minimalist touristi shade loungey vault purportedli possess color spotless fanni tulip birch hillsid italia hunch pub teen stylish shini modernist fasic\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1589.6023750162112\n",
      "Epoch 65, loss=111.92563037109375\n",
      "---------------Printing the Topics------------------\n",
      "marin sweetbread almond honey breast creme spaghetti pate calamari chees swiss egg beef whip bun potato dumpl gravi venison stuf lamb blueberri casserol alaskan fruit freshli langoustin fillet heavenli shiitak rigatoni assort lumpi bolognes peanut noodl mapl undrink roast dung eggplant atop masala kale enchilada tender shepard mushroom beet iciou\n",
      "overbook mia min oop gratuiti unwelcom question barista snooti custom rudest offens strick displeas ingridi apolog shock never disapp suddendli blame comb uncontrol unfriendli charbon allergi mumbl ditzi juggl websit confront verbal calmli shortli gratitu issu substitu annoy oth tearoom hover inconsider off abus atroci sta insincer patent teriibl deliicia\n",
      "sleek twinkl collag circa beig rail soundtrack overtli iovin roomi redecor neo carrol villag upholst histrion artsi bedeck revolut sprawl ricketi lit touristi play italia synchron causal flamenco vault tulip stylish fanni birch modernist hottest project bohemian hillsid contemporari outlandish loungey possess hunch musician invit secreci fenc shade color locat\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1584.3393458137723\n",
      "Epoch 70, loss=111.9025425415039\n",
      "---------------Printing the Topics------------------\n",
      "sweetbread almond marin breast spaghetti honey creme chees pate calamari dumpl blueberri bun beef whip venison swiss casserol fillet gravi fruit lamb mapl assort egg noodl heavenli langoustin potato stuf shiitak alaskan roast bolognes eggplant lumpi rigatoni tapenad freshli peanut dung undrink kale garlic churrasco shepard iciou po heartier enchilada\n",
      "overbook mia min gratuiti oop unwelcom question snooti strick rudest barista charbon unfriendli custom suddendli mumbl ingridi offens ditzi disapp comb apolog blame displeas issu never uncontrol shortli gratitu atroci juggl shock allergi blatantli deliicia verbal confront storm sta websit comput tearoom calmli leasur candid cloudi patent teriibl oth substitu\n",
      "twinkl sleek collag rail circa beig soundtrack neo overtli carrol histrion revolut villag touristi synchron tulip upholst iovin artsi redecor bohemian bedeck italia lit spotless ricketi roomi birch project modernist loungey strut play flamenco hunch blocki fanni tobacco vault magritt invit shini outlandish hottest causal possess fenc romat hillsid fasic\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1594.5353771448256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, loss=111.89050354003906\n",
      "---------------Printing the Topics------------------\n",
      "sweetbread marin almond creme breast spaghetti pate honey calamari swiss blueberri potato dumpl casserol beef chees bun mapl noodl venison egg alaskan undrink gravi fillet langoustin shiitak whip assort lamb heavenli eggplant peanut bolognes stuf rigatoni garlic roast fruit dung tapenad heartier mushroom lumpi enchilada freshli kha masala kale spaetzl\n",
      "overbook mia unwelcom oop strick min question rudest barista gratuiti displeas snooti charbon unfriendli uncontrol ingridi comb custom disapp juggl offens suddendli blame deliicia mumbl shortli shock verbal issu storm ditzi gratitu candid never apolog sta atroci tearoom cloudi contagi off confront abus leasur teriibl remi hover oth jamey patent\n",
      "twinkl collag rail circa carrol sleek histrion touristi overtli iovin revolut italia neo tulip redecor synchron beig bohemian soundtrack hottest roomi bedeck hunch tobacco sprawl blocki artsi lit birch fanni upholst spotless ricketi strut magritt modernist villag fasic loungey reminesc secreci purportedli flamenco tablewar play musician project flirtat locat causal\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1617.7546077019822\n",
      "Epoch 80, loss=111.90795336914063\n",
      "---------------Printing the Topics------------------\n",
      "marin sweetbread almond creme breast pate calamari spaghetti honey swiss blueberri potato dumpl chees whip noodl bun shiitak egg casserol gravi alaskan venison lamb assort beef bolognes roast eggplant heartier peanut langoustin mapl undrink heavenli stuf rigatoni masala julienn garlic mushroom enchilada dung kha fruit freshli asparagu lard tapenad lumpi\n",
      "overbook mia min unwelcom strick gratuiti oop displeas rudest comb question barista charbon uncontrol ingridi snooti custom apolog unfriendli mumbl suddendli deliicia offens verbal blame gratitu shock tearoom patent storm juggl teriibl disapp hover sta shortli substitu off candid confront never allergi oth ditzi websit cloudi subst disconnect agoni champ\n",
      "collag twinkl rail neo circa sleek carrol beig histrion revolut touristi overtli italia tulip hottest roomi bohemian iovin blocki synchron soundtrack sprawl redecor strut ricketi secreci hunch fanni purportedli bedeck reminesc fasic spotless magritt upholst dekalb play villag loungey hillsid lit leatherett tablewar locat minimalist pub ballato tobacco modernist outlandish\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1631.6078590596246\n",
      "Epoch 85, loss=111.86680389404297\n",
      "---------------Printing the Topics------------------\n",
      "marin sweetbread almond honey pate breast swiss potato blueberri creme calamari gravi alaskan spaghetti beef chees shiitak roast dumpl casserol bun whip undrink heartier noodl rigatoni lamb eggplant bolognes julienn freshli garlic stuf peanut lumpi egg venison kha langoustin masala atop asparagu shepard puffi mapl fruit brace enchilada tok dung\n",
      "unwelcom overbook mia question rudest strick uncontrol min comb displeas oop suddendli gratuiti barista charbon ingridi offens verbal teriibl apolog custom deliicia juggl unfriendli disapp sta snooti tearoom storm patent gratitu blame never allergi shortli ditzi leasur contagi torment oth candid jamey mumbl lulu insincer flit hover testi off incomprehens\n",
      "collag twinkl circa rail beig neo sleek revolut histrion carrol redecor soundtrack overtli iovin artsi blocki synchron upholst tulip touristi italia fanni purportedli villag bohemian hunch hottest ballato spotless fasic strut tablewar outlandish ricketi sprawl dekalb secreci causal magritt play bedeck leatherett modernist reminesc hillsid lit trickl roomi ionic babalu\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1646.2554820403238\n",
      "Epoch 90, loss=111.89178283691406\n",
      "---------------Printing the Topics------------------\n",
      "marin sweetbread honey almond breast pate potato blueberri swiss casserol calamari roast alaskan lamb spaghetti eggplant undrink creme shiitak garlic whip heartier beef chees lumpi langoustin bun julienn noodl bolognes rigatoni gravi venison stuf peanut dung shepard freshli heavenli egg kha dumpl brace masala fruit pecan atop piquant assort mapl\n",
      "overbook mia unwelcom uncontrol gratuiti rudest suddendli min comb strick question displeas teriibl custom barista charbon oop deliicia verbal tearoom snooti apolog allergi shortli ingridi sta atroci gratitu juggl flit disapp offens leasur patent mumbl jamey oth unfriendli eyebal disconnect ditzi never blame contagi hover torment bs storm substitu lulu\n",
      "collag sleek twinkl revolut beig circa rail redecor histrion soundtrack synchron fanni purportedli blocki carrol upholst ballato bohemian bedeck lit tulip neo spotless outlandish iovin hillsid causal artsi hunch dekalb leatherett overtli fasic italia modernist strut magritt ionic tablewar ricketi villag tobacco flirtat reminesc sprawl secreci play trickl loungey bavarian\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1634.299661977018\n",
      "Epoch 95, loss=111.86798913574219\n",
      "---------------Printing the Topics------------------\n",
      "marin sweetbread almond honey breast roast blueberri swiss pate alaskan spaghetti creme calamari casserol potato garlic undrink beef shiitak whip eggplant gravi chees julienn lumpi dung shepard langoustin rigatoni venison lamb heartier bun pecan atop asparagu egg stuf peanut tok freshli kha mapl brace piquant cro heavenli adventuresom fruit tapenad\n",
      "overbook mia unwelcom uncontrol strick barista min suddendli displeas gratuiti deliicia teriibl charbon juggl question comb custom verbal tearoom gratitu mumbl shortli rudest off disapp patent sta oop snooti ingridi atroci apolog empi allergi contagi storm cloudi unfriendli blab ditzi substitu never telephon lulu subst flit bernard leasur incomprehens insincer\n",
      "collag circa sleek beig twinkl histrion rail revolut redecor purportedli synchron soundtrack blocki neo upholst ricketi overtli fanni bedeck strut iovin magritt leatherett ballato hillsid spotless causal bohemian carrol lit fasic artsi dekalb hunch secreci modernist outlandish vault tulip flirtat villag possess touristi loungey ionic godard bavarian sprawl speakeasi bastianich\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1657.9817098298827\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    loss_epoch = 0.0\n",
    "    model.train()                    # switch to training mode\n",
    "    for input_, _ in train_dl:\n",
    "        recon, loss = model(input_, compute_loss=True)\n",
    "        # optimize\n",
    "        optimizer.zero_grad()        # clear previous gradients\n",
    "        loss.backward()              # backprop\n",
    "        optimizer.step()             # update parameters\n",
    "        # report\n",
    "        loss_epoch += loss.item()    # add loss to loss_epoch\n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch {}, loss={}'.format(epoch, loss_epoch / len(input_)))\n",
    "        emb = model.de.linear.weight.data.detach().cpu().numpy().T\n",
    "        print_top_words(emb, vocab, 50)\n",
    "        print_perp(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Printing the Topics------------------\n",
      "marin sweetbread honey almond breast roast blueberri alaskan creme pate swiss spaghetti gravi calamari undrink dung beef garlic whip shiitak shepard potato bun rigatoni casserol julienn lumpi eggplant asparagu heavenli atop freshli pecan chees venison puffi egg dumpl cro stuf tok langoustin brace mushroom tender lamb masala kha mapl peanut\n",
      "overbook mia unwelcom strick uncontrol barista suddendli charbon juggl question gratuiti teriibl min tearoom snooti oop deliicia gratitu displeas off atroci comb verbal sta rudest disapp ingridi blab shortli mumbl empi patent unfriendli custom ditzi leasur contagi storm never telephon apolog disconnect cloudi jamey blame sumatra incomprehens substitu flit uncal\n",
      "collag rail beig circa twinkl revolut histrion synchron purportedli leatherett bedeck blocki sleek redecor upholst ricketi overtli fanni soundtrack neo strut hunch causal tulip loungey bohemian ballato modernist hillsid magritt bastianich flirtat artsi sprawl iovin speakeasi spotless lit carrol flamenco mink dekalb teen secreci possess outlandish mullion babalu ionic birch\n",
      "---------------End of Topics------------------\n",
      "The approximated perplexity is:  1674.5596454802233\n"
     ]
    }
   ],
   "source": [
    "emb = model.de.linear.weight.data.cpu().numpy().T\n",
    "print_top_words(emb, vocab, 50)\n",
    "print_perp(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avi",
   "language": "python",
   "name": "avi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
